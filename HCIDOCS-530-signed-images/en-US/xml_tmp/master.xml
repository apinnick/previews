<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE book [
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<?asciidoc-toc?><?asciidoc-numbered?><book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
<info>
<title>User guide</title><subtitle>Deploying sandboxed containers in OpenShift Container Platform</subtitle>

<date>2024-11-18</date>
<productname>OpenShift sandboxed containers</productname>
<productnumber>1.8</productnumber>
<release>1.8</release>
<abstract>
	<para>
		Deploying OpenShift sandboxed containers in OpenShift Container Platform on bare metal, public cloud, and IBM platforms.
	</para>
</abstract>
<xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="Author_Group.xml"/>
<xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="Common_Content/Legal_Notice.xml"/>
</info>
<preface>
<title>Preface</title>

<bridgehead xml:id="providing-feedback-on-red-hat-documentation" renderas="sect1">Providing feedback on Red Hat documentation</bridgehead>
<simpara>You can provide feedback or report an error by submitting the <emphasis role="strong">Create Issue</emphasis> form in Jira. The Jira issue will be created in the Red Hat Hybrid Cloud Infrastructure Jira project, where you can track the progress of your feedback.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Ensure that you are logged in to Jira. If you do not have a Jira account, you must create a <link xlink:href="https://issues.redhat.com">Red Hat Jira account</link>.</simpara>
</listitem>
<listitem>
<simpara>Launch the <link xlink:href="https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12341520&amp;summary=Documentation+feedback&amp;issuetype=1&amp;description=Details:%0A%0ADocumentation+URL:%0A%0A&amp;priority=10200&amp;labels=hcidocs-feedback&amp;components=12393342"><emphasis role="strong">Create Issue</emphasis> form</link>.</simpara>
</listitem>
<listitem>
<simpara>Complete the <emphasis role="strong">Summary</emphasis>, <emphasis role="strong">Description</emphasis>, and <emphasis role="strong">Reporter</emphasis> fields.</simpara>
<simpara>In the <emphasis role="strong">Description</emphasis> field, include the documentation URL, chapter or section number, and a detailed description of the issue.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
</preface>
<chapter xml:id="about-osc">
<title>About OpenShift sandboxed containers</title>
<simpara>OpenShift sandboxed containers for OpenShift Container Platform integrates Kata Containers as an optional runtime, providing enhanced security and isolation by running containerized applications in lightweight virtual machines. This integration provides a more secure runtime environment for sensitive workloads without significant changes to existing OpenShift workflows. This runtime supports containers in dedicated virtual machines (VMs), providing improved workload isolation.</simpara>
<section xml:id="osc-features_about-osc">
<title>Features</title>
<simpara>OpenShift sandboxed containers provides the following features:</simpara>
<variablelist>
<varlistentry>
<term>Run privileged or untrusted workloads</term>
<listitem>
<simpara>You can safely run workloads that require specific privileges, without the risk of compromising cluster nodes by running privileged containers. Workloads that require special privileges include the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Workloads that require special capabilities from the kernel, beyond the default ones granted by standard container runtimes such as CRI-O, for example to access low-level networking features.</simpara>
</listitem>
<listitem>
<simpara>Workloads that need elevated root privileges, for example to access a specific physical device. With OpenShift sandboxed containers, it is possible to pass only a specific device through to the virtual machines (VM), ensuring that the workload cannot access or misconfigure the rest of the system.</simpara>
</listitem>
<listitem>
<simpara>Workloads for installing or using <literal>set-uid</literal> root binaries. These binaries grant special privileges and, as such, can present a security risk. With OpenShift sandboxed containers, additional privileges are restricted to the virtual machines, and grant no special access to the cluster nodes.</simpara>
<simpara>Some workloads require privileges specifically for configuring the cluster nodes. Such workloads should still use privileged containers, because running on a virtual machine would prevent them from functioning.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Ensure isolation for sensitive workloads</term>
<listitem>
<simpara>The OpenShift sandboxed containers for Red Hat OpenShift Container Platform integrates Kata Containers as an optional runtime, providing enhanced security and isolation by running containerized applications in lightweight virtual machines. This integration provides a more secure runtime environment for sensitive workloads without significant changes to existing OpenShift workflows. This runtime supports containers in dedicated virtual machines (VMs), providing improved workload isolation.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Ensure kernel isolation for each workload</term>
<listitem>
<simpara>You can run workloads that require custom kernel tuning (such as <literal>sysctl</literal>, scheduler changes, or cache tuning) and the creation of custom kernel modules (such as <literal>out of tree</literal> or special arguments).</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Share the same workload across tenants</term>
<listitem>
<simpara>You can run workloads that support many users (tenants) from different organizations sharing the same OpenShift Container Platform cluster. The system also supports running third-party workloads from multiple vendors, such as container network functions (CNFs) and enterprise applications. Third-party CNFs, for example, may not want their custom settings interfering with packet tuning or with <literal>sysctl</literal> variables set by other applications. Running inside a completely isolated kernel is helpful in preventing "noisy neighbor" configuration problems.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Ensure proper isolation and sandboxing for testing software</term>
<listitem>
<simpara>You can run containerized workloads with known vulnerabilities or handle issues in an existing application. This isolation enables administrators to give developers administrative control over pods, which is useful when the developer wants to test or validate configurations beyond those an administrator would typically grant. Administrators can, for example, safely and securely delegate kernel packet filtering (eBPF) to developers. eBPF requires <literal>CAP_ADMIN</literal> or <literal>CAP_BPF</literal> privileges, and is therefore not allowed under a standard CRI-O configuration, as this would grant access to every process on the Container Host worker node. Similarly, administrators can grant access to intrusive tools such as <literal>SystemTap</literal>, or support the loading of custom kernel modules during their development.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Ensure default resource containment through VM boundaries</term>
<listitem>
<simpara>By default, OpenShift sandboxed containers manages resources such as CPU, memory, storage, and networking in a robust and secure way. Since OpenShift sandboxed containers deploys on VMs, additional layers of isolation and security give a finer-grained access control to the resource. For example, an errant container will not be able to assign more memory than is available to the VM. Conversely, a container that needs dedicated access to a network card or to a disk can take complete control over that device without getting any access to other devices.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="compatiblity-platforms_about-osc">
<title>Compatibility with OpenShift Container Platform</title>
<simpara>The required functionality for the OpenShift Container Platform platform is supported by two main components:</simpara>
<itemizedlist>
<listitem>
<simpara>Kata runtime: This includes Red Hat Enterprise Linux CoreOS (RHCOS) and <link xlink:href="https://access.redhat.com/support/policy/updates/openshift/">updates</link> with every OpenShift Container Platform release.</simpara>
</listitem>
<listitem>
<simpara>OpenShift sandboxed containers Operator: Install the Operator using either the web console or OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<simpara>The OpenShift sandboxed containers Operator is a <link xlink:href="https://access.redhat.com/support/policy/updates/openshift_operators#rolling-stream">Rolling Stream Operator</link>, which means the latest version is the only supported version. It works with all currently supported versions of OpenShift Container Platform. For more information, see <link xlink:href="https://access.redhat.com/support/policy/updates/openshift/">OpenShift Container Platform Life Cycle Policy</link> for additional details.</simpara>
<simpara>The Operator depends on the features that come with the RHCOS host and the environment it runs in.</simpara>
<note>
<simpara>You must install Red Hat Enterprise Linux CoreOS (RHCOS) on the worker nodes. RHEL nodes are not supported.</simpara>
</note>
<simpara>The following compatibility matrix for OpenShift sandboxed containers and OpenShift Container Platform releases identifies compatible features and environments.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Supported architectures</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="bottom">Architecture</entry>
<entry align="left" valign="bottom">OpenShift Container Platform version</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>x86_64</simpara></entry>
<entry align="left" valign="top"><simpara>4.8 or later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>s390x</simpara></entry>
<entry align="left" valign="top"><simpara>4.14 or later</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>There are two ways to deploy Kata containers runtime:</simpara>
<itemizedlist>
<listitem>
<simpara>Bare metal</simpara>
</listitem>
<listitem>
<simpara>Peer pods</simpara>
</listitem>
</itemizedlist>
<simpara>Peer pods technology for the deployment of OpenShift sandboxed containers in public clouds was available as Developer Preview in OpenShift sandboxed containers 1.5 and OpenShift Container Platform 4.14.</simpara>
<simpara>With the release of OpenShift sandboxed containers 1.7, the Operator requires OpenShift Container Platform version 4.15 or later.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Feature availability by OpenShift version</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="bottom">Feature</entry>
<entry align="left" valign="bottom">Deployment method</entry>
<entry align="left" valign="bottom">OpenShift Container Platform 4.15</entry>
<entry align="left" valign="bottom">OpenShift Container Platform 4.16</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top" morerows="1"><simpara>Confidential Containers</simpara></entry>
<entry align="left" valign="top"><simpara>Bare metal</simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"/>
</row>
<row>
<entry align="left" valign="top"><simpara>Peer pods</simpara></entry>
<entry align="left" valign="top"><simpara>Technology Preview</simpara></entry>
<entry align="left" valign="top"><simpara>Technology Preview <superscript>[1]</superscript></simpara></entry>
</row>
<row>
<entry align="left" valign="top" morerows="1"><simpara>GPU support <superscript>[2]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Bare metal</simpara></entry>
<entry align="left" valign="top"/>
<entry align="left" valign="top"/>
</row>
<row>
<entry align="left" valign="top"><simpara>Peer pods</simpara></entry>
<entry align="left" valign="top"><simpara>Developer Preview</simpara></entry>
<entry align="left" valign="top"><simpara>Developer Preview</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<orderedlist numeration="arabic">
<listitem>
<simpara>Technology Preview of Confidential Containers has been available since OpenShift sandboxed containers 1.7.0.</simpara>
</listitem>
<listitem>
<simpara>GPU functionality is not available on IBM Z.</simpara>
</listitem>
</orderedlist>
<table frame="all" rowsep="1" colsep="1">
<title>Supported cloud platforms for OpenShift sandboxed containers</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33*"/>
<colspec colname="col_2" colwidth="33*"/>
<colspec colname="col_3" colwidth="33*"/>
<thead>
<row>
<entry align="left" valign="bottom">Platform</entry>
<entry align="left" valign="bottom">GPU</entry>
<entry align="left" valign="bottom">Confidential Containers</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>AWS Cloud Computing Services</simpara></entry>
<entry align="left" valign="top"><simpara>Developer Preview</simpara></entry>
<entry align="left" valign="top"/>
</row>
<row>
<entry align="left" valign="top"><simpara>Microsoft Azure Cloud Computing Services</simpara></entry>
<entry align="left" valign="top"><simpara>Developer Preview</simpara></entry>
<entry align="left" valign="top"><simpara>Technology Preview <superscript>[1]</superscript></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<orderedlist numeration="arabic">
<listitem>
<simpara>Technology Preview of Confidential Containers has been available since OpenShift sandboxed containers 1.7.0.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/support/offerings/devpreview">Developer Preview Support Scope</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/support/offerings/techpreview">Technology Preview Features - Scope of Support</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="about-node-eligibility-checks_about-osc">
<title>Node eligibility checks</title>
<simpara xml:id="node-eligibility-checks">You can verify that your bare-metal cluster nodes support OpenShift sandboxed containers by running a node eligibility check.
The most common reason for node ineligibility is lack of virtualization support.
If you run sandboxed workloads on ineligible nodes, you will experience errors.</simpara>
<orderedlist numeration="arabic">
<title>High-level workflow</title>
<listitem>
<simpara>Install the Node Feature Discovery Operator.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>NodeFeatureDiscovery</literal> custom resource (CR).</simpara>
</listitem>
<listitem>
<simpara>Enable node eligibility checks when you create the <literal>Kataconfig</literal> CR. You can run node eligibility checks on all worker nodes or on selected nodes.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/specialized_hardware_and_driver_enablement/index#about-node-feature-discovery-operator_node-feature-discovery-operator">Installing the Node Feature Discovery Operator</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="common-terms_about-osc">
<title>Common terms</title>
<simpara>The following terms are used throughout the documentation.</simpara>
<variablelist>
<varlistentry>
<term>Sandbox</term>
<listitem>
<simpara>A sandbox is an isolated environment where programs can run. In a sandbox, you can run untested or untrusted programs without risking harm to the host machine or the operating system.</simpara>
<simpara>In the context of OpenShift sandboxed containers, sandboxing is achieved by running workloads in a different kernel using virtualization, providing enhanced control over the interactions between multiple workloads that run on the same host.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Pod</term>
<listitem>
<simpara>A pod is a construct that is inherited from Kubernetes and OpenShift Container Platform. It represents resources where containers can be deployed. Containers run inside of pods, and pods are used to specify resources that can be shared between multiple containers.</simpara>
<simpara>In the context of OpenShift sandboxed containers, a pod is implemented as a virtual machine. Several containers can run in the same pod on the same virtual machine.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>OpenShift sandboxed containers Operator</term>
<listitem>
<simpara>The OpenShift sandboxed containers Operator manages the lifecycle of sandboxed containers on a cluster. You can use the OpenShift sandboxed containers Operator to perform tasks such as the installation and removal of sandboxed containers, software updates, and status monitoring.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Kata Containers</term>
<listitem>
<simpara>Kata Containers is a core upstream project that is used to build OpenShift sandboxed containers. OpenShift sandboxed containers integrate Kata Containers with OpenShift Container Platform.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>KataConfig</term>
<listitem>
<simpara><literal>KataConfig</literal> objects represent configurations of sandboxed containers. They store information about the state of the cluster, such as the nodes on which the software is deployed.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Runtime class</term>
<listitem>
<simpara>A <literal>RuntimeClass</literal> object describes which runtime can be used to run a given workload. A runtime class that is named <literal>kata</literal> is installed and deployed by the OpenShift sandboxed containers Operator. The runtime class contains information about the runtime that describes resources that the runtime needs to operate, such as the <link xlink:href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</link>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<variablelist xml:id="peer-pods">
<varlistentry>
<term>Peer pod</term>
<listitem>
<simpara>A peer pod in OpenShift sandboxed containers extends the concept of a standard pod. Unlike a standard sandboxed container, where the virtual machine is created on the worker node itself, in a peer pod, the virtual machine is created through a remote hypervisor using any supported hypervisor or cloud provider API.</simpara>
<simpara>The peer pod acts as a regular pod on the worker node, with its corresponding VM running elsewhere. The remote location of the VM is transparent to the user and is specified by the runtime class in the pod specification. The peer pod design circumvents the need for nested virtualization.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>IBM Secure Execution</term>
<listitem>
<simpara>IBM Secure Execution for Linux is an advanced security feature introduced with IBM z15® and LinuxONE III. This feature extends the protection provided by pervasive encryption. IBM Secure Execution safeguards data at rest, in transit, and in use. It enables secure deployment of workloads and ensures data protection throughout its lifecycle. For more information, see <link xlink:href="https://www.ibm.com/docs/en/linux-on-systems?topic=management-secure-execution">Introducing IBM Secure Execution for Linux</link>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Confidential Containers</term>
<listitem>
<simpara>Confidential Containers protects containers and data by verifying that your workload is running in a Trusted Execution Environment (TEE). You can deploy this feature to safeguard the privacy of big data analytics and machine learning inferences.</simpara>
<simpara><emphasis role="strong">Trustee</emphasis> is a component of Confidential Containers. Trustee is an attestation service that verifies the trustworthiness of the location where you plan to run your workload or where you plan to send confidential information. Trustee includes components deployed on a trusted side and used to verify whether the remote workload is running in a Trusted Execution Environment (TEE). Trustee is flexible and can be deployed in several different configurations to support a wide variety of applications and hardware platforms.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Confidential compute attestation Operator</term>
<listitem>
<simpara>The Confidential compute attestation Operator manages the installation, lifecycle, and configuration of Confidential Containers.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="osc-operator_about-osc">
<title>OpenShift sandboxed containers Operator</title>
<simpara>The OpenShift sandboxed containers Operator encapsulates all of the components from Kata containers. It manages installation, lifecycle, and configuration tasks.</simpara>
<simpara>The OpenShift sandboxed containers Operator is packaged in the <link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/operator_sdk/index#osdk-working-bundle-images.html">Operator bundle format</link> as two container images:</simpara>
<itemizedlist>
<listitem>
<simpara>The bundle image contains metadata and is required to make the operator OLM-ready.</simpara>
</listitem>
<listitem>
<simpara>The second container image contains the actual controller that monitors and manages the <literal>KataConfig</literal> resource.</simpara>
</listitem>
</itemizedlist>
<simpara>The OpenShift sandboxed containers Operator is based on the Red Hat Enterprise Linux CoreOS (RHCOS) extensions concept. RHCOS extensions are a mechanism to install optional OpenShift Container Platform software. The OpenShift sandboxed containers Operator uses this mechanism to deploy sandboxed containers on a cluster.</simpara>
<simpara>The sandboxed containers RHCOS extension contains RPMs for Kata, QEMU, and its dependencies. You can enable them by using the <literal>MachineConfig</literal> resources that the Machine Config Operator provides.</simpara>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/post_installation_configuration/index#rhcos-add-extensions_post-install-machine-configuration-tasks">Adding extensions to RHCOS</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="about-confidential-containers_about-osc">
<title>About Confidential Containers</title>
<simpara>Confidential Containers provides a confidential computing environment to protect containers and data by leveraging <link xlink:href="https://en.wikipedia.org/wiki/Trusted_execution_environment">Trusted Execution Environments</link>.</simpara>
<important>
<simpara>Confidential Containers on Microsoft Azure Cloud Computing Services, IBM Z®, and IBM® LinuxONE is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>You can sign container images by using a tool such as <link xlink:href="https://developers.redhat.com/products/trusted-artifact-signer/overview">Red Hat Trusted Artifact Signer</link>. Then, you create a container image signature verification policy.</simpara>
<simpara>The Trustee Operator verifies the signatures, ensuring that only trusted and authenticated container images are deployed in your environment.</simpara>
<simpara>For more information, see <link xlink:href="https://www.redhat.com/en/blog/exploring-openshift-confidential-containers-solution">Exploring the OpenShift Confidential Containers solution</link>.</simpara>
</section>
<section xml:id="ocp-virt-and-osc_about-osc">
<title>OpenShift Virtualization</title>
<simpara>You can deploy OpenShift sandboxed containers on clusters with OpenShift Virtualization.</simpara>
<simpara>To run OpenShift Virtualization and OpenShift sandboxed containers at the same time, your virtual machines must be live migratable so that they do not block node reboots. See <link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/virtualization/index#virt-about-live-migration">About live migration</link> in the OpenShift Virtualization documentation for details.</simpara>
</section>
<section xml:id="block_volume_support_about-osc">
<title>Block volume support</title>
<simpara>OpenShift Container Platform can statically provision raw block volumes. These volumes do not have a file system, and can provide performance benefits for applications that either write to the disk directly or implement their own storage service.</simpara>
<simpara>You can use a local block device as persistent volume (PV) storage with OpenShift sandboxed containers. This block device can be provisioned by using the Local Storage Operator (LSO).</simpara>
<simpara>The Local Storage Operator is not installed in OpenShift Container Platform by default. See <link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/storage/configuring-persistent-storage#local-storage-install_persistent-storage-local">Installing the Local Storage Operator</link> for installation instructions.</simpara>
<simpara>You can provision raw block volumes for OpenShift sandboxed containers by specifying <literal>volumeMode: Block</literal> in the PV specification.</simpara>
<formalpara>
<title>Block volume example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-disks"
  namespace: "openshift-local-storage"
spec:
  nodeSelector:
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-0
  storageClassDevices:
    - storageClassName: "local-sc"
      forceWipeDevicesAndDestroyAllData: false
      volumeMode: Block <co xml:id="CO1-1"/>
      devicePaths:
        - /path/to/device <co xml:id="CO1-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO1-1">
<para>Set <literal>volumeMode</literal> to <literal>Block</literal> to indicate that this PV is a raw block volume.</para>
</callout>
<callout arearefs="CO1-2">
<para>Replace this value with the filepath to your <literal>LocalVolume</literal> resource <literal>by-id</literal>. PVs are created for these local disks when the provisioner is deployed successfully. You must also use this path to label the node that uses the block device when deploying OpenShift sandboxed containers.</para>
</callout>
</calloutlist>
</section>
<section xml:id="fips-compliance_about-osc">
<title>FIPS compliance</title>
<simpara>OpenShift Container Platform is designed for Federal Information Processing Standards (FIPS) 140-2 and 140-3. When running Red Hat Enterprise Linux (RHEL) or Red Hat Enterprise Linux CoreOS (RHCOS) booted in FIPS mode, OpenShift Container Platform core components use the RHEL cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the <literal>x86_64</literal>, <literal>ppc64le</literal>, and <literal>s390x</literal> architectures.</simpara>
<simpara>For more information about the NIST validation program, see <link xlink:href="https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules">Cryptographic Module Validation Program</link>. For the latest NIST status for the individual versions of RHEL cryptographic libraries that have been submitted for validation, see <link xlink:href="https://access.redhat.com/articles/2918071#fips-140-2-and-fips-140-3-2">Compliance Activities and Government Standards</link>.</simpara>
<simpara>OpenShift sandboxed containers can be used on FIPS enabled clusters.</simpara>
<simpara>When running in FIPS mode, OpenShift sandboxed containers components, VMs, and VM images are adapted to comply with FIPS.</simpara>
<note>
<simpara>FIPS compliance for OpenShift sandboxed containers only applies to the <literal>kata</literal> runtime class. The peer pod runtime class, <literal>kata-remote</literal>, is not yet fully supported and has not been tested for FIPS compliance.</simpara>
</note>
<simpara>FIPS compliance is one of the most critical components required in highly secure environments, to ensure that only supported cryptographic technologies are allowed on nodes.</simpara>
<important>
<simpara>The use of FIPS Validated / Modules in Process cryptographic libraries is only supported on OpenShift Container Platform deployments on the <literal>x86_64</literal> architecture.</simpara>
</important>
<simpara>To understand Red Hat’s view of OpenShift Container Platform compliance frameworks, refer to the Risk Management and Regulatory Readiness chapter of the <link xlink:href="https://access.redhat.com/articles/5059881">OpenShift Security Guide Book</link>.</simpara>
</section>
</chapter>
<chapter xml:id="deploying-osc-bare-metal">
<title>Deploying OpenShift sandboxed containers on bare metal</title>
<simpara>You can deploy OpenShift sandboxed containers on an on-premise bare-metal cluster with Red Hat Enterprise Linux CoreOS (RHCOS) installed on the worker nodes.</simpara>
<note>
<itemizedlist>
<listitem>
<simpara>RHEL nodes are not supported.</simpara>
</listitem>
<listitem>
<simpara>Nested virtualization is not supported.</simpara>
</listitem>
</itemizedlist>
</note>
<simpara>You can use any installation method including <link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/installing/index#installing-bare-metal">user-provisioned</link>, <link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/installing/index#deploying-installer-provisioned-clusters-on-bare-metal">installer-provisioned</link>, or <link xlink:href="https://access.redhat.com/documentation/en-us/assisted_installer_for_openshift_container_platform">Assisted Installer</link> to deploy your cluster.</simpara>
<simpara>You can also install OpenShift sandboxed containers on Amazon Web Services (AWS) bare-metal instances. Bare-metal instances offered by other cloud providers are not supported.</simpara>
<itemizedlist>
<title>Cluster requirements</title>
<listitem>
<simpara>You have installed Red Hat OpenShift Container Platform 4.14 or later on the cluster where you are installing the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>Your cluster has at least one worker node.</simpara>
</listitem>
</itemizedlist>
<section xml:id="osc-resource-requirements_deploying-bare-metal">
<title>OpenShift sandboxed containers resource requirements</title>
<simpara>You must ensure that your cluster has sufficient resources.</simpara>
<simpara>OpenShift sandboxed containers lets users run workloads on their OpenShift Container Platform clusters inside a sandboxed runtime (Kata). Each pod is represented by a virtual machine (VM). Each VM runs in a QEMU process and hosts a <literal>kata-agent</literal> process that acts as a supervisor for managing container workloads, and the processes running in those containers. Two additional processes add more overhead:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>containerd-shim-kata-v2</literal> is used to communicate with the pod.</simpara>
</listitem>
<listitem>
<simpara><literal>virtiofsd</literal> handles host file system access on behalf of the guest.</simpara>
</listitem>
</itemizedlist>
<simpara>Each VM is configured with a default amount of memory. Additional memory is hot-plugged into the VM for containers that explicitly request memory.</simpara>
<simpara>A container running without a memory resource consumes free memory until the total memory used by the VM reaches the default allocation. The guest and its I/O buffers also consume memory.</simpara>
<simpara>If a container is given a specific amount of memory, then that memory is hot-plugged into the VM before the container starts.</simpara>
<simpara>When a memory limit is specified, the workload is terminated if it consumes more memory than the limit. If no memory limit is specified, the kernel running on the VM might run out of memory. If the kernel runs out of memory, it might terminate other processes on the VM.</simpara>
<formalpara>
<title>Default memory sizes</title>
<para>The following table lists some the default values for resource allocation.</para>
</formalpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Resource</entry>
<entry align="left" valign="top">Value</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Memory allocated by default to a virtual machine</simpara></entry>
<entry align="left" valign="top"><simpara>2Gi</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Guest Linux kernel memory usage at boot</simpara></entry>
<entry align="left" valign="top"><simpara>~110Mi</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Memory used by the QEMU process (excluding VM memory)</simpara></entry>
<entry align="left" valign="top"><simpara>~30Mi</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Memory used by the <literal>virtiofsd</literal> process (excluding VM I/O buffers)</simpara></entry>
<entry align="left" valign="top"><simpara>~10Mi</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Memory used by the <literal>containerd-shim-kata-v2</literal> process</simpara></entry>
<entry align="left" valign="top"><simpara>~20Mi</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>File buffer cache data after running <literal>dnf install</literal> on Fedora</simpara></entry>
<entry align="left" valign="top"><simpara>~300Mi* <superscript>[1]</superscript></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>File buffers appear and are accounted for in multiple locations:</simpara>
<itemizedlist>
<listitem>
<simpara>In the guest where it appears as file buffer cache.</simpara>
</listitem>
<listitem>
<simpara>In the <literal>virtiofsd</literal> daemon that maps allowed user-space file I/O operations.</simpara>
</listitem>
<listitem>
<simpara>In the QEMU process as guest memory.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Total memory usage is properly accounted for by the memory utilization metrics, which only count that memory once.</simpara>
</note>
<simpara><link xlink:href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">Pod overhead</link> describes the amount of system resources that a pod on a node uses. You can get the current pod overhead for the Kata runtime by using <literal>oc describe runtimeclass kata</literal> as shown below.</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe runtimeclass kata</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">kind: RuntimeClass
apiVersion: node.k8s.io/v1
metadata:
  name: kata
overhead:
  podFixed:
    memory: "500Mi"
    cpu: "500m"</programlisting>
</para>
</formalpara>
<simpara>You can change the pod overhead by changing the <literal>spec.overhead</literal> field for a <literal>RuntimeClass</literal>. For example, if the configuration that you run for your containers consumes more than 350Mi of memory for the QEMU process and guest kernel data, you can alter the <literal>RuntimeClass</literal> overhead to suit your needs.</simpara>
<note>
<simpara>The specified default overhead values are supported by Red Hat. Changing default overhead values is not supported and can result in technical issues.</simpara>
</note>
<simpara>When performing any kind of file system I/O in the guest, file buffers are allocated in the guest kernel. The file buffers are also mapped in the QEMU process on the host, as well as in the <literal>virtiofsd</literal> process.</simpara>
<simpara>For example, if you use 300Mi of file buffer cache in the guest, both QEMU and <literal>virtiofsd</literal> appear to use 300Mi additional memory. However, the same memory is used in all three cases. Therefore, the total memory usage is only 300Mi, mapped in three different places. This is correctly accounted for when reporting the memory utilization metrics.</simpara>
</section>
<section xml:id="deploying-osc-web_metal-web">
<title>Deploying OpenShift sandboxed containers by using the web console</title>
<simpara>You can deploy OpenShift sandboxed containers on bare metal by using the OpenShift Container Platform web console to perform the following tasks:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>Optional: Install the Node Feature Discovery (NFD) Operator to configure node eligibility checks. For more information, see <link linkend="about-node-eligibility-checks_about-osc">node eligibility checks</link> and the <link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/specialized_hardware_and_driver_enablement/index#about-node-feature-discovery-operator_node-feature-discovery-operator">NFD Operator documentation</link>.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>Configure the OpenShift sandboxed containers workload objects.</simpara>
</listitem>
</orderedlist>
<section xml:id="installing-operator-web-console_metal-web">
<title>Installing the OpenShift sandboxed containers Operator</title>
<simpara>You can install the OpenShift sandboxed containers Operator by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Filter by keyword</emphasis> field, type <literal>OpenShift sandboxed containers</literal>.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">OpenShift sandboxed containers Operator</emphasis> tile and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, select <emphasis role="strong">stable</emphasis> from the list of available <emphasis role="strong">Update Channel</emphasis> options.</simpara>
</listitem>
<listitem>
<simpara>Verify that <emphasis role="strong">Operator recommended Namespace</emphasis> is selected for <emphasis role="strong">Installed Namespace</emphasis>. This installs the Operator in the mandatory <literal>openshift-sandboxed-containers-operator</literal> namespace. If this namespace does not yet exist, it is automatically created.</simpara>
<note>
<simpara>Attempting to install the OpenShift sandboxed containers Operator in a namespace other than <literal>openshift-sandboxed-containers-operator</literal> causes the installation to fail.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify that <emphasis role="strong">Automatic</emphasis> is selected for <emphasis role="strong">Approval Strategy</emphasis>. <emphasis role="strong">Automatic</emphasis> is the default value, and enables automatic updates to OpenShift sandboxed containers when a new z-stream release is available.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis> to verify that the Operator is installed.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</link> for disconnected environments.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-kataconfig-cr-web_metal-web">
<title>Creating the KataConfig custom resource</title>
<simpara>You must create the <literal>KataConfig</literal> custom resource (CR) to install <literal>kata</literal> as a <literal>RuntimeClass</literal> on your worker nodes.</simpara>
<simpara>The <literal>kata</literal> runtime class is installed on all worker nodes by default. If you want to install <literal>kata</literal> on specific nodes, you can add labels to those nodes and then define the label in the <literal>KataConfig</literal> CR.</simpara>
<simpara>OpenShift sandboxed containers installs <literal>kata</literal> as a <emphasis>secondary, optional</emphasis> runtime on the cluster and not as the primary runtime.</simpara>
<important>
<simpara>Creating the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. The following factors might increase the reboot time:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard disk drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Optional: You have installed the Node Feature Discovery Operator if you want to enable node eligibility checks.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">KataConfig</emphasis> tab, click <emphasis role="strong">Create KataConfig</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the following details:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Name</emphasis>: Optional: The default name is <literal>example-kataconfig</literal>.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Labels</emphasis>: Optional: Enter any relevant, identifying attributes to the <literal>KataConfig</literal> resource. Each label represents a key-value pair.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">checkNodeEligibility</emphasis>: Optional: Select to use the Node Feature Discovery Operator (NFD) to detect node eligibility.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">kataConfigPoolSelector</emphasis>. Optional: To install <literal>kata</literal> on selected nodes, add a match expression for the labels on the selected nodes:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Expand the <emphasis role="strong">kataConfigPoolSelector</emphasis> area.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">kataConfigPoolSelector</emphasis> area, expand <emphasis role="strong">matchExpressions</emphasis>. This is a list of label selector requirements.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add matchExpressions</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Key</emphasis> field, enter the label key the selector applies to.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Operator</emphasis> field, enter the key’s relationship to the label values. Valid operators are <literal>In</literal>, <literal>NotIn</literal>, <literal>Exists</literal>, and <literal>DoesNotExist</literal>.</simpara>
</listitem>
<listitem>
<simpara>Expand the <emphasis role="strong">Values</emphasis> area and then click <emphasis role="strong">Add value</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Value</emphasis> field, enter <literal>true</literal> or <literal>false</literal> for <emphasis role="strong">key</emphasis> label value.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">logLevel</emphasis>: Define the level of log data retrieved for nodes with the <literal>kata</literal> runtime class.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>. The <literal>KataConfig</literal> CR is created and installs the <literal>kata</literal> runtime class on the worker nodes.</simpara>
<simpara>Wait for the <literal>kata</literal> installation to complete and the worker nodes to reboot before verifying the installation.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>On the <emphasis role="strong">KataConfig</emphasis> tab, click the <literal>KataConfig</literal> CR to view its details.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">YAML</emphasis> tab to view the <literal>status</literal> stanza.</simpara>
<simpara>The <literal>status</literal> stanza contains the <literal>conditions</literal> and <literal>kataNodes</literal> keys. The value of <literal>status.kataNodes</literal> is an array of nodes, each of which lists nodes in a particular state of <literal>kata</literal> installation. A message appears each time there is an update.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Reload</emphasis> to refresh the YAML.</simpara>
<simpara>When all workers in the <literal>status.kataNodes</literal> array display the values <literal>installed</literal> and <literal>conditions.InProgress: False</literal> with no specified reason, the <literal>kata</literal> is installed on the cluster.</simpara>
</listitem>
</orderedlist>
<bridgehead xml:id="additional_resources" role="_additional-resources" renderas="sect4" remap="_additional_resources">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara><link linkend="kataconfig-status-messages">KataConfig status messages</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-workload-objects_metal-web">
<title>Configuring workload objects</title>
<simpara>You must configure OpenShift sandboxed containers workload objects by setting <literal>kata</literal> as the runtime class for the following pod-templated objects:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Pod</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicaSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicationController</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>StatefulSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>Deployment</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>DeploymentConfig</literal> objects</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created the <literal>KataConfig</literal> custom resource (CR).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Workloads</emphasis> → workload type, for example, <emphasis role="strong">Pods</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the workload type page, click an object to view its details.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">YAML</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Add <literal>spec.runtimeClassName: kata</literal> to the manifest of each pod-templated workload object as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata
# ...</programlisting>
<simpara>OpenShift Container Platform creates the workload object and begins scheduling it.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Inspect the <literal>spec.runtimeClassName</literal> field of a pod-templated object. If the value is <literal>kata</literal>, then the workload is running on OpenShift sandboxed containers, using peer pods.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="deploying-osc-cli_metal-cli">
<title>Deploying OpenShift sandboxed containers by using the command line</title>
<simpara>You can deploy OpenShift sandboxed containers on bare metal by using the command line interface (CLI) to perform the following tasks:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>After installing the Operator, you can configure the following options:</simpara>
<itemizedlist>
<listitem>
<simpara>Configure a block storage device.</simpara>
</listitem>
<listitem>
<simpara>Install the Node Feature Discovery (NFD) Operator to configure node eligibility checks. For more information, see <link linkend="about-node-eligibility-checks_about-osc">node eligibility checks</link> and the <link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/specialized_hardware_and_driver_enablement/index#about-node-feature-discovery-operator_node-feature-discovery-operator">NFD Operator documentation</link>.</simpara>
<itemizedlist>
<listitem>
<simpara>Create a <literal>NodeFeatureDiscovery</literal> custom resource.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>Optional: Modify the pod overhead.</simpara>
</listitem>
<listitem>
<simpara>Configure the OpenShift sandboxed containers workload objects.</simpara>
</listitem>
</orderedlist>
<section xml:id="installing-operator-cli_metal-cli">
<title>Installing the OpenShift sandboxed containers Operator</title>
<simpara>You can install the OpenShift sandboxed containers Operator by using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>osc-namespace.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sandboxed-containers-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-namespace.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>osc-operatorgroup.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sandboxed-containers-operator-group
  namespace: openshift-sandboxed-containers-operator
spec:
  targetNamespaces:
  - openshift-sandboxed-containers-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the operator group by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-operatorgroup.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>osc-subscription.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sandboxed-containers-operator
  namespace: openshift-sandboxed-containers-operator
spec:
  channel: stable
  installPlanApproval: Automatic
  name: sandboxed-containers-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: sandboxed-containers-operator.v1.8.0</programlisting>
</listitem>
<listitem>
<simpara>Create the subscription by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-subscription.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the Operator is correctly installed by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-sandboxed-containers-operator</programlisting>
<simpara>This command can take several minutes to complete.</simpara>
</listitem>
<listitem>
<simpara>Watch the process by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch oc get csv -n openshift-sandboxed-containers-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                             DISPLAY                                  VERSION             REPLACES                   PHASE
openshift-sandboxed-containers   openshift-sandboxed-containers-operator  1.8.0    1.7.0        Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</link> for disconnected environments.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="optional_configurations" remap="_optional_configurations">
<title>Optional configurations</title>
<simpara>You can configure the following options after you install the OpenShift sandboxed containers Operator.</simpara>
<section xml:id="using-osc-local-volume_metal-cli">
<title>Provisioning local block volumes</title>
<simpara>You can use local block volumes with OpenShift sandboxed containers. You must first provision the local block volumes by using the Local Storage Operator (LSO). Then you must enable the nodes with the local block volumes to run OpenShift sandboxed containers workloads.</simpara>
<simpara>You can provision local block volumes for OpenShift sandboxed containers by using the Local Storage Operator (LSO). The local volume provisioner looks for any block volume devices at the paths specified in the defined resource.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the Local Storage Operator.</simpara>
</listitem>
<listitem>
<simpara>You have a local disk that meets the following conditions:</simpara>
<itemizedlist>
<listitem>
<simpara>It is attached to a node.</simpara>
</listitem>
<listitem>
<simpara>It is not mounted.</simpara>
</listitem>
<listitem>
<simpara>It does not contain partitions.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the local volume resource. This resource must define the nodes and paths to the local volumes.</simpara>
<note>
<simpara>Do not use different storage class names for the same device. Doing so creates multiple persistent volumes (PVs).</simpara>
</note>
<formalpara>
<title>Example: Block</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-disks"
  namespace: "openshift-local-storage" <co xml:id="CO2-1"/>
spec:
  nodeSelector: <co xml:id="CO2-2"/>
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - ip-10-0-136-143
          - ip-10-0-140-255
          - ip-10-0-144-180
  storageClassDevices:
    - storageClassName: "local-sc" <co xml:id="CO2-3"/>
      forceWipeDevicesAndDestroyAllData: false <co xml:id="CO2-4"/>
      volumeMode: Block
      devicePaths: <co xml:id="CO2-5"/>
        - /path/to/device <co xml:id="CO2-6"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO2-1">
<para>The namespace where the Local Storage Operator is installed.</para>
</callout>
<callout arearefs="CO2-2">
<para>Optional: A node selector containing a list of nodes where the local storage volumes are attached. This example uses the node hostnames, obtained from <literal>oc get node</literal>. If a value is not defined, then the Local Storage Operator will attempt to find matching disks on all available nodes.</para>
</callout>
<callout arearefs="CO2-3">
<para>The name of the storage class to use when creating persistent volume objects.</para>
</callout>
<callout arearefs="CO2-4">
<para>This setting defines whether or not to call <literal>wipefs</literal>, which removes partition table signatures (magic strings) making the disk ready to use for Local Storage Operator provisioning. No other data besides signatures is erased. The default is "false" (<literal>wipefs</literal> is not invoked). Setting <literal>forceWipeDevicesAndDestroyAllData</literal> to "true" can be useful in scenarios where previous data can remain on disks that need to be re-used. In these scenarios, setting this field to true eliminates the need for administrators to erase the disks manually.</para>
</callout>
<callout arearefs="CO2-5">
<para>The path containing a list of local storage devices to choose from. You must use this path when enabling a node with a local block device to run OpenShift sandboxed containers workloads.</para>
</callout>
<callout arearefs="CO2-6">
<para>Replace this value with the filepath to your <literal>LocalVolume</literal> resource <literal>by-id</literal>, such as <literal>/dev/disk/by-id/wwn</literal>. PVs are created for these local disks when the provisioner is deployed successfully.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the local volume resource in your OpenShift Container Platform cluster. Specify the file you just created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;local-volume&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the provisioner was created and that the corresponding daemon sets were created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-local-storage</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                          READY   STATUS    RESTARTS   AGE
pod/diskmaker-manager-9wzms                   1/1     Running   0          5m43s
pod/diskmaker-manager-jgvjp                   1/1     Running   0          5m43s
pod/diskmaker-manager-tbdsj                   1/1     Running   0          5m43s
pod/local-storage-operator-7db4bd9f79-t6k87   1/1     Running   0          14m

NAME                                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
service/local-storage-operator-metrics   ClusterIP   172.30.135.36   &lt;none&gt;        8383/TCP,8686/TCP   14m

NAME                               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/diskmaker-manager   3         3         3       3            3           &lt;none&gt;          5m43s

NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/local-storage-operator   1/1     1            1           14m

NAME                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/local-storage-operator-7db4bd9f79   1         1         1       14m</programlisting>
</para>
</formalpara>
<simpara>Note the <literal>desired</literal> and <literal>current</literal> number of daemon set processes. A <literal>desired</literal> count of <literal>0</literal> indicates that the label selectors were invalid.</simpara>
</listitem>
<listitem>
<simpara>Verify that the persistent volumes were created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pv</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
local-pv-1cec77cf   100Gi      RWO            Delete           Available           local-sc                88m
local-pv-2ef7cd2a   100Gi      RWO            Delete           Available           local-sc                82m
local-pv-3fa1c73    100Gi      RWO            Delete           Available           local-sc                48m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<important>
<simpara>Editing the <literal>LocalVolume</literal> object does not change existing persistent volumes because doing so might result in a destructive operation.</simpara>
</important>
</section>
<section xml:id="deploy-nodes-block-device_metal-cli">
<title>Enabling nodes to use a local block device</title>
<simpara>You can configure nodes with a local block device to run OpenShift sandboxed containers workloads at the paths specified in the defined volume resource.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You provisioned a block device using the Local Storage Operator (LSO).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Enable each node with a local block device to run OpenShift sandboxed containers workloads by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/worker-0 -- chcon -vt container_file_t /host/path/to/device</programlisting>
<simpara>The <literal>/path/to/device</literal> must be the same path you defined when creating the local storage resource.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">system_u:object_r:container_file_t:s0 /host/path/to/device</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-nfd-cr_metal-cli">
<title>Creating a NodeFeatureDiscovery custom resource</title>
<simpara>You create a <literal>NodeFeatureDiscovery</literal> custom resource (CR) to define the configuration parameters that the Node Feature Discovery (NFD) Operator checks to determine that the worker nodes can support OpenShift sandboxed containers.</simpara>
<note>
<simpara>To install the <literal>kata</literal> runtime on only selected worker nodes that you know are eligible, apply the <literal>feature.node.kubernetes.io/runtime.kata=true</literal> label to the selected nodes and set <literal>checkNodeEligibility: true</literal> in the <literal>KataConfig</literal> CR.</simpara>
<simpara>To install the <literal>kata</literal> runtime on all worker nodes, set <literal>checkNodeEligibility: false</literal> in the <literal>KataConfig</literal> CR.</simpara>
<simpara>In both these scenarios, you do not need to create the <literal>NodeFeatureDiscovery</literal> CR. You should only apply the <literal>feature.node.kubernetes.io/runtime.kata=true</literal> label manually if you are sure that the node is eligible to run OpenShift sandboxed containers.</simpara>
</note>
<simpara>The following procedure applies the <literal>feature.node.kubernetes.io/runtime.kata=true</literal> label to all eligible nodes and configures the <literal>KataConfig</literal> resource to check for node eligibility.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the NFD Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>nfd.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nfd.openshift.io/v1
kind: NodeFeatureDiscovery
metadata:
  name: nfd-kata
  namespace: openshift-nfd
spec:
  workerConfig:
    configData: |
      sources:
        custom:
          - name: "feature.node.kubernetes.io/runtime.kata"
            matchOn:
              - cpuId: ["SSE4", "VMX"]
                loadedKMod: ["kvm", "kvm_intel"]
              - cpuId: ["SSE4", "SVM"]
                loadedKMod: ["kvm", "kvm_amd"]
# ...</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>NodeFeatureDiscovery</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f nfd.yaml</programlisting>
<simpara>The <literal>NodeFeatureDiscovery</literal> CR applies the <literal>feature.node.kubernetes.io/runtime.kata=true</literal> label to all qualifying worker nodes.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a <literal>kata-config.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  checkNodeEligibility: true</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f kata-config.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that qualifying nodes in the cluster have the correct label applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes --selector='feature.node.kubernetes.io/runtime.kata=true'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           STATUS                     ROLES    AGE     VERSION
compute-3.example.com          Ready                      worker   4h38m   v1.25.0
compute-2.example.com          Ready                      worker   4h35m   v1.25.0</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="creating-kataconfig-cr-cli_metal-cli">
<title>Creating the KataConfig custom resource</title>
<simpara>You must create the <literal>KataConfig</literal> custom resource (CR) to install <literal>kata</literal> as a runtime class on your worker nodes.</simpara>
<simpara>Creating the <literal>KataConfig</literal> CR triggers the OpenShift sandboxed containers Operator to do the following:
* Install the needed RHCOS extensions, such as QEMU and <literal>kata-containers</literal>, on your RHCOS node.
* Ensure that the <link xlink:href="https://github.com/cri-o/cri-o">CRI-O</link> runtime is configured with the correct runtime handlers.
* Create a <literal>RuntimeClass</literal> CR named <literal>kata</literal> with a default configuration. This enables users to configure workloads to use <literal>kata</literal> as the runtime by referencing the CR in the <literal>RuntimeClassName</literal> field. This CR also specifies the resource overhead for the runtime.</simpara>
<simpara>OpenShift sandboxed containers installs <literal>kata</literal> as a <emphasis>secondary, optional</emphasis> runtime on the cluster and not as the primary runtime.</simpara>
<important>
<simpara>Creating the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard disk drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Optional: You have installed the Node Feature Discovery Operator if you want to enable node eligibility checks.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>example-kataconfig.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  checkNodeEligibility: false <co xml:id="CO3-1"/>
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <co xml:id="CO3-2"/></programlisting>
<calloutlist>
<callout arearefs="CO3-1">
<para>Optional: Set`checkNodeEligibility` to <literal>true</literal> to run node eligibility checks.</para>
</callout>
<callout arearefs="CO3-2">
<para>Optional: If you have applied node labels to install OpenShift sandboxed containers on specific nodes, specify the key and value.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-kataconfig.yaml</programlisting>
<simpara>The new <literal>KataConfig</literal> CR is created and installs <literal>kata</literal> as a runtime class on the worker nodes.</simpara>
<simpara>Wait for the <literal>kata</literal> installation to complete and the worker nodes to reboot before verifying the installation.</simpara>
</listitem>
<listitem>
<simpara>Monitor the installation progress by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</programlisting>
<simpara>When the status of all workers under <literal>kataNodes</literal> is <literal>installed</literal> and the condition <literal>InProgress</literal> is <literal>False</literal> without specifying a reason, the <literal>kata</literal> is installed on the cluster.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="modifying-pod-overhead.adoc_metal-cli">
<title>Modifying pod overhead</title>
<simpara><link xlink:href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">Pod overhead</link> describes the amount of system resources that a pod on a node uses.
You can modify the pod overhead by changing the <literal>spec.overhead</literal> field for a <literal>RuntimeClass</literal> custom resource. For example, if the configuration that you run for your containers consumes more than 350Mi of memory for the QEMU process and guest kernel data, you can alter the <literal>RuntimeClass</literal> overhead to suit your needs.</simpara>
<simpara>When performing any kind of file system I/O in the guest, file buffers are allocated in the guest kernel. The file buffers are also mapped in the QEMU process on the host, as well as in the <literal>virtiofsd</literal> process.</simpara>
<simpara>For example, if you use 300Mi of file buffer cache in the guest, both QEMU and <literal>virtiofsd</literal> appear to use 300Mi additional memory. However, the same memory is being used in all three cases. Therefore, the total memory usage is only 300Mi, mapped in three different places. This is correctly accounted for when reporting the memory utilization metrics.</simpara>
<note>
<simpara>The default values are supported by Red Hat. Changing default overhead values is not supported and can result in technical issues.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the <literal>RuntimeClass</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe runtimeclass kata</programlisting>
</listitem>
<listitem>
<simpara>Update the <literal>overhead.podFixed.memory</literal> and <literal>cpu</literal> values and save as the file as <literal>runtimeclass.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: RuntimeClass
apiVersion: node.k8s.io/v1
metadata:
  name: kata
overhead:
  podFixed:
    memory: "500Mi"
    cpu: "500m"</programlisting>
</listitem>
<listitem>
<simpara>Apply the changes by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f runtimeclass.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-workload-objects_metal-cli">
<title>Configuring workload objects</title>
<simpara>You must configure OpenShift sandboxed containers workload objects by setting <literal>kata</literal> as the runtime class for the following pod-templated objects:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Pod</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicaSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicationController</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>StatefulSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>Deployment</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>DeploymentConfig</literal> objects</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created the <literal>KataConfig</literal> custom resource (CR).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add <literal>spec.runtimeClassName: kata</literal> to the manifest of each pod-templated workload object as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata
# ...</programlisting>
<simpara>OpenShift Container Platform creates the workload object and begins scheduling it.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Inspect the <literal>spec.runtimeClassName</literal> field of a pod-templated object. If the value is <literal>kata</literal>, then the workload is running on OpenShift sandboxed containers, using peer pods.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="deploying-aws">
<title>Deploying OpenShift sandboxed containers on AWS</title>
<simpara>You can deploy OpenShift sandboxed containers on AWS Cloud Computing Services by using the OpenShift Container Platform web console or the command line interface (CLI).</simpara>
<simpara>OpenShift sandboxed containers deploys peer pods. The peer pod design circumvents the need for nested virtualization. For more information, see <link linkend="peer-pods">peer pods</link>.</simpara>
<itemizedlist>
<title>Cluster requirements</title>
<listitem>
<simpara>You have installed Red Hat OpenShift Container Platform 4.14 or later on the cluster where you are installing the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>Your cluster has at least one worker node.</simpara>
</listitem>
</itemizedlist>
<section xml:id="peer-pod-resource-requirements_aws">
<title>Peer pod resource requirements</title>
<simpara>You must ensure that your cluster has sufficient resources.</simpara>
<simpara>Peer pod virtual machines (VMs) require resources in two locations:</simpara>
<itemizedlist>
<listitem>
<simpara>The worker node. The worker node stores metadata, Kata shim resources (<literal>containerd-shim-kata-v2</literal>), remote-hypervisor resources (<literal>cloud-api-adaptor</literal>), and the tunnel setup between the worker nodes and the peer pod VM.</simpara>
</listitem>
<listitem>
<simpara>The cloud instance. This is the actual peer pod VM running in the cloud.</simpara>
</listitem>
</itemizedlist>
<simpara>The CPU and memory resources used in the Kubernetes worker node are handled by the <link xlink:href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</link> included in the RuntimeClass (<literal>kata-remote</literal>) definition used for creating peer pods.</simpara>
<simpara>The total number of peer pod VMs running in the cloud is defined as Kubernetes Node extended resources.
This limit is per node and is set by the <literal>limit</literal> attribute in the <literal>peerpodConfig</literal> custom resource (CR).</simpara>
<simpara>The <literal>peerpodConfig</literal> CR, named <literal>peerpodconfig-openshift</literal>, is created when you create the <literal>kataConfig</literal> CR and enable peer pods, and is located in the <literal>openshift-sandboxed-containers-operator</literal> namespace.</simpara>
<simpara>The following <literal>peerpodConfig</literal> CR example displays the default <literal>spec</literal> values:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: confidentialcontainers.org/v1alpha1
kind: PeerPodConfig
metadata:
  name: peerpodconfig-openshift
  namespace: openshift-sandboxed-containers-operator
spec:
  cloudSecretName: peer-pods-secret
  configMapName: peer-pods-cm
  limit: "10" <co xml:id="CO4-1"/>
  nodeSelector:
    node-role.kubernetes.io/kata-oc: ""</programlisting>
<calloutlist>
<callout arearefs="CO4-1">
<para>The default limit is 10 VMs per node.</para>
</callout>
</calloutlist>
<simpara>The extended resource is named <literal>kata.peerpods.io/vm</literal>, and enables the Kubernetes scheduler to handle capacity tracking and accounting.</simpara>
<simpara>You can edit the limit per node based on the requirements for your environment after you install the OpenShift sandboxed containers Operator.</simpara>
<simpara>A <link xlink:href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">mutating webhook</link> adds the extended resource <literal>kata.peerpods.io/vm</literal> to the pod specification. It also removes any resource-specific entries from the pod specification, if present. This enables the Kubernetes scheduler to account for these extended resources, ensuring the peer pod is only scheduled when resources are available.</simpara>
<simpara>The mutating webhook modifies a Kubernetes pod as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>The mutating webhook checks the pod for the expected <literal>RuntimeClassName</literal> value, specified in the <literal>TARGET_RUNTIME_CLASS</literal> environment variable. If the value in the pod specification does not match the value in the <literal>TARGET_RUNTIME_CLASS</literal>, the webhook exits without modifying the pod.</simpara>
</listitem>
<listitem>
<simpara>If the <literal>RuntimeClassName</literal> values match, the webhook makes the following changes to the pod spec:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The webhook removes every resource specification from the <literal>resources</literal> field of all containers and init containers in the pod.</simpara>
</listitem>
<listitem>
<simpara>The webhook adds the extended resource (<literal>kata.peerpods.io/vm</literal>) to the spec by modifying the resources field of the first container in the pod. The extended resource <literal>kata.peerpods.io/vm</literal> is used by the Kubernetes scheduler for accounting purposes.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<note>
<simpara>The mutating webhook excludes specific system namespaces in OpenShift Container Platform from mutation. If a peer pod is created in those system namespaces, then resource accounting using Kubernetes extended resources does not work unless the pod spec includes the extended resource.</simpara>
<simpara>As a best practice, define a cluster-wide policy to only allow peer pod creation in specific namespaces.</simpara>
</note>
</section>
<section xml:id="deploying-osc-web_aws-web">
<title>Deploying OpenShift sandboxed containers by using the web console</title>
<simpara>You can deploy OpenShift sandboxed containers on AWS by using the OpenShift Container Platform web console to perform the following tasks:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>Enable ports 15150 and 9000 to allow internal communication with peer pods.</simpara>
</listitem>
<listitem>
<simpara>Create the peer pods secret.</simpara>
</listitem>
<listitem>
<simpara>Create the peer pods config map.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>Configure the OpenShift sandboxed containers workload objects.</simpara>
</listitem>
</orderedlist>
<section xml:id="installing-operator-web-console_aws-web">
<title>Installing the OpenShift sandboxed containers Operator</title>
<simpara>You can install the OpenShift sandboxed containers Operator by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Filter by keyword</emphasis> field, type <literal>OpenShift sandboxed containers</literal>.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">OpenShift sandboxed containers Operator</emphasis> tile and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, select <emphasis role="strong">stable</emphasis> from the list of available <emphasis role="strong">Update Channel</emphasis> options.</simpara>
</listitem>
<listitem>
<simpara>Verify that <emphasis role="strong">Operator recommended Namespace</emphasis> is selected for <emphasis role="strong">Installed Namespace</emphasis>. This installs the Operator in the mandatory <literal>openshift-sandboxed-containers-operator</literal> namespace. If this namespace does not yet exist, it is automatically created.</simpara>
<note>
<simpara>Attempting to install the OpenShift sandboxed containers Operator in a namespace other than <literal>openshift-sandboxed-containers-operator</literal> causes the installation to fail.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify that <emphasis role="strong">Automatic</emphasis> is selected for <emphasis role="strong">Approval Strategy</emphasis>. <emphasis role="strong">Automatic</emphasis> is the default value, and enables automatic updates to OpenShift sandboxed containers when a new z-stream release is available.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis> to verify that the Operator is installed.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</link> for disconnected environments.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="aws-enabling-ports_aws-web">
<title>Enabling ports for AWS</title>
<simpara>You must enable ports 15150 and 9000 to allow internal communication with peer pods running on AWS.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>You have installed the AWS command line tool.</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to your OpenShift Container Platform cluster and retrieve the instance ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ INSTANCE_ID=$(oc get nodes -l 'node-role.kubernetes.io/worker' \
  -o jsonpath='{.items[0].spec.providerID}' | sed 's#[^ ]*/##g')</programlisting>
</listitem>
<listitem>
<simpara>Retrieve the AWS region:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_REGION=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.aws.region}')</programlisting>
</listitem>
<listitem>
<simpara>Retrieve the security group IDs and store them in an array:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_SG_IDS=($(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} \
  --query 'Reservations[*].Instances[*].SecurityGroups[*].GroupId' \
  --output text --region $AWS_REGION))</programlisting>
</listitem>
<listitem>
<simpara>For each security group ID, authorize the peer pods shim to access kata-agent communication, and set up the peer pods tunnel:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for AWS_SG_ID in "${AWS_SG_IDS[@]}"; do \
  aws ec2 authorize-security-group-ingress --group-id $AWS_SG_ID --protocol tcp --port 15150 --source-group $AWS_SG_ID --region $AWS_REGION \
  aws ec2 authorize-security-group-ingress --group-id $AWS_SG_ID --protocol tcp --port 9000 --source-group $AWS_SG_ID --region $AWS_REGION \
done</programlisting>
</listitem>
</orderedlist>
<simpara>The ports are now enabled.</simpara>
</section>
<section xml:id="creating-peer-pods-secret_aws-web">
<title>Creating the peer pods secret</title>
<simpara>You must create the peer pods secret for OpenShift sandboxed containers.</simpara>
<simpara>The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.</simpara>
<simpara>By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have the following values generated by using the AWS console:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>AWS_ACCESS_KEY_ID</literal></simpara>
</listitem>
<listitem>
<simpara><literal>AWS_SECRET_ACCESS_KEY</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the OpenShift sandboxed containers Operator tile.</simpara>
</listitem>
<listitem>
<simpara>Click the Import icon (<emphasis role="strong">+</emphasis>) on the top right corner.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Import YAML</emphasis> window, paste the following YAML manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  AWS_ACCESS_KEY_ID: "&lt;aws_access_key&gt;" <co xml:id="CO5-1"/>
  AWS_SECRET_ACCESS_KEY: "&lt;aws_secret_access_key&gt;" <co xml:id="CO5-2"/></programlisting>
<calloutlist>
<callout arearefs="CO5-1">
<para>Specify the <literal>AWS_ACCESS_KEY_ID</literal> value.</para>
</callout>
<callout arearefs="CO5-2">
<para>Specify the <literal>AWS_SECRET_ACCESS_KEY</literal> value.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis> to apply the changes.</simpara>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Workloads</emphasis> → <emphasis role="strong">Secrets</emphasis> to verify the peer pods secret.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-peer-pods-config-map_aws-web">
<title>Creating the peer pods config map</title>
<simpara>You must create the peer pods config map for OpenShift sandboxed containers.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have your Amazon Machine Image (AMI) ID if you are not using the default AMI ID based on your cluster credentials.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the following values from your AWS instance:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Retrieve and record the instance ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ INSTANCE_ID=$(oc get nodes -l 'node-role.kubernetes.io/worker' -o jsonpath='{.items[0].spec.providerID}' | sed 's#[^ ]*/##g')</programlisting>
<simpara>This is used to retrieve other values for the secret object.</simpara>
</listitem>
<listitem>
<simpara>Retrieve and record the AWS region:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_REGION=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.aws.region}') &amp;&amp; echo "AWS_REGION: \"$AWS_REGION\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the AWS subnet ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_SUBNET_ID=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].SubnetId' --region ${AWS_REGION} --output text) &amp;&amp; echo "AWS_SUBNET_ID: \"$AWS_SUBNET_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the AWS VPC ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_VPC_ID=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].VpcId' --region ${AWS_REGION} --output text) &amp;&amp; echo "AWS_VPC_ID: \"$AWS_VPC_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the AWS security group IDs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_SG_IDS=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].SecurityGroups[*].GroupId' --region  $AWS_REGION --output json | jq -r '.[][][]' | paste -sd ",") &amp;&amp; echo "AWS_SG_IDS: \"$AWS_SG_IDS\""</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the OpenShift sandboxed containers Operator from the list of operators.</simpara>
</listitem>
<listitem>
<simpara>Click the Import icon (<emphasis role="strong">+</emphasis>) in the top right corner.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Import YAML</emphasis> window, paste the following YAML manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "aws"
  VXLAN_PORT: "9000"
  PODVM_INSTANCE_TYPE: "t3.medium" <co xml:id="CO6-1"/>
  PODVM_INSTANCE_TYPES: "t2.small,t2.medium,t3.large" <co xml:id="CO6-2"/>
  PROXY_TIMEOUT: "5m"
  PODVM_AMI_ID: "&lt;podvm_ami_id&gt;" <co xml:id="CO6-3"/>
  AWS_REGION: "&lt;aws_region&gt;" <co xml:id="CO6-4"/>
  AWS_SUBNET_ID: "&lt;aws_subnet_id&gt;" <co xml:id="CO6-5"/>
  AWS_VPC_ID: "&lt;aws_vpc_id&gt;" <co xml:id="CO6-6"/>
  AWS_SG_IDS: "&lt;aws_sg_ids&gt;" <co xml:id="CO6-7"/>
  DISABLECVM: "true"</programlisting>
<calloutlist>
<callout arearefs="CO6-1">
<para>Defines the default instance type that is used when a type is not defined in the workload.</para>
</callout>
<callout arearefs="CO6-2">
<para>Lists all of the instance types you can specify when creating the pod. This allows you to define smaller instance types for workloads that need less memory and fewer CPUs or larger instance types for larger workloads.</para>
</callout>
<callout arearefs="CO6-3">
<para>Optional: By default, this value is populated when you run the <literal>KataConfig</literal> CR, using an AMI ID based on your cluster credentials. If you create your own AMI, specify the correct AMI ID.</para>
</callout>
<callout arearefs="CO6-4">
<para>Specify the <literal>AWS_REGION</literal> value you retrieved.</para>
</callout>
<callout arearefs="CO6-5">
<para>Specify the <literal>AWS_SUBNET_ID</literal> value you retrieved.</para>
</callout>
<callout arearefs="CO6-6">
<para>Specify the <literal>AWS_VPC_ID</literal> value you retrieved.</para>
</callout>
<callout arearefs="CO6-7">
<para>Specify the <literal>AWS_SG_IDS</literal> value you retrieved.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis> to apply the changes.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</programlisting>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Workloads</emphasis> → <emphasis role="strong">ConfigMaps</emphasis> to view the new config map.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-kataconfig-cr-web_aws-web">
<title>Creating the KataConfig custom resource</title>
<simpara>You must create the <literal>KataConfig</literal> custom resource (CR) to install <literal>kata-remote</literal> as a <literal>RuntimeClass</literal> on your worker nodes.</simpara>
<simpara>The <literal>kata-remote</literal> runtime class is installed on all worker nodes by default. If you want to install <literal>kata-remote</literal> on specific nodes, you can add labels to those nodes and then define the label in the <literal>KataConfig</literal> CR.</simpara>
<simpara>OpenShift sandboxed containers installs <literal>kata-remote</literal> as a <emphasis>secondary, optional</emphasis> runtime on the cluster and not as the primary runtime.</simpara>
<important>
<simpara>Creating the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. The following factors might increase the reboot time:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard disk drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Optional: You have installed the Node Feature Discovery Operator if you want to enable node eligibility checks.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">KataConfig</emphasis> tab, click <emphasis role="strong">Create KataConfig</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the following details:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Name</emphasis>: Optional: The default name is <literal>example-kataconfig</literal>.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Labels</emphasis>: Optional: Enter any relevant, identifying attributes to the <literal>KataConfig</literal> resource. Each label represents a key-value pair.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">enablePeerPods</emphasis>: Select for public cloud, IBM Z®, and IBM® LinuxONE deployments.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">kataConfigPoolSelector</emphasis>. Optional: To install <literal>kata-remote</literal> on selected nodes, add a match expression for the labels on the selected nodes:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Expand the <emphasis role="strong">kataConfigPoolSelector</emphasis> area.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">kataConfigPoolSelector</emphasis> area, expand <emphasis role="strong">matchExpressions</emphasis>. This is a list of label selector requirements.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add matchExpressions</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Key</emphasis> field, enter the label key the selector applies to.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Operator</emphasis> field, enter the key’s relationship to the label values. Valid operators are <literal>In</literal>, <literal>NotIn</literal>, <literal>Exists</literal>, and <literal>DoesNotExist</literal>.</simpara>
</listitem>
<listitem>
<simpara>Expand the <emphasis role="strong">Values</emphasis> area and then click <emphasis role="strong">Add value</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Value</emphasis> field, enter <literal>true</literal> or <literal>false</literal> for <emphasis role="strong">key</emphasis> label value.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">logLevel</emphasis>: Define the level of log data retrieved for nodes with the <literal>kata-remote</literal> runtime class.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>. The <literal>KataConfig</literal> CR is created and installs the <literal>kata-remote</literal> runtime class on the worker nodes.</simpara>
<simpara>Wait for the <literal>kata-remote</literal> installation to complete and the worker nodes to reboot before verifying the installation.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>On the <emphasis role="strong">KataConfig</emphasis> tab, click the <literal>KataConfig</literal> CR to view its details.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">YAML</emphasis> tab to view the <literal>status</literal> stanza.</simpara>
<simpara>The <literal>status</literal> stanza contains the <literal>conditions</literal> and <literal>kataNodes</literal> keys. The value of <literal>status.kataNodes</literal> is an array of nodes, each of which lists nodes in a particular state of <literal>kata-remote</literal> installation. A message appears each time there is an update.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Reload</emphasis> to refresh the YAML.</simpara>
<simpara>When all workers in the <literal>status.kataNodes</literal> array display the values <literal>installed</literal> and <literal>conditions.InProgress: False</literal> with no specified reason, the <literal>kata-remote</literal> is installed on the cluster.</simpara>
</listitem>
</orderedlist>
<bridgehead xml:id="additional_resources_2" role="_additional-resources" renderas="sect4" remap="_additional_resources_2">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara><link linkend="kataconfig-status-messages">KataConfig status messages</link></simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="verifying-pod-vm-image-creation_aws-web" renderas="sect4">Verifying the pod VM image</bridgehead>
<simpara>After <literal>kata-remote</literal> is installed on your cluster, the OpenShift sandboxed containers Operator creates a pod VM image, which is used to create peer pods.
This process can take a long time because the image is created on the cloud instance.
You can verify that the pod VM image was created successfully by checking the config map that you created for the cloud provider.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Workloads</emphasis> → <emphasis role="strong">ConfigMaps</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the provider config map to view its details.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">YAML</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Check the <literal>status</literal> stanza of the YAML file.</simpara>
<simpara>If the <literal>PODVM_AMI_ID</literal> parameter is populated, the pod VM image was created successfully.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Troubleshooting</title>
<listitem>
<simpara>Retrieve the events log by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get events -n openshift-sandboxed-containers-operator --field-selector involvedObject.name=osc-podvm-image-creation</programlisting>
</listitem>
<listitem>
<simpara>Retrieve the job log by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n openshift-sandboxed-containers-operator jobs/osc-podvm-image-creation</programlisting>
</listitem>
</orderedlist>
<simpara>If you cannot resolve the issue, submit a Red Hat Support case and attach the output of both logs.</simpara>
</section>
<section xml:id="configuring-workload-objects_aws-web">
<title>Configuring workload objects</title>
<simpara>You must configure OpenShift sandboxed containers workload objects by setting <literal>kata-remote</literal> as the runtime class for the following pod-templated objects:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Pod</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicaSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicationController</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>StatefulSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>Deployment</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>DeploymentConfig</literal> objects</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.</simpara>
</important>
<simpara>You can define whether the workload should be deployed using the default instance type, which you defined in the config map, by adding an annotation to the YAML file.</simpara>
<simpara>If you do not want to define the instance type manually, you can add an annotation to use an automatic instance type, based on the memory available.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created the <literal>KataConfig</literal> custom resource (CR).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Workloads</emphasis> → workload type, for example, <emphasis role="strong">Pods</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the workload type page, click an object to view its details.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">YAML</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Add <literal>spec.runtimeClassName: kata-remote</literal> to the manifest of each pod-templated workload object as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata-remote
# ...</programlisting>
</listitem>
<listitem>
<simpara>Add an annotation to the pod-templated object to use a manually defined instance type or an automatic instance type:</simpara>
<itemizedlist>
<listitem>
<simpara>To use a manually defined instance type, add the following annotation:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;object&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.machine_type: "t3.medium" <co xml:id="CO7-1"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO7-1">
<para>Specify the instance type that you defined in the config map.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To use an automatic instance type, add the following annotations:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;Pod&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.default_vcpus: &lt;vcpus&gt;
    io.katacontainers.config.hypervisor.default_memory: &lt;memory&gt;
# ...</programlisting>
<simpara>Define the amount of memory available for the workload to use.
The workload will run on an automatic instance type based on the amount of memory available.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis> to apply the changes.</simpara>
<simpara>OpenShift Container Platform creates the workload object and begins scheduling it.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Inspect the <literal>spec.runtimeClassName</literal> field of a pod-templated object. If the value is <literal>kata-remote</literal>, then the workload is running on OpenShift sandboxed containers, using peer pods.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="deploying-osc-cli_aws-cli">
<title>Deploying OpenShift sandboxed containers by using the command line</title>
<simpara>You can deploy OpenShift sandboxed containers on AWS by using the command line interface (CLI) to perform the following tasks:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>Optional: Change the number of virtual machines running on each worker node.</simpara>
</listitem>
<listitem>
<simpara>Enable ports 15150 and 9000 to allow internal communication with peer pods.</simpara>
</listitem>
<listitem>
<simpara>Create the peer pods secret.</simpara>
</listitem>
<listitem>
<simpara>Create the peer pods config map.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>Configure the OpenShift sandboxed containers workload objects.</simpara>
</listitem>
</orderedlist>
<section xml:id="installing-operator-cli_aws-cli">
<title>Installing the OpenShift sandboxed containers Operator</title>
<simpara>You can install the OpenShift sandboxed containers Operator by using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>osc-namespace.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sandboxed-containers-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-namespace.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>osc-operatorgroup.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sandboxed-containers-operator-group
  namespace: openshift-sandboxed-containers-operator
spec:
  targetNamespaces:
  - openshift-sandboxed-containers-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the operator group by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-operatorgroup.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>osc-subscription.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sandboxed-containers-operator
  namespace: openshift-sandboxed-containers-operator
spec:
  channel: stable
  installPlanApproval: Automatic
  name: sandboxed-containers-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: sandboxed-containers-operator.v1.8.0</programlisting>
</listitem>
<listitem>
<simpara>Create the subscription by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-subscription.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the Operator is correctly installed by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-sandboxed-containers-operator</programlisting>
<simpara>This command can take several minutes to complete.</simpara>
</listitem>
<listitem>
<simpara>Watch the process by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch oc get csv -n openshift-sandboxed-containers-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                             DISPLAY                                  VERSION             REPLACES                   PHASE
openshift-sandboxed-containers   openshift-sandboxed-containers-operator  1.8.0    1.7.0        Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</link> for disconnected environments.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="modifying-peer-pod-vm-limit_aws-cli">
<title>Modifying the number of peer pod VMs per node</title>
<simpara>You can change the limit of peer pod virtual machines (VMs) per node by editing the <literal>peerpodConfig</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check the current limit by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
-o jsonpath='{.spec.limit}{"\n"}'</programlisting>
</listitem>
<listitem>
<simpara>Modify the <literal>limit</literal> attribute of the <literal>peerpodConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
--type merge --patch '{"spec":{"limit":"&lt;value&gt;"}}' <co xml:id="CO8-1"/></programlisting>
<calloutlist>
<callout arearefs="CO8-1">
<para>Replace &lt;value&gt; with the limit you want to define.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="aws-enabling-ports_aws-cli">
<title>Enabling ports for AWS</title>
<simpara>You must enable ports 15150 and 9000 to allow internal communication with peer pods running on AWS.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>You have installed the AWS command line tool.</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to your OpenShift Container Platform cluster and retrieve the instance ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ INSTANCE_ID=$(oc get nodes -l 'node-role.kubernetes.io/worker' \
  -o jsonpath='{.items[0].spec.providerID}' | sed 's#[^ ]*/##g')</programlisting>
</listitem>
<listitem>
<simpara>Retrieve the AWS region:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_REGION=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.aws.region}')</programlisting>
</listitem>
<listitem>
<simpara>Retrieve the security group IDs and store them in an array:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_SG_IDS=($(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} \
  --query 'Reservations[*].Instances[*].SecurityGroups[*].GroupId' \
  --output text --region $AWS_REGION))</programlisting>
</listitem>
<listitem>
<simpara>For each security group ID, authorize the peer pods shim to access kata-agent communication, and set up the peer pods tunnel:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for AWS_SG_ID in "${AWS_SG_IDS[@]}"; do \
  aws ec2 authorize-security-group-ingress --group-id $AWS_SG_ID --protocol tcp --port 15150 --source-group $AWS_SG_ID --region $AWS_REGION \
  aws ec2 authorize-security-group-ingress --group-id $AWS_SG_ID --protocol tcp --port 9000 --source-group $AWS_SG_ID --region $AWS_REGION \
done</programlisting>
</listitem>
</orderedlist>
<simpara>The ports are now enabled.</simpara>
</section>
<section xml:id="creating-peer-pods-secret_aws-cli">
<title>Creating the peer pods secret</title>
<simpara>You must create the peer pods secret for OpenShift sandboxed containers.</simpara>
<simpara>The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.</simpara>
<simpara>By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have the following values generated by using the AWS console:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>AWS_ACCESS_KEY_ID</literal></simpara>
</listitem>
<listitem>
<simpara><literal>AWS_SECRET_ACCESS_KEY</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>peer-pods-secret.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  AWS_ACCESS_KEY_ID: "&lt;aws_access_key&gt;" <co xml:id="CO9-1"/>
  AWS_SECRET_ACCESS_KEY: "&lt;aws_secret_access_key&gt;" <co xml:id="CO9-2"/></programlisting>
<calloutlist>
<callout arearefs="CO9-1">
<para>Specify the <literal>AWS_ACCESS_KEY_ID</literal> value.</para>
</callout>
<callout arearefs="CO9-2">
<para>Specify the <literal>AWS_SECRET_ACCESS_KEY</literal> value.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the secret by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f peer-pods-secret.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: To update an existing peer pods config map, restart the <literal>peerpodconfig-ctrl-caa-daemon</literal> daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-peer-pods-config-map_aws-cli">
<title>Creating the peer pods config map</title>
<simpara>You must create the peer pods config map for OpenShift sandboxed containers.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have your Amazon Machine Image (AMI) ID if you are not using the default AMI ID based on your cluster credentials.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the following values from your AWS instance:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Retrieve and record the instance ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ INSTANCE_ID=$(oc get nodes -l 'node-role.kubernetes.io/worker' -o jsonpath='{.items[0].spec.providerID}' | sed 's#[^ ]*/##g')</programlisting>
<simpara>This is used to retrieve other values for the secret object.</simpara>
</listitem>
<listitem>
<simpara>Retrieve and record the AWS region:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_REGION=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.aws.region}') &amp;&amp; echo "AWS_REGION: \"$AWS_REGION\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the AWS subnet ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_SUBNET_ID=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].SubnetId' --region ${AWS_REGION} --output text) &amp;&amp; echo "AWS_SUBNET_ID: \"$AWS_SUBNET_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the AWS VPC ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_VPC_ID=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].VpcId' --region ${AWS_REGION} --output text) &amp;&amp; echo "AWS_VPC_ID: \"$AWS_VPC_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the AWS security group IDs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AWS_SG_IDS=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].SecurityGroups[*].GroupId' --region  $AWS_REGION --output json | jq -r '.[][][]' | paste -sd ",") &amp;&amp; echo "AWS_SG_IDS: \"$AWS_SG_IDS\""</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>peer-pods-cm.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "aws"
  VXLAN_PORT: "9000"
  PODVM_INSTANCE_TYPE: "t3.medium" <co xml:id="CO10-1"/>
  PODVM_INSTANCE_TYPES: "t2.small,t2.medium,t3.large" <co xml:id="CO10-2"/>
  PROXY_TIMEOUT: "5m"
  PODVM_AMI_ID: "&lt;podvm_ami_id&gt;" <co xml:id="CO10-3"/>
  AWS_REGION: "&lt;aws_region&gt;" <co xml:id="CO10-4"/>
  AWS_SUBNET_ID: "&lt;aws_subnet_id&gt;" <co xml:id="CO10-5"/>
  AWS_VPC_ID: "&lt;aws_vpc_id&gt;" <co xml:id="CO10-6"/>
  AWS_SG_IDS: "&lt;aws_sg_ids&gt;" <co xml:id="CO10-7"/>
  DISABLECVM: "true"</programlisting>
<calloutlist>
<callout arearefs="CO10-1">
<para>Defines the default instance type that is used when a type is not defined in the workload.</para>
</callout>
<callout arearefs="CO10-2">
<para>Lists all of the instance types you can specify when creating the pod. This allows you to define smaller instance types for workloads that need less memory and fewer CPUs or larger instance types for larger workloads.</para>
</callout>
<callout arearefs="CO10-3">
<para>Optional: By default, this value is populated when you run the <literal>KataConfig</literal> CR, using an AMI ID based on your cluster credentials. If you create your own AMI, specify the correct AMI ID.</para>
</callout>
<callout arearefs="CO10-4">
<para>Specify the <literal>AWS_REGION</literal> value you retrieved.</para>
</callout>
<callout arearefs="CO10-5">
<para>Specify the <literal>AWS_SUBNET_ID</literal> value you retrieved.</para>
</callout>
<callout arearefs="CO10-6">
<para>Specify the <literal>AWS_VPC_ID</literal> value you retrieved.</para>
</callout>
<callout arearefs="CO10-7">
<para>Specify the <literal>AWS_SG_IDS</literal> value you retrieved.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f peer-pods-cm.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: To update an existing peer pods config map, restart the <literal>peerpodconfig-ctrl-caa-daemon</literal> daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-kataconfig-cr-cli_aws-cli">
<title>Creating the KataConfig custom resource</title>
<simpara>You must create the <literal>KataConfig</literal> custom resource (CR) to install <literal>kata-remote</literal> as a runtime class on your worker nodes.</simpara>
<simpara>Creating the <literal>KataConfig</literal> CR triggers the OpenShift sandboxed containers Operator to do the following:
* Create a <literal>RuntimeClass</literal> CR named <literal>kata-remote</literal> with a default configuration. This enables users to configure workloads to use <literal>kata-remote</literal> as the runtime by referencing the CR in the <literal>RuntimeClassName</literal> field. This CR also specifies the resource overhead for the runtime.</simpara>
<simpara>OpenShift sandboxed containers installs <literal>kata-remote</literal> as a <emphasis>secondary, optional</emphasis> runtime on the cluster and not as the primary runtime.</simpara>
<important>
<simpara>Creating the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard disk drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>example-kataconfig.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  enablePeerPods: true
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <co xml:id="CO11-1"/></programlisting>
<calloutlist>
<callout arearefs="CO11-1">
<para>Optional: If you have applied node labels to install <literal>kata-remote</literal> on specific nodes, specify the key and value, for example, <literal>osc: 'true'</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-kataconfig.yaml</programlisting>
<simpara>The new <literal>KataConfig</literal> CR is created and installs <literal>kata-remote</literal> as a runtime class on the worker nodes.</simpara>
<simpara>Wait for the <literal>kata-remote</literal> installation to complete and the worker nodes to reboot before verifying the installation.</simpara>
</listitem>
<listitem>
<simpara>Monitor the installation progress by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</programlisting>
<simpara>When the status of all workers under <literal>kataNodes</literal> is <literal>installed</literal> and the condition <literal>InProgress</literal> is <literal>False</literal> without specifying a reason, the <literal>kata-remote</literal> is installed on the cluster.</simpara>
</listitem>
<listitem>
<simpara>Verify the daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon</programlisting>
</listitem>
<listitem>
<simpara>Verify the runtime classes by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get runtimeclass</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME             HANDLER          AGE
kata             kata             152m
kata-remote      kata-remote      152m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<bridgehead xml:id="verifying-pod-vm-image-creation_aws-cli" renderas="sect4">Verifying the pod VM image</bridgehead>
<simpara>After <literal>kata-remote</literal> is installed on your cluster, the OpenShift sandboxed containers Operator creates a pod VM image, which is used to create peer pods.
This process can take a long time because the image is created on the cloud instance.
You can verify that the pod VM image was created successfully by checking the config map that you created for the cloud provider.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the config map you created for the peer pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmap peer-pods-cm -n openshift-sandboxed-containers-operator -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>status</literal> stanza of the YAML file.</simpara>
<simpara>If the <literal>PODVM_AMI_ID</literal> parameter is populated, the pod VM image was created successfully.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Troubleshooting</title>
<listitem>
<simpara>Retrieve the events log by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get events -n openshift-sandboxed-containers-operator --field-selector involvedObject.name=osc-podvm-image-creation</programlisting>
</listitem>
<listitem>
<simpara>Retrieve the job log by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n openshift-sandboxed-containers-operator jobs/osc-podvm-image-creation</programlisting>
</listitem>
</orderedlist>
<simpara>If you cannot resolve the issue, submit a Red Hat Support case and attach the output of both logs.</simpara>
</section>
<section xml:id="configuring-workload-objects_aws-cli">
<title>Configuring workload objects</title>
<simpara>You must configure OpenShift sandboxed containers workload objects by setting <literal>kata-remote</literal> as the runtime class for the following pod-templated objects:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Pod</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicaSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicationController</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>StatefulSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>Deployment</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>DeploymentConfig</literal> objects</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.</simpara>
</important>
<simpara>You can define whether the workload should be deployed using the default instance type, which you defined in the config map, by adding an annotation to the YAML file.</simpara>
<simpara>If you do not want to define the instance type manually, you can add an annotation to use an automatic instance type, based on the memory available.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created the <literal>KataConfig</literal> custom resource (CR).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add <literal>spec.runtimeClassName: kata-remote</literal> to the manifest of each pod-templated workload object as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata-remote
# ...</programlisting>
</listitem>
<listitem>
<simpara>Add an annotation to the pod-templated object to use a manually defined instance type or an automatic instance type:</simpara>
<itemizedlist>
<listitem>
<simpara>To use a manually defined instance type, add the following annotation:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;object&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.machine_type: "t3.medium" <co xml:id="CO12-1"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO12-1">
<para>Specify the instance type that you defined in the config map.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To use an automatic instance type, add the following annotations:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;Pod&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.default_vcpus: &lt;vcpus&gt;
    io.katacontainers.config.hypervisor.default_memory: &lt;memory&gt;
# ...</programlisting>
<simpara>Define the amount of memory available for the workload to use.
The workload will run on an automatic instance type based on the amount of memory available.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Apply the changes to the workload object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;object.yaml&gt;</programlisting>
<simpara>OpenShift Container Platform creates the workload object and begins scheduling it.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Inspect the <literal>spec.runtimeClassName</literal> field of a pod-templated object. If the value is <literal>kata-remote</literal>, then the workload is running on OpenShift sandboxed containers, using peer pods.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="deploying-osc-on-azure">
<title>Deploying OpenShift sandboxed containers on Azure</title>
<simpara>You can deploy OpenShift sandboxed containers on Microsoft Azure Cloud Computing Services.</simpara>
<simpara>OpenShift sandboxed containers deploys peer pods. The peer pod design circumvents the need for nested virtualization. For more information, see <link linkend="peer-pods">peer pods</link>.</simpara>
<itemizedlist>
<title>Cluster requirements</title>
<listitem>
<simpara>You have installed Red Hat OpenShift Container Platform 4.14 or later on the cluster where you are installing the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>Your cluster has at least one worker node.</simpara>
</listitem>
</itemizedlist>
<section xml:id="peer-pod-resource-requirements_azure">
<title>Peer pod resource requirements</title>
<simpara>You must ensure that your cluster has sufficient resources.</simpara>
<simpara>Peer pod virtual machines (VMs) require resources in two locations:</simpara>
<itemizedlist>
<listitem>
<simpara>The worker node. The worker node stores metadata, Kata shim resources (<literal>containerd-shim-kata-v2</literal>), remote-hypervisor resources (<literal>cloud-api-adaptor</literal>), and the tunnel setup between the worker nodes and the peer pod VM.</simpara>
</listitem>
<listitem>
<simpara>The cloud instance. This is the actual peer pod VM running in the cloud.</simpara>
</listitem>
</itemizedlist>
<simpara>The CPU and memory resources used in the Kubernetes worker node are handled by the <link xlink:href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</link> included in the RuntimeClass (<literal>kata-remote</literal>) definition used for creating peer pods.</simpara>
<simpara>The total number of peer pod VMs running in the cloud is defined as Kubernetes Node extended resources.
This limit is per node and is set by the <literal>limit</literal> attribute in the <literal>peerpodConfig</literal> custom resource (CR).</simpara>
<simpara>The <literal>peerpodConfig</literal> CR, named <literal>peerpodconfig-openshift</literal>, is created when you create the <literal>kataConfig</literal> CR and enable peer pods, and is located in the <literal>openshift-sandboxed-containers-operator</literal> namespace.</simpara>
<simpara>The following <literal>peerpodConfig</literal> CR example displays the default <literal>spec</literal> values:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: confidentialcontainers.org/v1alpha1
kind: PeerPodConfig
metadata:
  name: peerpodconfig-openshift
  namespace: openshift-sandboxed-containers-operator
spec:
  cloudSecretName: peer-pods-secret
  configMapName: peer-pods-cm
  limit: "10" <co xml:id="CO13-1"/>
  nodeSelector:
    node-role.kubernetes.io/kata-oc: ""</programlisting>
<calloutlist>
<callout arearefs="CO13-1">
<para>The default limit is 10 VMs per node.</para>
</callout>
</calloutlist>
<simpara>The extended resource is named <literal>kata.peerpods.io/vm</literal>, and enables the Kubernetes scheduler to handle capacity tracking and accounting.</simpara>
<simpara>You can edit the limit per node based on the requirements for your environment after you install the OpenShift sandboxed containers Operator.</simpara>
<simpara>A <link xlink:href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">mutating webhook</link> adds the extended resource <literal>kata.peerpods.io/vm</literal> to the pod specification. It also removes any resource-specific entries from the pod specification, if present. This enables the Kubernetes scheduler to account for these extended resources, ensuring the peer pod is only scheduled when resources are available.</simpara>
<simpara>The mutating webhook modifies a Kubernetes pod as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>The mutating webhook checks the pod for the expected <literal>RuntimeClassName</literal> value, specified in the <literal>TARGET_RUNTIME_CLASS</literal> environment variable. If the value in the pod specification does not match the value in the <literal>TARGET_RUNTIME_CLASS</literal>, the webhook exits without modifying the pod.</simpara>
</listitem>
<listitem>
<simpara>If the <literal>RuntimeClassName</literal> values match, the webhook makes the following changes to the pod spec:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The webhook removes every resource specification from the <literal>resources</literal> field of all containers and init containers in the pod.</simpara>
</listitem>
<listitem>
<simpara>The webhook adds the extended resource (<literal>kata.peerpods.io/vm</literal>) to the spec by modifying the resources field of the first container in the pod. The extended resource <literal>kata.peerpods.io/vm</literal> is used by the Kubernetes scheduler for accounting purposes.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<note>
<simpara>The mutating webhook excludes specific system namespaces in OpenShift Container Platform from mutation. If a peer pod is created in those system namespaces, then resource accounting using Kubernetes extended resources does not work unless the pod spec includes the extended resource.</simpara>
<simpara>As a best practice, define a cluster-wide policy to only allow peer pod creation in specific namespaces.</simpara>
</note>
</section>
<section xml:id="deploying-osc-web_azure-web">
<title>Deploying OpenShift sandboxed containers by using the web console</title>
<simpara>You can deploy OpenShift sandboxed containers on Azure by using the OpenShift Container Platform web console to perform the following tasks:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>Create the peer pods secret.</simpara>
</listitem>
<listitem>
<simpara>Create the peer pods config map.</simpara>
</listitem>
<listitem>
<simpara>Create the Azure secret.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>Configure the OpenShift sandboxed containers workload objects.</simpara>
</listitem>
</orderedlist>
<section xml:id="installing-operator-web-console_azure-web">
<title>Installing the OpenShift sandboxed containers Operator</title>
<simpara>You can install the OpenShift sandboxed containers Operator by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Filter by keyword</emphasis> field, type <literal>OpenShift sandboxed containers</literal>.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">OpenShift sandboxed containers Operator</emphasis> tile and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, select <emphasis role="strong">stable</emphasis> from the list of available <emphasis role="strong">Update Channel</emphasis> options.</simpara>
</listitem>
<listitem>
<simpara>Verify that <emphasis role="strong">Operator recommended Namespace</emphasis> is selected for <emphasis role="strong">Installed Namespace</emphasis>. This installs the Operator in the mandatory <literal>openshift-sandboxed-containers-operator</literal> namespace. If this namespace does not yet exist, it is automatically created.</simpara>
<note>
<simpara>Attempting to install the OpenShift sandboxed containers Operator in a namespace other than <literal>openshift-sandboxed-containers-operator</literal> causes the installation to fail.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify that <emphasis role="strong">Automatic</emphasis> is selected for <emphasis role="strong">Approval Strategy</emphasis>. <emphasis role="strong">Automatic</emphasis> is the default value, and enables automatic updates to OpenShift sandboxed containers when a new z-stream release is available.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis> to verify that the Operator is installed.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</link> for disconnected environments.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-peer-pods-secret_azure-web">
<title>Creating the peer pods secret</title>
<simpara>You must create the peer pods secret for OpenShift sandboxed containers.</simpara>
<simpara>The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.</simpara>
<simpara>By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed and configured the Azure CLI tool.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Retrieve the Azure subscription ID by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_SUBSCRIPTION_ID=$(az account list --query "[?isDefault].id" \
  -o tsv) &amp;&amp; echo "AZURE_SUBSCRIPTION_ID: \"$AZURE_SUBSCRIPTION_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Generate the RBAC content by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az ad sp create-for-rbac --role Contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID \
  --query "{ client_id: appId, client_secret: password, tenant_id: tenant }"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">{
  "client_id": `AZURE_CLIENT_ID`,
  "client_secret": `AZURE_CLIENT_SECRET`,
  "tenant_id": `AZURE_TENANT_ID`
}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Record the RBAC output to use in the <literal>secret</literal> object.</simpara>
</listitem>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the OpenShift sandboxed containers Operator tile.</simpara>
</listitem>
<listitem>
<simpara>Click the Import icon (<emphasis role="strong">+</emphasis>) on the top right corner.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Import YAML</emphasis> window, paste the following YAML manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  AZURE_CLIENT_ID: "&lt;azure_client_id&gt;" <co xml:id="CO14-1"/>
  AZURE_CLIENT_SECRET: "&lt;azure_client_secret&gt;" <co xml:id="CO14-2"/>
  AZURE_TENANT_ID: "&lt;azure_tenant_id&gt;" <co xml:id="CO14-3"/>
  AZURE_SUBSCRIPTION_ID: "&lt;azure_subscription_id&gt;" <co xml:id="CO14-4"/></programlisting>
<calloutlist>
<callout arearefs="CO14-1">
<para>Specify the <literal>AZURE_CLIENT_ID</literal> value.</para>
</callout>
<callout arearefs="CO14-2">
<para>Specify the <literal>AZURE_CLIENT_SECRET</literal> value.</para>
</callout>
<callout arearefs="CO14-3">
<para>Specify the <literal>AZURE_TENANT_ID</literal> value.</para>
</callout>
<callout arearefs="CO14-4">
<para>Specify the <literal>AZURE_SUBSCRIPTION_ID</literal> value.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis> to apply the changes.</simpara>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Workloads</emphasis> → <emphasis role="strong">Secrets</emphasis> to verify the peer pods secret.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-peer-pods-config-map_azure-web">
<title>Creating the peer pods config map</title>
<simpara>You must create the peer pods config map for OpenShift sandboxed containers.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the following values from your Azure instance:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Retrieve and record the Azure resource group:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_RESOURCE_GROUP=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.azure.resourceGroupName}') &amp;&amp; echo "AZURE_RESOURCE_GROUP: \"$AZURE_RESOURCE_GROUP\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure VNet name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_VNET_NAME=$(az network vnet list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Name:name}" --output tsv)</programlisting>
<simpara>This value is used to retrieve the Azure subnet ID.</simpara>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure subnet ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_SUBNET_ID=$(az network vnet subnet list --resource-group ${AZURE_RESOURCE_GROUP} --vnet-name $AZURE_VNET_NAME --query "[].{Id:id} | [? contains(Id, 'worker')]" --output tsv) &amp;&amp; echo "AZURE_SUBNET_ID: \"$AZURE_SUBNET_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure network security group (NSG) ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_NSG_ID=$(az network nsg list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Id:id}" --output tsv) &amp;&amp; echo "AZURE_NSG_ID: \"$AZURE_NSG_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure region:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_REGION=$(az group show --resource-group ${AZURE_RESOURCE_GROUP} --query "{Location:location}" --output tsv) &amp;&amp; echo "AZURE_REGION: \"$AZURE_REGION\""</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the OpenShift sandboxed containers Operator from the list of operators.</simpara>
</listitem>
<listitem>
<simpara>Click the Import icon (<emphasis role="strong">+</emphasis>) in the top right corner.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Import YAML</emphasis> window, paste the following YAML manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "azure"
  VXLAN_PORT: "9000"
  AZURE_INSTANCE_SIZE: "Standard_B2als_v2" <co xml:id="CO15-1"/>
  AZURE_INSTANCE_SIZES: "Standard_B2als_v2,Standard_D2as_v5,Standard_D4as_v5,Standard_D2ads_v5" <co xml:id="CO15-2"/>
  AZURE_SUBNET_ID: "&lt;azure_subnet_id&gt;" <co xml:id="CO15-3"/>
  AZURE_NSG_ID: "&lt;azure_nsg_id&gt;" <co xml:id="CO15-4"/>
  PROXY_TIMEOUT: "5m"
  AZURE_IMAGE_ID: "&lt;azure_image_id&gt;" <co xml:id="CO15-5"/>
  AZURE_REGION: "&lt;azure_region&gt;" <co xml:id="CO15-6"/>
  AZURE_RESOURCE_GROUP: "&lt;azure_resource_group&gt;" <co xml:id="CO15-7"/>
  DISABLECVM: "true"</programlisting>
<calloutlist>
<callout arearefs="CO15-1">
<para>This value is the default if an instance size is not defined in the workload.</para>
</callout>
<callout arearefs="CO15-2">
<para>Lists all of the instance sizes you can specify when creating the pod. This allows you to define smaller instance sizes for workloads that need less memory and fewer CPUs or larger instance sizes for larger workloads.</para>
</callout>
<callout arearefs="CO15-3">
<para>Specify the <literal>AZURE_SUBNET_ID</literal> value that you retrieved.</para>
</callout>
<callout arearefs="CO15-4">
<para>Specify the <literal>AZURE_NSG_ID</literal> value that you retrieved.</para>
</callout>
<callout arearefs="CO15-5">
<para>Optional: By default, this value is populated when you run the <literal>KataConfig</literal> CR, using an Azure image ID based on your cluster credentials. If you create your own Azure image, specify the correct image ID.</para>
</callout>
<callout arearefs="CO15-6">
<para>Specify the <literal>AZURE_REGION</literal> value you retrieved.</para>
</callout>
<callout arearefs="CO15-7">
<para>Specify the <literal>AZURE_RESOURCE_GROUP</literal> value you retrieved.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis> to apply the changes.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</programlisting>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Workloads</emphasis> → <emphasis role="strong">ConfigMaps</emphasis> to view the new config map.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-ssh-key-secret_azure-web">
<title>Creating the Azure secret</title>
<simpara>You must create the secret for Azure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to your OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>Generate an SSH key pair by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh-keygen -f ./id_rsa -N ""</programlisting>
</listitem>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Workloads</emphasis> → <emphasis role="strong">Secrets</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Secrets</emphasis> page, verify that you are in the <emphasis role="strong">openshift-sandboxed-containers-operator</emphasis> project.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis> and select <emphasis role="strong">Key/value secret</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Secret name</emphasis> field, enter <literal>ssh-key-secret</literal>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Key</emphasis> field, enter <literal>id_rsa.pub</literal>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Value</emphasis> field, paste your public SSH key.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Delete the SSH keys you created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ shred --remove id_rsa.pub id_rsa</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-kataconfig-cr-web_azure-web">
<title>Creating the KataConfig custom resource</title>
<simpara>You must create the <literal>KataConfig</literal> custom resource (CR) to install <literal>kata-remote</literal> as a <literal>RuntimeClass</literal> on your worker nodes.</simpara>
<simpara>The <literal>kata-remote</literal> runtime class is installed on all worker nodes by default. If you want to install <literal>kata-remote</literal> on specific nodes, you can add labels to those nodes and then define the label in the <literal>KataConfig</literal> CR.</simpara>
<simpara>OpenShift sandboxed containers installs <literal>kata-remote</literal> as a <emphasis>secondary, optional</emphasis> runtime on the cluster and not as the primary runtime.</simpara>
<important>
<simpara>Creating the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. The following factors might increase the reboot time:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard disk drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Optional: You have installed the Node Feature Discovery Operator if you want to enable node eligibility checks.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">KataConfig</emphasis> tab, click <emphasis role="strong">Create KataConfig</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the following details:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Name</emphasis>: Optional: The default name is <literal>example-kataconfig</literal>.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Labels</emphasis>: Optional: Enter any relevant, identifying attributes to the <literal>KataConfig</literal> resource. Each label represents a key-value pair.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">enablePeerPods</emphasis>: Select for public cloud, IBM Z®, and IBM® LinuxONE deployments.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">kataConfigPoolSelector</emphasis>. Optional: To install <literal>kata-remote</literal> on selected nodes, add a match expression for the labels on the selected nodes:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Expand the <emphasis role="strong">kataConfigPoolSelector</emphasis> area.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">kataConfigPoolSelector</emphasis> area, expand <emphasis role="strong">matchExpressions</emphasis>. This is a list of label selector requirements.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add matchExpressions</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Key</emphasis> field, enter the label key the selector applies to.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Operator</emphasis> field, enter the key’s relationship to the label values. Valid operators are <literal>In</literal>, <literal>NotIn</literal>, <literal>Exists</literal>, and <literal>DoesNotExist</literal>.</simpara>
</listitem>
<listitem>
<simpara>Expand the <emphasis role="strong">Values</emphasis> area and then click <emphasis role="strong">Add value</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Value</emphasis> field, enter <literal>true</literal> or <literal>false</literal> for <emphasis role="strong">key</emphasis> label value.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">logLevel</emphasis>: Define the level of log data retrieved for nodes with the <literal>kata-remote</literal> runtime class.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>. The <literal>KataConfig</literal> CR is created and installs the <literal>kata-remote</literal> runtime class on the worker nodes.</simpara>
<simpara>Wait for the <literal>kata-remote</literal> installation to complete and the worker nodes to reboot before verifying the installation.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>On the <emphasis role="strong">KataConfig</emphasis> tab, click the <literal>KataConfig</literal> CR to view its details.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">YAML</emphasis> tab to view the <literal>status</literal> stanza.</simpara>
<simpara>The <literal>status</literal> stanza contains the <literal>conditions</literal> and <literal>kataNodes</literal> keys. The value of <literal>status.kataNodes</literal> is an array of nodes, each of which lists nodes in a particular state of <literal>kata-remote</literal> installation. A message appears each time there is an update.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Reload</emphasis> to refresh the YAML.</simpara>
<simpara>When all workers in the <literal>status.kataNodes</literal> array display the values <literal>installed</literal> and <literal>conditions.InProgress: False</literal> with no specified reason, the <literal>kata-remote</literal> is installed on the cluster.</simpara>
</listitem>
</orderedlist>
<bridgehead xml:id="additional_resources_3" role="_additional-resources" renderas="sect4" remap="_additional_resources_3">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara><link linkend="kataconfig-status-messages">KataConfig status messages</link></simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="verifying-pod-vm-image-creation_azure-web" renderas="sect4">Verifying the pod VM image</bridgehead>
<simpara>After <literal>kata-remote</literal> is installed on your cluster, the OpenShift sandboxed containers Operator creates a pod VM image, which is used to create peer pods.
This process can take a long time because the image is created on the cloud instance.
You can verify that the pod VM image was created successfully by checking the config map that you created for the cloud provider.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Workloads</emphasis> → <emphasis role="strong">ConfigMaps</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the provider config map to view its details.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">YAML</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Check the <literal>status</literal> stanza of the YAML file.</simpara>
<simpara>If the <literal>AZURE_IMAGE_ID</literal> parameter is populated, the pod VM image was created successfully.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Troubleshooting</title>
<listitem>
<simpara>Retrieve the events log by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get events -n openshift-sandboxed-containers-operator --field-selector involvedObject.name=osc-podvm-image-creation</programlisting>
</listitem>
<listitem>
<simpara>Retrieve the job log by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n openshift-sandboxed-containers-operator jobs/osc-podvm-image-creation</programlisting>
</listitem>
</orderedlist>
<simpara>If you cannot resolve the issue, submit a Red Hat Support case and attach the output of both logs.</simpara>
</section>
<section xml:id="configuring-workload-objects_azure-web">
<title>Configuring workload objects</title>
<simpara>You must configure OpenShift sandboxed containers workload objects by setting <literal>kata-remote</literal> as the runtime class for the following pod-templated objects:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Pod</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicaSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicationController</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>StatefulSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>Deployment</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>DeploymentConfig</literal> objects</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.</simpara>
</important>
<simpara>You can define whether the workload should be deployed using the default instance size, which you defined in the config map, by adding an annotation to the YAML file.</simpara>
<simpara>If you do not want to define the instance size manually, you can add an annotation to use an automatic instance size, based on the memory available.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created the <literal>KataConfig</literal> custom resource (CR).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Workloads</emphasis> → workload type, for example, <emphasis role="strong">Pods</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the workload type page, click an object to view its details.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">YAML</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Add <literal>spec.runtimeClassName: kata-remote</literal> to the manifest of each pod-templated workload object as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata-remote
# ...</programlisting>
</listitem>
<listitem>
<simpara>Add an annotation to the pod-templated object to use a manually defined instance size or an automatic instance size:</simpara>
<itemizedlist>
<listitem>
<simpara>To use a manually defined instance size, add the following annotation:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;object&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.machine_type: "Standard_B2als_v2" <co xml:id="CO16-1"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO16-1">
<para>Specify the instance size that you defined in the config map.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To use an automatic instance size, add the following annotations:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;Pod&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.default_vcpus: &lt;vcpus&gt;
    io.katacontainers.config.hypervisor.default_memory: &lt;memory&gt;
# ...</programlisting>
<simpara>Define the amount of memory available for the workload to use.
The workload will run on an automatic instance size based on the amount of memory available.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis> to apply the changes.</simpara>
<simpara>OpenShift Container Platform creates the workload object and begins scheduling it.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Inspect the <literal>spec.runtimeClassName</literal> field of a pod-templated object. If the value is <literal>kata-remote</literal>, then the workload is running on OpenShift sandboxed containers, using peer pods.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="deploying-osc-cli_azure-cli">
<title>Deploying OpenShift sandboxed containers by using the command line</title>
<simpara>You can deploy OpenShift sandboxed containers on Azure by using the command line interface (CLI) to perform the following tasks:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>Optional: Change the number of virtual machines running on each worker node.</simpara>
</listitem>
<listitem>
<simpara>Create the peer pods secret.</simpara>
</listitem>
<listitem>
<simpara>Create the peer pods config map.</simpara>
</listitem>
<listitem>
<simpara>Create the Azure secret.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>Configure the OpenShift sandboxed containers workload objects.</simpara>
</listitem>
</orderedlist>
<section xml:id="installing-operator-cli_azure-cli">
<title>Installing the OpenShift sandboxed containers Operator</title>
<simpara>You can install the OpenShift sandboxed containers Operator by using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>osc-namespace.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sandboxed-containers-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-namespace.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>osc-operatorgroup.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sandboxed-containers-operator-group
  namespace: openshift-sandboxed-containers-operator
spec:
  targetNamespaces:
  - openshift-sandboxed-containers-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the operator group by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-operatorgroup.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>osc-subscription.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sandboxed-containers-operator
  namespace: openshift-sandboxed-containers-operator
spec:
  channel: stable
  installPlanApproval: Automatic
  name: sandboxed-containers-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: sandboxed-containers-operator.v1.8.0</programlisting>
</listitem>
<listitem>
<simpara>Create the subscription by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-subscription.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the Operator is correctly installed by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-sandboxed-containers-operator</programlisting>
<simpara>This command can take several minutes to complete.</simpara>
</listitem>
<listitem>
<simpara>Watch the process by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch oc get csv -n openshift-sandboxed-containers-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                             DISPLAY                                  VERSION             REPLACES                   PHASE
openshift-sandboxed-containers   openshift-sandboxed-containers-operator  1.8.0    1.7.0        Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</link> for disconnected environments.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="modifying-peer-pod-vm-limit_azure-cli">
<title>Modifying the number of peer pod VMs per node</title>
<simpara>You can change the limit of peer pod virtual machines (VMs) per node by editing the <literal>peerpodConfig</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check the current limit by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
-o jsonpath='{.spec.limit}{"\n"}'</programlisting>
</listitem>
<listitem>
<simpara>Modify the <literal>limit</literal> attribute of the <literal>peerpodConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
--type merge --patch '{"spec":{"limit":"&lt;value&gt;"}}' <co xml:id="CO17-1"/></programlisting>
<calloutlist>
<callout arearefs="CO17-1">
<para>Replace &lt;value&gt; with the limit you want to define.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-peer-pods-secret_azure-cli">
<title>Creating the peer pods secret</title>
<simpara>You must create the peer pods secret for OpenShift sandboxed containers.</simpara>
<simpara>The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.</simpara>
<simpara>By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed and configured the Azure CLI tool.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Retrieve the Azure subscription ID by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_SUBSCRIPTION_ID=$(az account list --query "[?isDefault].id" \
  -o tsv) &amp;&amp; echo "AZURE_SUBSCRIPTION_ID: \"$AZURE_SUBSCRIPTION_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Generate the RBAC content by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az ad sp create-for-rbac --role Contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID \
  --query "{ client_id: appId, client_secret: password, tenant_id: tenant }"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">{
  "client_id": `AZURE_CLIENT_ID`,
  "client_secret": `AZURE_CLIENT_SECRET`,
  "tenant_id": `AZURE_TENANT_ID`
}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Record the RBAC output to use in the <literal>secret</literal> object.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>peer-pods-secret.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  AZURE_CLIENT_ID: "&lt;azure_client_id&gt;" <co xml:id="CO18-1"/>
  AZURE_CLIENT_SECRET: "&lt;azure_client_secret&gt;" <co xml:id="CO18-2"/>
  AZURE_TENANT_ID: "&lt;azure_tenant_id&gt;" <co xml:id="CO18-3"/>
  AZURE_SUBSCRIPTION_ID: "&lt;azure_subscription_id&gt;" <co xml:id="CO18-4"/></programlisting>
<calloutlist>
<callout arearefs="CO18-1">
<para>Specify the <literal>AZURE_CLIENT_ID</literal> value.</para>
</callout>
<callout arearefs="CO18-2">
<para>Specify the <literal>AZURE_CLIENT_SECRET</literal> value.</para>
</callout>
<callout arearefs="CO18-3">
<para>Specify the <literal>AZURE_TENANT_ID</literal> value.</para>
</callout>
<callout arearefs="CO18-4">
<para>Specify the <literal>AZURE_SUBSCRIPTION_ID</literal> value.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the secret by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f peer-pods-secret.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: To update an existing peer pods config map, restart the <literal>peerpodconfig-ctrl-caa-daemon</literal> daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-peer-pods-config-map_azure-cli">
<title>Creating the peer pods config map</title>
<simpara>You must create the peer pods config map for OpenShift sandboxed containers.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the following values from your Azure instance:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Retrieve and record the Azure resource group:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_RESOURCE_GROUP=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.azure.resourceGroupName}') &amp;&amp; echo "AZURE_RESOURCE_GROUP: \"$AZURE_RESOURCE_GROUP\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure VNet name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_VNET_NAME=$(az network vnet list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Name:name}" --output tsv)</programlisting>
<simpara>This value is used to retrieve the Azure subnet ID.</simpara>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure subnet ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_SUBNET_ID=$(az network vnet subnet list --resource-group ${AZURE_RESOURCE_GROUP} --vnet-name $AZURE_VNET_NAME --query "[].{Id:id} | [? contains(Id, 'worker')]" --output tsv) &amp;&amp; echo "AZURE_SUBNET_ID: \"$AZURE_SUBNET_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure network security group (NSG) ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_NSG_ID=$(az network nsg list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Id:id}" --output tsv) &amp;&amp; echo "AZURE_NSG_ID: \"$AZURE_NSG_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure region:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_REGION=$(az group show --resource-group ${AZURE_RESOURCE_GROUP} --query "{Location:location}" --output tsv) &amp;&amp; echo "AZURE_REGION: \"$AZURE_REGION\""</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>peer-pods-cm.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "azure"
  VXLAN_PORT: "9000"
  AZURE_INSTANCE_SIZE: "Standard_B2als_v2" <co xml:id="CO19-1"/>
  AZURE_INSTANCE_SIZES: "Standard_B2als_v2,Standard_D2as_v5,Standard_D4as_v5,Standard_D2ads_v5" <co xml:id="CO19-2"/>
  AZURE_SUBNET_ID: "&lt;azure_subnet_id&gt;" <co xml:id="CO19-3"/>
  AZURE_NSG_ID: "&lt;azure_nsg_id&gt;" <co xml:id="CO19-4"/>
  PROXY_TIMEOUT: "5m"
  AZURE_IMAGE_ID: "&lt;azure_image_id&gt;" <co xml:id="CO19-5"/>
  AZURE_REGION: "&lt;azure_region&gt;" <co xml:id="CO19-6"/>
  AZURE_RESOURCE_GROUP: "&lt;azure_resource_group&gt;" <co xml:id="CO19-7"/>
  DISABLECVM: "true"</programlisting>
<calloutlist>
<callout arearefs="CO19-1">
<para>This value is the default if an instance size is not defined in the workload.</para>
</callout>
<callout arearefs="CO19-2">
<para>Lists all of the instance sizes you can specify when creating the pod. This allows you to define smaller instance sizes for workloads that need less memory and fewer CPUs or larger instance sizes for larger workloads.</para>
</callout>
<callout arearefs="CO19-3">
<para>Specify the <literal>AZURE_SUBNET_ID</literal> value that you retrieved.</para>
</callout>
<callout arearefs="CO19-4">
<para>Specify the <literal>AZURE_NSG_ID</literal> value that you retrieved.</para>
</callout>
<callout arearefs="CO19-5">
<para>Optional: By default, this value is populated when you run the <literal>KataConfig</literal> CR, using an Azure image ID based on your cluster credentials. If you create your own Azure image, specify the correct image ID.</para>
</callout>
<callout arearefs="CO19-6">
<para>Specify the <literal>AZURE_REGION</literal> value you retrieved.</para>
</callout>
<callout arearefs="CO19-7">
<para>Specify the <literal>AZURE_RESOURCE_GROUP</literal> value you retrieved.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f peer-pods-cm.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: To update an existing peer pods config map, restart the <literal>peerpodconfig-ctrl-caa-daemon</literal> daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-ssh-key-secret_azure-cli">
<title>Creating the Azure secret</title>
<simpara>You must create the secret for Azure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to your OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>Generate an SSH key pair by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh-keygen -f ./id_rsa -N ""</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>Secret</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic ssh-key-secret \
  -n openshift-sandboxed-containers-operator \
  --from-file=id_rsa.pub=./id_rsa.pub \
  --from-file=id_rsa=./id_rsa</programlisting>
</listitem>
<listitem>
<simpara>Delete the SSH keys you created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ shred --remove id_rsa.pub id_rsa</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-kataconfig-cr-cli_azure-cli">
<title>Creating the KataConfig custom resource</title>
<simpara>You must create the <literal>KataConfig</literal> custom resource (CR) to install <literal>kata-remote</literal> as a runtime class on your worker nodes.</simpara>
<simpara>Creating the <literal>KataConfig</literal> CR triggers the OpenShift sandboxed containers Operator to do the following:
* Create a <literal>RuntimeClass</literal> CR named <literal>kata-remote</literal> with a default configuration. This enables users to configure workloads to use <literal>kata-remote</literal> as the runtime by referencing the CR in the <literal>RuntimeClassName</literal> field. This CR also specifies the resource overhead for the runtime.</simpara>
<simpara>OpenShift sandboxed containers installs <literal>kata-remote</literal> as a <emphasis>secondary, optional</emphasis> runtime on the cluster and not as the primary runtime.</simpara>
<important>
<simpara>Creating the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard disk drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>example-kataconfig.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  enablePeerPods: true
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <co xml:id="CO20-1"/></programlisting>
<calloutlist>
<callout arearefs="CO20-1">
<para>Optional: If you have applied node labels to install <literal>kata-remote</literal> on specific nodes, specify the key and value, for example, <literal>osc: 'true'</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-kataconfig.yaml</programlisting>
<simpara>The new <literal>KataConfig</literal> CR is created and installs <literal>kata-remote</literal> as a runtime class on the worker nodes.</simpara>
<simpara>Wait for the <literal>kata-remote</literal> installation to complete and the worker nodes to reboot before verifying the installation.</simpara>
</listitem>
<listitem>
<simpara>Monitor the installation progress by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</programlisting>
<simpara>When the status of all workers under <literal>kataNodes</literal> is <literal>installed</literal> and the condition <literal>InProgress</literal> is <literal>False</literal> without specifying a reason, the <literal>kata-remote</literal> is installed on the cluster.</simpara>
</listitem>
<listitem>
<simpara>Verify the daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon</programlisting>
</listitem>
<listitem>
<simpara>Verify the runtime classes by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get runtimeclass</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME             HANDLER          AGE
kata             kata             152m
kata-remote      kata-remote      152m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<bridgehead xml:id="verifying-pod-vm-image-creation_azure-cli" renderas="sect4">Verifying the pod VM image</bridgehead>
<simpara>After <literal>kata-remote</literal> is installed on your cluster, the OpenShift sandboxed containers Operator creates a pod VM image, which is used to create peer pods.
This process can take a long time because the image is created on the cloud instance.
You can verify that the pod VM image was created successfully by checking the config map that you created for the cloud provider.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the config map you created for the peer pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmap peer-pods-cm -n openshift-sandboxed-containers-operator -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>status</literal> stanza of the YAML file.</simpara>
<simpara>If the <literal>AZURE_IMAGE_ID</literal> parameter is populated, the pod VM image was created successfully.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Troubleshooting</title>
<listitem>
<simpara>Retrieve the events log by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get events -n openshift-sandboxed-containers-operator --field-selector involvedObject.name=osc-podvm-image-creation</programlisting>
</listitem>
<listitem>
<simpara>Retrieve the job log by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n openshift-sandboxed-containers-operator jobs/osc-podvm-image-creation</programlisting>
</listitem>
</orderedlist>
<simpara>If you cannot resolve the issue, submit a Red Hat Support case and attach the output of both logs.</simpara>
</section>
<section xml:id="configuring-workload-objects_azure-cli">
<title>Configuring workload objects</title>
<simpara>You must configure OpenShift sandboxed containers workload objects by setting <literal>kata-remote</literal> as the runtime class for the following pod-templated objects:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Pod</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicaSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicationController</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>StatefulSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>Deployment</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>DeploymentConfig</literal> objects</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.</simpara>
</important>
<simpara>You can define whether the workload should be deployed using the default instance size, which you defined in the config map, by adding an annotation to the YAML file.</simpara>
<simpara>If you do not want to define the instance size manually, you can add an annotation to use an automatic instance size, based on the memory available.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created the <literal>KataConfig</literal> custom resource (CR).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add <literal>spec.runtimeClassName: kata-remote</literal> to the manifest of each pod-templated workload object as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata-remote
# ...</programlisting>
</listitem>
<listitem>
<simpara>Add an annotation to the pod-templated object to use a manually defined instance size or an automatic instance size:</simpara>
<itemizedlist>
<listitem>
<simpara>To use a manually defined instance size, add the following annotation:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;object&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.machine_type: "Standard_B2als_v2" <co xml:id="CO21-1"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO21-1">
<para>Specify the instance size that you defined in the config map.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To use an automatic instance size, add the following annotations:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;Pod&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.default_vcpus: &lt;vcpus&gt;
    io.katacontainers.config.hypervisor.default_memory: &lt;memory&gt;
# ...</programlisting>
<simpara>Define the amount of memory available for the workload to use.
The workload will run on an automatic instance size based on the amount of memory available.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Apply the changes to the workload object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;object.yaml&gt;</programlisting>
<simpara>OpenShift Container Platform creates the workload object and begins scheduling it.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Inspect the <literal>spec.runtimeClassName</literal> field of a pod-templated object. If the value is <literal>kata-remote</literal>, then the workload is running on OpenShift sandboxed containers, using peer pods.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="deploying-cc_azure-cc">
<title>Deploying Confidential Containers on Azure</title>
<simpara>You can deploy Confidential Containers on Microsoft Azure Cloud Computing Services after you deploy OpenShift sandboxed containers.</simpara>
<important>
<simpara>Confidential Containers on Azure is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<itemizedlist>
<title>Cluster requirements</title>
<listitem>
<simpara>You have installed Red Hat OpenShift Container Platform 4.15 or later on the cluster where you are installing the Confidential compute attestation Operator.</simpara>
</listitem>
</itemizedlist>
<simpara>You deploy Confidential Containers by performing the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the Confidential compute attestation Operator.</simpara>
</listitem>
<listitem>
<simpara>Create the route for Trustee.</simpara>
</listitem>
<listitem>
<simpara>Enable the Confidential Containers feature gate.</simpara>
</listitem>
<listitem>
<simpara>Update the peer pods config map.</simpara>
</listitem>
<listitem>
<simpara>Delete the <literal>KataConfig</literal> custom resource (CR).</simpara>
</listitem>
<listitem>
<simpara>Re-create the <literal>KataConfig</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>Create the Trustee authentication secret.</simpara>
</listitem>
<listitem>
<simpara>Create the Trustee config map.</simpara>
</listitem>
<listitem>
<simpara>Configure Trustee values, policies, and secrets.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>KbsConfig</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>Verify the Trustee configuration.</simpara>
</listitem>
<listitem>
<simpara>Verify the attestation process.</simpara>
</listitem>
</orderedlist>
<section xml:id="cc-installing-cc-operator-cli_azure-cc">
<title>Installing the Confidential compute attestation Operator</title>
<simpara>You can install the Confidential compute attestation Operator on Azure by using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>trustee-namespace.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: trustee-operator-system</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>trustee-operator-system</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f trustee-namespace.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>trustee-operatorgroup.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: trustee-operator-group
  namespace: trustee-operator-system
spec:
  targetNamespaces:
  - trustee-operator-system</programlisting>
</listitem>
<listitem>
<simpara>Create the operator group by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f trustee-operatorgroup.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>trustee-subscription.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: trustee-operator
  namespace: trustee-operator-system
spec:
  channel: stable
  installPlanApproval: Automatic
  name: trustee-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: trustee-operator.v0.1.0</programlisting>
</listitem>
<listitem>
<simpara>Create the subscription by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f trustee-subscription.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the Operator is correctly installed by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n trustee-operator-system</programlisting>
<simpara>This command can take several minutes to complete.</simpara>
</listitem>
<listitem>
<simpara>Watch the process by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch oc get csv -n trustee-operator-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                      DISPLAY                        PHASE
trustee-operator.v0.1.0   Trustee Operator  0.1.0        Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-enabling-feature-gate_azure-cc">
<title>Enabling the Confidential Containers feature gate</title>
<simpara>You must enable the Confidential Containers feature gate.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have subscribed to the OpenShift sandboxed containers Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>cc-feature-gate.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: osc-feature-gates
  namespace: openshift-sandboxed-containers-operator
data:
  confidential: "true"</programlisting>
</listitem>
<listitem>
<simpara>Create the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f cc-feature-gate.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-route_azure-cc">
<title>Creating the route for Trustee</title>
<simpara>You can create a secure route with edge TLS termination for Trustee. External ingress traffic reaches the router pods as HTTPS and passes on to the Trustee pods as HTTP.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the Confidential compute attestation Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an edge route by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create route edge --service=kbs-service --port kbs-port \
  -n trustee-operator-system</programlisting>
<note>
<simpara>Note: Currently, only a route with a valid CA-signed certificate is supported. You cannot use a route with self-signed certificate.</simpara>
</note>
</listitem>
<listitem>
<simpara>Set the <literal>TRUSTEE_HOST</literal> variable by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ TRUSTEE_HOST=$(oc get route -n trustee-operator-system kbs-service \
  -o jsonpath={.spec.host})</programlisting>
</listitem>
<listitem>
<simpara>Verify the route by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $TRUSTEE_HOST</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">kbs-service-trustee-operator-system.apps.memvjias.eastus.aroapp.io</programlisting>
</para>
</formalpara>
<simpara>Record this value for the peer pods config map.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-peer-pods-config-map_azure-cc">
<title>Updating the peer pods config map</title>
<simpara>You must update the peer pods config map for Confidential Containers.</simpara>
<note>
<simpara>Set Secure Boot to <literal>true</literal> to enable it by default. The default value is <literal>false</literal>, which presents a security risk.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the following values from your Azure instance:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Retrieve and record the Azure resource group:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_RESOURCE_GROUP=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.azure.resourceGroupName}') &amp;&amp; echo "AZURE_RESOURCE_GROUP: \"$AZURE_RESOURCE_GROUP\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure VNet name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_VNET_NAME=$(az network vnet list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Name:name}" --output tsv)</programlisting>
<simpara>This value is used to retrieve the Azure subnet ID.</simpara>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure subnet ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_SUBNET_ID=$(az network vnet subnet list --resource-group ${AZURE_RESOURCE_GROUP} --vnet-name $AZURE_VNET_NAME --query "[].{Id:id} | [? contains(Id, 'worker')]" --output tsv) &amp;&amp; echo "AZURE_SUBNET_ID: \"$AZURE_SUBNET_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure network security group (NSG) ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_NSG_ID=$(az network nsg list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Id:id}" --output tsv) &amp;&amp; echo "AZURE_NSG_ID: \"$AZURE_NSG_ID\""</programlisting>
</listitem>
<listitem>
<simpara>Retrieve and record the Azure region:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_REGION=$(az group show --resource-group ${AZURE_RESOURCE_GROUP} --query "{Location:location}" --output tsv) &amp;&amp; echo "AZURE_REGION: \"$AZURE_REGION\""</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>peer-pods-cm.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "azure"
  VXLAN_PORT: "9000"
  AZURE_INSTANCE_SIZE: "Standard_DC2as_v5" <co xml:id="CO22-1"/>
  AZURE_INSTANCE_SIZES: "Standard_DC2as_v5,Standard_DC4as_v5,Standard_DC8as_v5,Standard_DC16as_v5" <co xml:id="CO22-2"/>
  AZURE_SUBNET_ID: "&lt;azure_subnet_id&gt;" <co xml:id="CO22-3"/>
  AZURE_NSG_ID: "&lt;azure_nsg_id&gt;" <co xml:id="CO22-4"/>
  PROXY_TIMEOUT: "5m"
  AZURE_IMAGE_ID: "&lt;azure_image_id&gt;" <co xml:id="CO22-5"/>
  AZURE_REGION: "&lt;azure_region&gt;" <co xml:id="CO22-6"/>
  AZURE_RESOURCE_GROUP: "&lt;azure_resource_group&gt;" <co xml:id="CO22-7"/>
  DISABLECVM: "false"
  AA_KBC_PARAMS: "cc_kbc::https://${TRUSTEE_HOST}" <co xml:id="CO22-8"/>
  ENABLE_SECURE_BOOT: "true" <co xml:id="CO22-9"/></programlisting>
<calloutlist>
<callout arearefs="CO22-1">
<para>This value is the default if an instance size is not defined in the workload.</para>
</callout>
<callout arearefs="CO22-2">
<para>Lists all of the instance sizes you can specify when creating the pod. This allows you to define smaller instance sizes for workloads that need less memory and fewer CPUs or larger instance sizes for larger workloads.</para>
</callout>
<callout arearefs="CO22-3">
<para>Specify the <literal>AZURE_SUBNET_ID</literal> value that you retrieved.</para>
</callout>
<callout arearefs="CO22-4">
<para>Specify the <literal>AZURE_NSG_ID</literal> value that you retrieved.</para>
</callout>
<callout arearefs="CO22-5">
<para>Optional: By default, this value is populated when you run the <literal>KataConfig</literal> CR, using an Azure image ID based on your cluster credentials. If you create your own Azure image, specify the correct image ID.</para>
</callout>
<callout arearefs="CO22-6">
<para>Specify the <literal>AZURE_REGION</literal> value you retrieved.</para>
</callout>
<callout arearefs="CO22-7">
<para>Specify the <literal>AZURE_RESOURCE_GROUP</literal> value you retrieved.</para>
</callout>
<callout arearefs="CO22-8">
<para>Specify the host name of the Trustee route.</para>
</callout>
<callout arearefs="CO22-9">
<para>Specify <literal>true</literal> to enable Secure Boot by default.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f peer-pods-cm.yaml</programlisting>
</listitem>
<listitem>
<simpara>Restart the <literal>peerpodconfig-ctrl-caa-daemon</literal> daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="deleting-cr-cli_azure-cc">
<title>Deleting the KataConfig custom resource</title>
<simpara>You can delete the <literal>KataConfig</literal> custom resource (CR) by using the command line.</simpara>
<simpara>Deleting the <literal>KataConfig</literal> CR removes the runtime and its related resources from your cluster.</simpara>
<important>
<simpara>Deleting the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>KataConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete kataconfig example-kataconfig</programlisting>
<simpara>The OpenShift sandboxed containers Operator removes all resources that were initially created to enable the runtime on your cluster.</simpara>
<important>
<simpara>When you delete the <literal>KataConfig</literal> CR, the CLI stops responding until all worker nodes reboot. You must for the deletion process to complete before performing the verification.</simpara>
</important>
</listitem>
<listitem>
<simpara>Verify that the custom resource was deleted by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kataconfig example-kataconfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">No example-kataconfig instances exist</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-kataconfig-cr-cli_azure-cc">
<title>Re-creating the KataConfig custom resource</title>
<simpara>You must re-create the <literal>KataConfig</literal> custom resource (CR) for Confidential Containers.</simpara>
<important>
<simpara>Creating the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard disk drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>example-kataconfig.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  enablePeerPods: true
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <co xml:id="CO23-1"/></programlisting>
<calloutlist>
<callout arearefs="CO23-1">
<para>Optional: If you have applied node labels to install <literal>kata-remote</literal> on specific nodes, specify the key and value, for example, <literal>cc: 'true'</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-kataconfig.yaml</programlisting>
<simpara>The new <literal>KataConfig</literal> CR is created and installs <literal>kata-remote</literal> as a runtime class on the worker nodes.</simpara>
<simpara>Wait for the <literal>kata-remote</literal> installation to complete and the worker nodes to reboot before verifying the installation.</simpara>
</listitem>
<listitem>
<simpara>Monitor the installation progress by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</programlisting>
<simpara>When the status of all workers under <literal>kataNodes</literal> is <literal>installed</literal> and the condition <literal>InProgress</literal> is <literal>False</literal> without specifying a reason, the <literal>kata-remote</literal> is installed on the cluster.</simpara>
</listitem>
<listitem>
<simpara>Verify the daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon</programlisting>
</listitem>
<listitem>
<simpara>Verify the runtime classes by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get runtimeclass</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME             HANDLER          AGE
kata             kata             152m
kata-remote      kata-remote      152m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-trustee-auth-secret_azure-cc">
<title>Creating the Trustee authentication secret</title>
<simpara>You must create the authentication secret for Trustee.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a private key by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openssl genpkey -algorithm ed25519 &gt; privateKey</programlisting>
</listitem>
<listitem>
<simpara>Create a public key by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openssl pkey -in privateKey -pubout -out publicKey</programlisting>
</listitem>
<listitem>
<simpara>Create a secret by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic kbs-auth-public-key --from-file=publicKey -n trustee-operator-system</programlisting>
</listitem>
<listitem>
<simpara>Verify the secret by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get secret -n trustee-operator-system</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-trustee-config-map_azure-cc">
<title>Creating the Trustee config map</title>
<simpara>You must create the config map to configure the Trustee server.</simpara>
<note>
<simpara>The following configuration example turns off security features to enable demonstration of Technology Preview features. It is not meant for a production environment.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created a route for Trustee.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>kbs-config-cm.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: kbs-config-cm
  namespace: trustee-operator-system
data:
  kbs-config.json: |
    {
      "insecure_http" : true,
      "sockets": ["0.0.0.0:8080"],
      "auth_public_key": "/etc/auth-secret/publicKey",
      "attestation_token_config": {
        "attestation_token_type": "CoCo"
      },
      "repository_config": {
        "type": "LocalFs",
        "dir_path": "/opt/confidential-containers/kbs/repository"
      },
      "as_config": {
        "work_dir": "/opt/confidential-containers/attestation-service",
        "policy_engine": "opa",
        "attestation_token_broker": "Simple",
          "attestation_token_config": {
          "duration_min": 5
          },
        "rvps_config": {
          "store_type": "LocalJson",
          "store_config": {
            "file_path": "/opt/confidential-containers/rvps/reference-values/reference-values.json"
          }
         }
      },
      "policy_engine_config": {
        "policy_path": "/opt/confidential-containers/opa/policy.rego"
      }
    }</programlisting>
</listitem>
<listitem>
<simpara>Create the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f kbs-config-cm.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-trustee_azure-cc">
<title>Configuring Trustee values, policies, and secrets</title>
<simpara>You can configure the following values, policies, and secrets for Trustee:</simpara>
<itemizedlist>
<listitem>
<simpara>Optional: Reference values for the Reference Value Provider Service.</simpara>
</listitem>
<listitem>
<simpara>Optional: Attestation policy.</simpara>
</listitem>
<listitem>
<simpara>Provisioning Certificate Caching Service for Intel Trust Domain Extensions (TDX).</simpara>
</listitem>
<listitem>
<simpara>Optional: Secret for custom keys for Trustee clients.</simpara>
</listitem>
<listitem>
<simpara>Optional: Secret for container image signature verification.</simpara>
</listitem>
<listitem>
<simpara>Container image signature verification policy. This policy is mandatory. If you do not use container image signature verification, you must create a policy that does not verify signatures.</simpara>
</listitem>
<listitem>
<simpara>Resource access policy.</simpara>
</listitem>
</itemizedlist>
<section xml:id="cc-configuring-reference-values_azure-cc">
<title>Configuring reference values</title>
<simpara>You can configure reference values for the Reference Value Provider Service (RVPS) by specifying the trusted digests of your hardware platform.</simpara>
<simpara>The client collects measurements from the running software, the Trusted Execution Environment (TEE) hardware and firmware and it submits a quote with the claims to the Attestation Server. These measurements must match the trusted digests registered to the Trustee. This process ensures that the confidential VM (CVM) is running the expected software stack and has not been tampered with.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>rvps-configmap.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: rvps-reference-values
  namespace: trustee-operator-system
data:
  reference-values.json: |
    [ <co xml:id="CO24-1"/>
    ]</programlisting>
<calloutlist>
<callout arearefs="CO24-1">
<para>Specify the trusted digests for your hardware platform if required. Otherwise, leave it empty.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the RVPS config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f rvps-configmap.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-attestation-policy_azure-cc">
<title>Creating an attestation policy</title>
<simpara>You can create an attestation policy that overrides the default attestation policy.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>attestation-policy.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: attestation-policy
  namespace: trustee-operator-system
data:
  default.rego: |
    package policy <co xml:id="CO25-1"/>
    import future.keywords.every

    default allow = false

    allow {
      every k, v in input {
          judge_field(k, v)
      }
    }

    judge_field(input_key, input_value) {
      has_key(data.reference, input_key)
      reference_value := data.reference[input_key]
      match_value(reference_value, input_value)
    }

    judge_field(input_key, input_value) {
      not has_key(data.reference, input_key)
    }

    match_value(reference_value, input_value) {
      not is_array(reference_value)
      input_value == reference_value
    }

    match_value(reference_value, input_value) {
      is_array(reference_value)
      array_include(reference_value, input_value)
    }

    array_include(reference_value_array, input_value) {
      reference_value_array == []
    }

    array_include(reference_value_array, input_value) {
      reference_value_array != []
      some i
      reference_value_array[i] == input_value
    }

    has_key(m, k) {
      _ = m[k]
    }</programlisting>
<calloutlist>
<callout arearefs="CO25-1">
<para>The attestation policy follows the <link xlink:href="https://www.openpolicyagent.org/docs/latest/policy-language/">Open Policy Agent</link> specification.
In this example, the attestation policy compares the claims provided in the attestation report to the reference values registered in the RVPS database. The attestation process is successful only if all the values match.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the attestation policy config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f attestation-policy.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-configuring-pccs-for-tdx_azure-cc">
<title>Configuring PCCS for TDX</title>
<simpara>If you use Intel Trust Domain Extensions (TDX), you must configure Trustee to use the Provisioning Certificate Caching Service (PCCS).</simpara>
<simpara>The PCCS retrieves the Provisioning Certification Key (PCK) certificates and caches them in a local database.</simpara>
<important>
<simpara>Do not use the public Intel PCCS service. Use a local caching service on-premise or on the public cloud.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>tdx-config.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: tdx-config
  namespace: trustee-operator-system
data:
  sgx_default_qcnl.conf: | \
      {
        "collateral_service": "https://api.trustedservices.intel.com/sgx/certification/v4/",
        "pccs_url": "&lt;pccs_url&gt;" <co xml:id="CO26-1"/>
      }</programlisting>
<calloutlist>
<callout arearefs="CO26-1">
<para>Specify the PCCS URL, for example, <literal>https://localhost:8081/sgx/certification/v4/</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the TDX config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f tdx-config.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-secret-for-clients_azure-cc">
<title>Creating a secret with custom keys for clients</title>
<simpara>You can create a secret that contains one or more custom keys for Trustee clients.</simpara>
<simpara>In this example, the <literal>kbsres1</literal> secret has two entries (<literal>key1</literal>, <literal>key2</literal>), which the clients retrieve. You can add additional secrets according to your requirements by using the same format.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created one or more custom keys.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a secret for the custom keys according to the following example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply secret generic kbsres1 \
  --from-literal key1=&lt;custom_key1&gt; \ <co xml:id="CO27-1"/>
  --from-literal key2=&lt;custom_key2&gt; \
  -n trustee-operator-system</programlisting>
<calloutlist>
<callout arearefs="CO27-1">
<para>Specify a custom key.</para>
</callout>
</calloutlist>
<simpara>The <literal>kbsres1</literal> secret is specified in the <literal>spec.kbsSecretResources</literal> key of the <literal>KbsConfig</literal> custom resource.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="cc-creating-secret-signed-container-images_azure-cc">
<title>Creating a secret for container image signature verification</title>
<simpara>If you use container image signature verification, you must create a secret that contains the public container image signing key.</simpara>
<simpara>The Key Broker Service on the Trustee cluster uses the secret to verify the signature, ensuring that only trusted and authenticated container images are deployed in your environment.</simpara>
<simpara>You can use <link xlink:href="https://developers.redhat.com/products/trusted-artifact-signer/overview">Red Hat Trusted Artifact Signer</link> or other tools to sign container images.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a secret for container image signature verification by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply secret generic &lt;type&gt; \ <co xml:id="CO28-1"/>
  --from-file=&lt;tag&gt;=./&lt;public_key_file&gt; \ <co xml:id="CO28-2"/>
  -n trustee-operator-system</programlisting>
<calloutlist>
<callout arearefs="CO28-1">
<para>Specify the KBS secret type, for example, <literal>img-sig</literal>.</para>
</callout>
<callout arearefs="CO28-2">
<para>Specify the secret tag, for example, <literal>pub-key</literal>, and the public container image signing key.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Record the <literal>&lt;type&gt;</literal> value. You must add this value to the <literal>spec.kbsSecretResources</literal> key when you create the <literal>KbsConfig</literal> custom resource.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-container-image-sig-policy_azure-cc">
<title>Creating the container image signature verification policy</title>
<simpara>You create the container image signature verification policy because signature verification is always enabled. If this policy is missing, the pods will not start.</simpara>
<simpara>If you are not using container image signature verification, you create the policy without signature verification.</simpara>
<simpara>For more information, see <link xlink:href="https://github.com/containers/image/blob/main/docs/containers-policy.json.5.md">containers-policy.json 5</link>.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>security-policy-config.json</literal> file according to the following examples:</simpara>
<itemizedlist>
<listitem>
<simpara>Without signature verification:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "default": [
  {
    "type": "insecureAcceptAnything"
  }],
  "transports": {}
}</programlisting>
</listitem>
<listitem>
<simpara>With signature verification:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "default": [
      {
      "type": "insecureAcceptAnything"
      }
  ],
  "transports": {
      "&lt;transport&gt;": { <co xml:id="CO29-1"/>
          "&lt;registry&gt;/&lt;image&gt;": <co xml:id="CO29-2"/>
          [
              {
                  "type": "sigstoreSigned",
                  "keyPath": "kbs:///default/&lt;type&gt;/&lt;tag&gt;" <co xml:id="CO29-3"/>
              }
          ]
      }
  }
}</programlisting>
<calloutlist>
<callout arearefs="CO29-1">
<para>Specify the image repository for <literal>transport</literal>, for example, <literal>"docker":</literal>. For more information, see <link xlink:href="https://github.com/containers/image/blob/main/docs/containers-transports.5.md">containers-transports 5</link>.</para>
</callout>
<callout arearefs="CO29-2">
<para>Specify the container registry and image, for example, "quay.io/my-image".</para>
</callout>
<callout arearefs="CO29-3">
<para>Specify the type and tag of the container image signature verification secret that you created, for example, <literal>img-sig/pub-key</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Create the security policy by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply secret generic security-policy \
  --from-file=osc=./&lt;security-policy-config.json&gt; \
  -n trustee-operator-system</programlisting>
<simpara>Do not alter the secret type, <literal>security-policy</literal>, or the key, <literal>osc</literal>.</simpara>
<simpara>The <literal>security-policy</literal> secret is specified in the <literal>spec.kbsSecretResources</literal> key of the <literal>KbsConfig</literal> custom resource.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-resource-access-policy_azure-cc">
<title>Creating the resource access policy</title>
<simpara>You configure the resource access policy for the Trustee policy engine. This policy determines which resources Trustee can access.</simpara>
<note>
<simpara>The Trustee policy engine is different from the Attestation Service policy engine, which determines the validity of TEE evidence.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>resourcepolicy-configmap.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: resource-policy
  namespace: trustee-operator-system
data:
  policy.rego: | <co xml:id="CO30-1"/>
    package policy <co xml:id="CO30-2"/>
    default allow = false
    allow {
      input["tee"] != "sample"
    }</programlisting>
<calloutlist>
<callout arearefs="CO30-1">
<para>The name of the resource policy, <literal>policy.rego</literal>, must match the resource policy defined in the Trustee config map.</para>
</callout>
<callout arearefs="CO30-2">
<para>The resource policy follows the <link xlink:href="https://www.openpolicyagent.org/docs/latest/policy-language/">Open Policy Agent</link> specification. This example allows the retrieval of all resources when the TEE is not the sample attester.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the resource policy config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f resourcepolicy-configmap.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="cc-creating-kbsconfig-cr_azure-cc">
<title>Creating the KbsConfig custom resource</title>
<simpara>You create the <literal>KbsConfig</literal> custom resource (CR) to launch Trustee.</simpara>
<simpara>Then, you check the Trustee pods and pod logs to verify the configuration.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>kbsconfig-cr.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: confidentialcontainers.org/v1alpha1
kind: KbsConfig
metadata:
  labels:
    app.kubernetes.io/name: kbsconfig
    app.kubernetes.io/instance: kbsconfig
    app.kubernetes.io/part-of: trustee-operator
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/created-by: trustee-operator
  name: kbsconfig
  namespace: trustee-operator-system
spec:
  kbsConfigMapName: kbs-config-cm
  kbsAuthSecretName: kbs-auth-public-key
  kbsDeploymentType: AllInOneDeployment
  kbsRvpsRefValuesConfigMapName: rvps-reference-values
  kbsSecretResources: ["kbsres1", "security-policy", "&lt;type&gt;"] <co xml:id="CO31-1"/>
  kbsResourcePolicyConfigMapName: resource-policy
# tdxConfigSpec:
#   kbsTdxConfigMapName: tdx-config <co xml:id="CO31-2"/>
# kbsAttestationPolicyConfigMapName: attestation-policy <co xml:id="CO31-3"/>
# kbsServiceType: &lt;service_type&gt; <co xml:id="CO31-4"/></programlisting>
<calloutlist>
<callout arearefs="CO31-1">
<para>Specify the <literal>type</literal> value of the container image signature verification secret you created, for example, <literal>img-sig</literal>.</para>
</callout>
<callout arearefs="CO31-2">
<para><literal>tdxConfigSpec.kbsTdxConfigMapName: tdx-config</literal> is required for Intel Trust Domain Extensions.</para>
</callout>
<callout arearefs="CO31-3">
<para><literal>kbsAttestationPolicyConfigMapName: attestation-policy</literal> is required if you create a customized attestation policy.</para>
</callout>
<callout arearefs="CO31-4">
<para><literal>kbsServiceType: &lt;service_type&gt;</literal> is required if you created a service type. Specify <literal>NodePort</literal>, <literal>LoadBalancer</literal>, or <literal>ExternalName</literal>. The default service type is <literal>ClusterIP</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>KbsConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f kbsconfig-cr.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-verifing-trustee-config_azure-cc">
<title>Verifying the Trustee configuration</title>
<simpara>You verify the Trustee configuration by checking the Trustee pods and logs.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Set the default project by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project trustee-operator-system</programlisting>
</listitem>
<listitem>
<simpara>Check the Trustee pods by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n trustee-operator-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">NAME                                                   READY   STATUS    RESTARTS   AGE
trustee-deployment-8585f98449-9bbgl                    1/1     Running   0          22m
trustee-operator-controller-manager-5fbd44cd97-55dlh   2/2     Running   0          59m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Set the <literal>POD_NAME</literal> environmental variable by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ POD_NAME=$(oc get pods -l app=kbs -o jsonpath='{.items[0].metadata.name}' -n trustee-operator-system)</programlisting>
</listitem>
<listitem>
<simpara>Check the pod logs by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n trustee-operator-system $POD_NAME</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">[2024-05-30T13:44:24Z INFO  kbs] Using config file /etc/kbs-config/kbs-config.json
[2024-05-30T13:44:24Z WARN  attestation_service::rvps] No RVPS address provided and will launch a built-in rvps
[2024-05-30T13:44:24Z INFO  attestation_service::token::simple] No Token Signer key in config file, create an ephemeral key and without CA pubkey cert
[2024-05-30T13:44:24Z INFO  api_server] Starting HTTPS server at [0.0.0.0:8080]
[2024-05-30T13:44:24Z INFO  actix_server::builder] starting 12 workers
[2024-05-30T13:44:24Z INFO  actix_server::server] Tokio runtime found; starting in existing Tokio runtime</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="verifying-attestation-process_azure-cc">
<title>Verifying the attestation process</title>
<simpara>You verify the attestation process by creating a test pod and retrieving its secret.</simpara>
<important>
<simpara>This procedure is an example to verify that attestation is working. Do not write sensitive data to standard I/O because the data can be captured by using a memory dump. Only data written to memory is encrypted.</simpara>
</important>
<simpara>By default, an agent side policy embedded in the pod VM image disables the <literal>exec</literal> and <literal>log</literal> APIs for a Confidential Containers pod. This policy ensures that sensitive data is not written to standard I/O.</simpara>
<simpara>In a test scenario, you can override the restriction at runtime by adding a policy annotation to the pod. For Technology Preview, runtime policy annotations are not verified by remote attestation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created a route if the Trustee server and the test pod are not running in the same cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>verification-pod.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: ocp-cc-pod
  labels:
    app: ocp-cc-pod
  annotations:
    io.katacontainers.config.agent.policy: cGFja2FnZSBhZ2VudF9wb2xpY3kKCmRlZmF1bHQgQWRkQVJQTmVpZ2hib3JzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgQWRkU3dhcFJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENsb3NlU3RkaW5SZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBDb3B5RmlsZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENyZWF0ZUNvbnRhaW5lclJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENyZWF0ZVNhbmRib3hSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBEZXN0cm95U2FuZGJveFJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IEV4ZWNQcm9jZXNzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgR2V0TWV0cmljc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IEdldE9PTUV2ZW50UmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgR3Vlc3REZXRhaWxzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgTGlzdEludGVyZmFjZXNSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBMaXN0Um91dGVzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgTWVtSG90cGx1Z0J5UHJvYmVSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBPbmxpbmVDUFVNZW1SZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBQYXVzZUNvbnRhaW5lclJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFB1bGxJbWFnZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFJlYWRTdHJlYW1SZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZW1vdmVDb250YWluZXJSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZW1vdmVTdGFsZVZpcnRpb2ZzU2hhcmVNb3VudHNSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZXNlZWRSYW5kb21EZXZSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZXN1bWVDb250YWluZXJSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBTZXRHdWVzdERhdGVUaW1lUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU2V0UG9saWN5UmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU2lnbmFsUHJvY2Vzc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFN0YXJ0Q29udGFpbmVyUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU3RhcnRUcmFjaW5nUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU3RhdHNDb250YWluZXJSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBTdG9wVHJhY2luZ1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFR0eVdpblJlc2l6ZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZUNvbnRhaW5lclJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZUVwaGVtZXJhbE1vdW50c1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZUludGVyZmFjZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZVJvdXRlc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFdhaXRQcm9jZXNzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgV3JpdGVTdHJlYW1SZXF1ZXN0IDo9IHRydWUK <co xml:id="CO32-1"/>
spec:
  runtimeClassName: kata-remote
  containers:
    - name: skr-openshift
      image: registry.access.redhat.com/ubi9/ubi:9.3
      command:
        - sleep
        - "36000"
      securityContext:
        privileged: false
        seccompProfile:
          type: RuntimeDefault</programlisting>
<calloutlist>
<callout arearefs="CO32-1">
<para>This pod annotation overrides the policy that prevents sensitive data from being written to standard I/O.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f verification-pod.yaml</programlisting>
</listitem>
<listitem>
<simpara>Connect to the Bash shell of the <literal>ocp-cc-pod</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -it ocp-cc-pod -- bash</programlisting>
</listitem>
<listitem>
<simpara>Fetch the pod secret by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl http://127.0.0.1:8006/cdh/resource/default/kbsres1/key1</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">res1val1</programlisting>
</para>
</formalpara>
<simpara>The Trustee server returns the secret only if the attestation is successful.</simpara>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="deploying-osc-ibm">
<title>Deploying OpenShift sandboxed containers on IBM Z and IBM LinuxONE</title>
<simpara>You can deploy OpenShift sandboxed containers on IBM Z® and IBM® LinuxONE.</simpara>
<simpara>OpenShift sandboxed containers deploys peer pods. The peer pod design circumvents the need for nested virtualization. For more information, see <link linkend="peer-pods">peer pods</link>.</simpara>
<important>
<simpara>OpenShift sandboxed containers on IBM Z® and IBM® LinuxONE is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<itemizedlist>
<title>Cluster requirements</title>
<listitem>
<simpara>You have installed Red Hat OpenShift Container Platform 4.14 or later on the cluster where you are installing the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>Your cluster has at least one worker node.</simpara>
</listitem>
</itemizedlist>
<section xml:id="peer-pod-resource-requirements_ibm-osc">
<title>Peer pod resource requirements</title>
<simpara>You must ensure that your cluster has sufficient resources.</simpara>
<simpara>Peer pod virtual machines (VMs) require resources in two locations:</simpara>
<itemizedlist>
<listitem>
<simpara>The worker node. The worker node stores metadata, Kata shim resources (<literal>containerd-shim-kata-v2</literal>), remote-hypervisor resources (<literal>cloud-api-adaptor</literal>), and the tunnel setup between the worker nodes and the peer pod VM.</simpara>
</listitem>
<listitem>
<simpara>The libvirt virtual machine instance. This is the actual peer pod VM running in the LPAR (KVM host).</simpara>
</listitem>
</itemizedlist>
<simpara>The CPU and memory resources used in the Kubernetes worker node are handled by the <link xlink:href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</link> included in the RuntimeClass (<literal>kata-remote</literal>) definition used for creating peer pods.</simpara>
<simpara>The total number of peer pod VMs running in the cloud is defined as Kubernetes Node extended resources.
This limit is per node and is set by the <literal>limit</literal> attribute in the <literal>peerpodConfig</literal> custom resource (CR).</simpara>
<simpara>The <literal>peerpodConfig</literal> CR, named <literal>peerpodconfig-openshift</literal>, is created when you create the <literal>kataConfig</literal> CR and enable peer pods, and is located in the <literal>openshift-sandboxed-containers-operator</literal> namespace.</simpara>
<simpara>The following <literal>peerpodConfig</literal> CR example displays the default <literal>spec</literal> values:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: confidentialcontainers.org/v1alpha1
kind: PeerPodConfig
metadata:
  name: peerpodconfig-openshift
  namespace: openshift-sandboxed-containers-operator
spec:
  cloudSecretName: peer-pods-secret
  configMapName: peer-pods-cm
  limit: "10" <co xml:id="CO33-1"/>
  nodeSelector:
    node-role.kubernetes.io/kata-oc: ""</programlisting>
<calloutlist>
<callout arearefs="CO33-1">
<para>The default limit is 10 VMs per node.</para>
</callout>
</calloutlist>
<simpara>The extended resource is named <literal>kata.peerpods.io/vm</literal>, and enables the Kubernetes scheduler to handle capacity tracking and accounting.</simpara>
<simpara>You can edit the limit per node based on the requirements for your environment after you install the OpenShift sandboxed containers Operator.</simpara>
<simpara>A <link xlink:href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">mutating webhook</link> adds the extended resource <literal>kata.peerpods.io/vm</literal> to the pod specification. It also removes any resource-specific entries from the pod specification, if present. This enables the Kubernetes scheduler to account for these extended resources, ensuring the peer pod is only scheduled when resources are available.</simpara>
<simpara>The mutating webhook modifies a Kubernetes pod as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>The mutating webhook checks the pod for the expected <literal>RuntimeClassName</literal> value, specified in the <literal>TARGET_RUNTIME_CLASS</literal> environment variable. If the value in the pod specification does not match the value in the <literal>TARGET_RUNTIME_CLASS</literal>, the webhook exits without modifying the pod.</simpara>
</listitem>
<listitem>
<simpara>If the <literal>RuntimeClassName</literal> values match, the webhook makes the following changes to the pod spec:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The webhook removes every resource specification from the <literal>resources</literal> field of all containers and init containers in the pod.</simpara>
</listitem>
<listitem>
<simpara>The webhook adds the extended resource (<literal>kata.peerpods.io/vm</literal>) to the spec by modifying the resources field of the first container in the pod. The extended resource <literal>kata.peerpods.io/vm</literal> is used by the Kubernetes scheduler for accounting purposes.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<note>
<simpara>The mutating webhook excludes specific system namespaces in OpenShift Container Platform from mutation. If a peer pod is created in those system namespaces, then resource accounting using Kubernetes extended resources does not work unless the pod spec includes the extended resource.</simpara>
<simpara>As a best practice, define a cluster-wide policy to only allow peer pod creation in specific namespaces.</simpara>
</note>
</section>
<section xml:id="deploying-osc-cli_ibm-osc">
<title>Deploying OpenShift sandboxed containers on IBM Z and IBM LinuxONE</title>
<simpara>You can deploy OpenShift sandboxed containers on IBM Z® and IBM® LinuxONE by using the command line interface (CLI) to perform the following tasks:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>Optional: Change the number of virtual machines running on each worker node.</simpara>
</listitem>
<listitem>
<simpara>Configure the libvirt volume.</simpara>
</listitem>
<listitem>
<simpara>Optional: Create a custom peer pod VM image.</simpara>
</listitem>
<listitem>
<simpara>Create the peer pods secret.</simpara>
</listitem>
<listitem>
<simpara>Create the peer pods config map.</simpara>
</listitem>
<listitem>
<simpara>Create the peer pod VM image config map.</simpara>
</listitem>
<listitem>
<simpara>Create the KVM host secret.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>Configure the OpenShift sandboxed containers workload objects.</simpara>
</listitem>
</orderedlist>
<section xml:id="installing-operator-cli_ibm-osc">
<title>Installing the OpenShift sandboxed containers Operator</title>
<simpara>You can install the OpenShift sandboxed containers Operator by using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>osc-namespace.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sandboxed-containers-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-namespace.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>osc-operatorgroup.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sandboxed-containers-operator-group
  namespace: openshift-sandboxed-containers-operator
spec:
  targetNamespaces:
  - openshift-sandboxed-containers-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the operator group by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-operatorgroup.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>osc-subscription.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sandboxed-containers-operator
  namespace: openshift-sandboxed-containers-operator
spec:
  channel: stable
  installPlanApproval: Automatic
  name: sandboxed-containers-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: sandboxed-containers-operator.v1.8.0</programlisting>
</listitem>
<listitem>
<simpara>Create the subscription by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f osc-subscription.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the Operator is correctly installed by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-sandboxed-containers-operator</programlisting>
<simpara>This command can take several minutes to complete.</simpara>
</listitem>
<listitem>
<simpara>Watch the process by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch oc get csv -n openshift-sandboxed-containers-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                             DISPLAY                                  VERSION             REPLACES                   PHASE
openshift-sandboxed-containers   openshift-sandboxed-containers-operator  1.8.0    1.7.0        Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</link> for disconnected environments.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="modifying-peer-pod-vm-limit_ibm-osc">
<title>Modifying the number of peer pod VMs per node</title>
<simpara>You can change the limit of peer pod virtual machines (VMs) per node by editing the <literal>peerpodConfig</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check the current limit by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
-o jsonpath='{.spec.limit}{"\n"}'</programlisting>
</listitem>
<listitem>
<simpara>Modify the <literal>limit</literal> attribute of the <literal>peerpodConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
--type merge --patch '{"spec":{"limit":"&lt;value&gt;"}}' <co xml:id="CO34-1"/></programlisting>
<calloutlist>
<callout arearefs="CO34-1">
<para>Replace &lt;value&gt; with the limit you want to define.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="ibm-configuring-libvirt_ibm-osc">
<title>Configuring the libvirt volume</title>
<simpara>You must configure the libvirt volume on your KVM host. Peer pods use the libvirt provider of the Cloud API Adaptor to create and manage virtual machines.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift sandboxed containers Operator on your OpenShift Container Platform cluster by using the OpenShift Container Platform web console or the command line.</simpara>
</listitem>
<listitem>
<simpara>You have administrator privileges for your KVM host.</simpara>
</listitem>
<listitem>
<simpara>You have installed <literal>podman</literal> on your KVM host.</simpara>
</listitem>
<listitem>
<simpara>You have installed <literal>virt-customize</literal> on your KVM host.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to the KVM host.</simpara>
</listitem>
<listitem>
<simpara>Set the name of the libvirt pool by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export LIBVIRT_POOL=&lt;libvirt_pool&gt;</programlisting>
<simpara>You need the <literal>LIBVIRT_POOL</literal> value to create the secret for the libvirt provider.</simpara>
</listitem>
<listitem>
<simpara>Set the name of the libvirt pool by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export LIBVIRT_VOL_NAME=&lt;libvirt_volume&gt;</programlisting>
<simpara>You need the <literal>LIBVIRT_VOL_NAME</literal> value to create the secret for the libvirt provider.</simpara>
</listitem>
<listitem>
<simpara>Set the path of the default storage pool location, by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export LIBVIRT_POOL_DIRECTORY=&lt;target_directory&gt; <co xml:id="CO35-1"/></programlisting>
<calloutlist>
<callout arearefs="CO35-1">
<para>To ensure libvirt has read and write access permissions, use a subdirectory of the libvirt storage directory. The default is <literal>/var/lib/libvirt/images/</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a libvirt pool by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virsh pool-define-as $LIBVIRT_POOL --type dir --target "$LIBVIRT_POOL_DIRECTORY"</programlisting>
</listitem>
<listitem>
<simpara>Start the libvirt pool by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virsh pool-start $LIBVIRT_POOL</programlisting>
</listitem>
<listitem>
<simpara>Create a libvirt volume for the pool by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virsh -c qemu:///system \
  vol-create-as --pool $LIBVIRT_POOL \
  --name $LIBVIRT_VOL_NAME \
  --capacity 20G \
  --allocation 2G \
  --prealloc-metadata \
  --format qcow2</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="ibm-embedding-podvm-image_ibm-osc">
<title>Creating a custom peer pod VM image</title>
<simpara>You can create a custom peer pod virtual machine (VM) image instead of using the default Operator-built image.</simpara>
<simpara>You build an Open Container Initiative (OCI) container with the peer pod QCOW2 image. Later, you add the container registry URL and the image path to the peer pod VM image config map.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Dockerfile.podvm-oci</literal> file:</simpara>
<programlisting language="docker" linenumbering="unnumbered">FROM scratch

ARG PODVM_IMAGE_SRC
ENV PODVM_IMAGE_PATH="/image/podvm.qcow2"

COPY $PODVM_IMAGE_SRC $PODVM_IMAGE_PATH</programlisting>
</listitem>
<listitem>
<simpara>Build a container with the pod VM QCOW2 image by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ docker build -t podvm-libvirt \
  --build-arg PODVM_IMAGE_SRC=&lt;podvm_image_source&gt; \ <co xml:id="CO36-1"/>
  --build-arg PODVM_IMAGE_PATH=&lt;podvm_image_path&gt; \ <co xml:id="CO36-2"/>
  -f Dockerfile.podvm-oci .</programlisting>
<calloutlist>
<callout arearefs="CO36-1">
<para>Specify the QCOW2 image source on the host.</para>
</callout>
<callout arearefs="CO36-2">
<para>Optional: Specify the path of the QCOW2 image if you do not use the default, <literal>/image/podvm.qcow2</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-peer-pods-secret_ibm-osc">
<title>Creating the peer pods secret</title>
<simpara>You must create the peer pods secret for OpenShift sandboxed containers.</simpara>
<simpara>The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.</simpara>
<simpara>By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><literal>LIBVIRT_POOL</literal>. Use the value you set when you configured libvirt on the KVM host.</simpara>
</listitem>
<listitem>
<simpara><literal>LIBVIRT_VOL_NAME</literal>. Use the value you set when you configured libvirt on the KVM host.</simpara>
</listitem>
<listitem>
<simpara><literal>LIBVIRT_URI</literal>. This value is the default gateway IP address of the libvirt network. Check your libvirt network setup to obtain this value.</simpara>
<note>
<simpara>If libvirt uses the default bridge virtual network, you can obtain the <literal>LIBVIRT_URI</literal> by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtint=$(bridge_line=$(virsh net-info default | grep Bridge);  echo "${bridge_line//Bridge:/}" | tr -d [:blank:])

$ LIBVIRT_URI=$( ip -4 addr show $virtint | grep -oP '(?&lt;=inet\s)\d+(\.\d+){3}')

$ LIBVIRT_GATEWAY_URI="qemu+ssh://root@${LIBVIRT_URI}/system?no_verify=1"</programlisting>
</note>
</listitem>
<listitem>
<simpara><literal>REDHAT_OFFLINE_TOKEN</literal>. You have generated this token to download the RHEL image at <link xlink:href="https://access.redhat.com/management/api">Red Hat API Tokens</link>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>peer-pods-secret.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  CLOUD_PROVIDER: "libvirt"
  LIBVIRT_URI: "&lt;libvirt_gateway_uri&gt;" <co xml:id="CO37-1"/>
  LIBVIRT_POOL: "&lt;libvirt_pool&gt;" <co xml:id="CO37-2"/>
  LIBVIRT_VOL_NAME: "&lt;libvirt_volume&gt;" <co xml:id="CO37-3"/>
  REDHAT_OFFLINE_TOKEN: "&lt;rh_offline_token&gt;" <co xml:id="CO37-4"/></programlisting>
<calloutlist>
<callout arearefs="CO37-1">
<para>Specify the libvirt URI.</para>
</callout>
<callout arearefs="CO37-2">
<para>Specify the libvirt pool.</para>
</callout>
<callout arearefs="CO37-3">
<para>Specify the libvirt volume name.</para>
</callout>
<callout arearefs="CO37-4">
<para>Specify the Red Hat offline token, which is required for the Operator-built image.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the secret by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f peer-pods-secret.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: To update an existing peer pods config map, restart the <literal>peerpodconfig-ctrl-caa-daemon</literal> daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-peer-pods-config-map_ibm-osc">
<title>Creating the peer pods config map</title>
<simpara>You must create the peer pods config map for OpenShift sandboxed containers.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>peer-pods-cm.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "libvirt"
  DISABLECVM: "true"</programlisting>
</listitem>
<listitem>
<simpara>Create the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f peer-pods-cm.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: To update an existing peer pods config map, restart the <literal>peerpodconfig-ctrl-caa-daemon</literal> daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-libvirt-config-map_ibm-osc">
<title>Creating the peer pod VM image config map</title>
<simpara>You must create the config map for the peer pod VM image.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>libvirt-podvm-image-cm.yaml</literal> manifest according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: libvirt-podvm-image-cm
  namespace: openshift-sandboxed-containers-operator
data:
  PODVM_DISTRO: "rhel"
  CAA_SRC: "https://github.com/confidential-containers/cloud-api-adaptor"
  CAA_REF: "&lt;cloud_api_adaptor_version&gt;" <co xml:id="CO38-1"/>
  DOWNLOAD_SOURCES: "no"
  CONFIDENTIAL_COMPUTE_ENABLED: "yes"
  UPDATE_PEERPODS_CM: "yes"
  ORG_ID: "&lt;rhel_organization_id&gt;"
  ACTIVATION_KEY: "&lt;rhel_activation_key&gt;" <co xml:id="CO38-2"/>
  IMAGE_NAME: "&lt;podvm_libvirt_image&gt;"
  PODVM_IMAGE_URI: "oci::&lt;image_repo_url&gt;:&lt;image_tag&gt;::&lt;image_path&gt;" <co xml:id="CO38-3"/>
  SE_BOOT: "true" <co xml:id="CO38-4"/>
  BASE_OS_VERSION: "&lt;rhel_image_os_version&gt;" <co xml:id="CO38-5"/></programlisting>
<calloutlist>
<callout arearefs="CO38-1">
<para>Specify the latest version of the Cloud API Adaptor source.</para>
</callout>
<callout arearefs="CO38-2">
<para>Specify your RHEL activation key.</para>
</callout>
<callout arearefs="CO38-3">
<para>Optional: Specify the following values if you created a container image:</para>
<itemizedlist>
<listitem>
<simpara><literal>image_repo_url</literal>: Container registry URL.</simpara>
</listitem>
<listitem>
<simpara><literal>image_tag</literal>: Image tag.</simpara>
</listitem>
<listitem>
<simpara><literal>image_path</literal>: Image path. Default: <literal>/image/podvm.qcow2</literal>.</simpara>
</listitem>
</itemizedlist>
</callout>
<callout arearefs="CO38-4">
<para><literal>SE_BOOT: "true"</literal> enables IBM Secure Execution for an Operator-built image. Set to <literal>false</literal> if you created a container image.</para>
</callout>
<callout arearefs="CO38-5">
<para>Specify the RHEL image operating system version. IBM Z® Secure Execution supports RHEL 9.4 and later versions.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f libvirt-podvm-image-cm.yaml</programlisting>
<simpara>The libvirt pod VM image config map is created for your libvirt provider.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-ssh-key-secret_ibm-osc">
<title>Creating the KVM host secret</title>
<simpara>You must create the secret for your KVM host.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to your OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>Generate an SSH key pair by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh-keygen -f ./id_rsa -N ""</programlisting>
</listitem>
<listitem>
<simpara>Copy the public SSH key to your KVM host:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh-copy-id -i ./id_rsa.pub &lt;KVM_HOST_IP&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>Secret</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic ssh-key-secret \
  -n openshift-sandboxed-containers-operator \
  --from-file=id_rsa.pub=./id_rsa.pub \
  --from-file=id_rsa=./id_rsa</programlisting>
</listitem>
<listitem>
<simpara>Delete the SSH keys you created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ shred --remove id_rsa.pub id_rsa</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-kataconfig-cr-cli_ibm-osc">
<title>Creating the KataConfig custom resource</title>
<simpara>You must create the <literal>KataConfig</literal> custom resource (CR) to install <literal>kata-remote</literal> as a runtime class on your worker nodes.</simpara>
<simpara>Creating the <literal>KataConfig</literal> CR triggers the OpenShift sandboxed containers Operator to do the following:
* Create a <literal>RuntimeClass</literal> CR named <literal>kata-remote</literal> with a default configuration. This enables users to configure workloads to use <literal>kata-remote</literal> as the runtime by referencing the CR in the <literal>RuntimeClassName</literal> field. This CR also specifies the resource overhead for the runtime.</simpara>
<simpara>OpenShift sandboxed containers installs <literal>kata-remote</literal> as a <emphasis>secondary, optional</emphasis> runtime on the cluster and not as the primary runtime.</simpara>
<important>
<simpara>Creating the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard disk drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>example-kataconfig.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  enablePeerPods: true
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <co xml:id="CO39-1"/></programlisting>
<calloutlist>
<callout arearefs="CO39-1">
<para>Optional: If you have applied node labels to install <literal>kata-remote</literal> on specific nodes, specify the key and value, for example, <literal>osc: 'true'</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-kataconfig.yaml</programlisting>
<simpara>The new <literal>KataConfig</literal> CR is created and installs <literal>kata-remote</literal> as a runtime class on the worker nodes.</simpara>
<simpara>Wait for the <literal>kata-remote</literal> installation to complete and the worker nodes to reboot before verifying the installation.</simpara>
</listitem>
<listitem>
<simpara>Monitor the installation progress by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</programlisting>
<simpara>When the status of all workers under <literal>kataNodes</literal> is <literal>installed</literal> and the condition <literal>InProgress</literal> is <literal>False</literal> without specifying a reason, the <literal>kata-remote</literal> is installed on the cluster.</simpara>
</listitem>
<listitem>
<simpara>Verify that you have built the peer pod image and uploaded it to the libvirt volume by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe configmap peer-pods-cm -n openshift-sandboxed-containers-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Name: peer-pods-cm
Namespace: openshift-sandboxed-containers-operator
Labels: &lt;none&gt;
Annotations: &lt;none&gt;

Data
====
CLOUD_PROVIDER: libvirt

BinaryData
====
Events: &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Monitor the <literal>kata-oc</literal> machine config pool progress to ensure that it is in the <literal>UPDATED</literal> state, when <literal>UPDATEDMACHINECOUNT</literal> equals <literal>MACHINECOUNT</literal>, by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch oc get mcp/kata-oc</programlisting>
</listitem>
<listitem>
<simpara>Verify the daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon</programlisting>
</listitem>
<listitem>
<simpara>Verify the runtime classes by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get runtimeclass</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME             HANDLER          AGE
kata             kata             152m
kata-remote      kata-remote      152m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-workload-objects_ibm-osc">
<title>Configuring workload objects</title>
<simpara>You must configure OpenShift sandboxed containers workload objects by setting <literal>kata-remote</literal> as the runtime class for the following pod-templated objects:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Pod</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicaSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>ReplicationController</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>StatefulSet</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>Deployment</literal> objects</simpara>
</listitem>
<listitem>
<simpara><literal>DeploymentConfig</literal> objects</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created the <literal>KataConfig</literal> custom resource (CR).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add <literal>spec.runtimeClassName: kata-remote</literal> to the manifest of each pod-templated workload object as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata-remote
# ...</programlisting>
<simpara>OpenShift Container Platform creates the workload object and begins scheduling it.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Inspect the <literal>spec.runtimeClassName</literal> field of a pod-templated object. If the value is <literal>kata-remote</literal>, then the workload is running on OpenShift sandboxed containers, using peer pods.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="deploying-cc_ibm-cc">
<title>Deploying Confidential Containers on IBM Z and IBM LinuxONE</title>
<simpara>You can deploy Confidential Containers on IBM Z® and IBM® LinuxONE after you deploy OpenShift sandboxed containers.</simpara>
<important>
<simpara>Confidential Containers on IBM Z® and IBM® LinuxONE is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<itemizedlist>
<title>Cluster requirements</title>
<listitem>
<simpara>You have installed Red Hat OpenShift Container Platform 4.15 or later on the cluster where you are installing the Confidential compute attestation Operator.</simpara>
</listitem>
</itemizedlist>
<simpara>You deploy Confidential Containers by performing the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the Confidential compute attestation Operator.</simpara>
</listitem>
<listitem>
<simpara>Create the route for Trustee.</simpara>
</listitem>
<listitem>
<simpara>Enable the Confidential Containers feature gate.</simpara>
</listitem>
<listitem>
<simpara>Update the peer pods config map.</simpara>
</listitem>
<listitem>
<simpara>Delete the <literal>KataConfig</literal> custom resource (CR).</simpara>
</listitem>
<listitem>
<simpara>Update the peer pods secret.</simpara>
</listitem>
<listitem>
<simpara>Re-create the <literal>KataConfig</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>Create the Trustee authentication secret.</simpara>
</listitem>
<listitem>
<simpara>Create the Trustee config map.</simpara>
</listitem>
<listitem>
<simpara>Obtain the IBM Secure Execution (SE) header.</simpara>
</listitem>
<listitem>
<simpara>Configure the SE certificates and keys.</simpara>
</listitem>
<listitem>
<simpara>Create the persistent storage components.</simpara>
</listitem>
<listitem>
<simpara>Configure Trustee values, policies, and secrets.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>KbsConfig</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>Verify the Trustee configuration.</simpara>
</listitem>
<listitem>
<simpara>Verify the attestation process.</simpara>
</listitem>
</orderedlist>
<section xml:id="cc-installing-cc-operator-cli_ibm-cc">
<title>Installing the Confidential compute attestation Operator</title>
<simpara>You can install the Confidential compute attestation Operator on IBM Z® and IBM® LinuxONE by using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>trustee-namespace.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: trustee-operator-system</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>trustee-operator-system</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f trustee-namespace.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>trustee-operatorgroup.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: trustee-operator-group
  namespace: trustee-operator-system
spec:
  targetNamespaces:
  - trustee-operator-system</programlisting>
</listitem>
<listitem>
<simpara>Create the operator group by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f trustee-operatorgroup.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>trustee-subscription.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: trustee-operator
  namespace: trustee-operator-system
spec:
  channel: stable
  installPlanApproval: Automatic
  name: trustee-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: trustee-operator.v0.1.0</programlisting>
</listitem>
<listitem>
<simpara>Create the subscription by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f trustee-subscription.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the Operator is correctly installed by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n trustee-operator-system</programlisting>
<simpara>This command can take several minutes to complete.</simpara>
</listitem>
<listitem>
<simpara>Watch the process by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch oc get csv -n trustee-operator-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                      DISPLAY                        PHASE
trustee-operator.v0.1.0   Trustee Operator  0.1.0        Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-enabling-feature-gate_ibm-cc">
<title>Enabling the Confidential Containers feature gate</title>
<simpara>You must enable the Confidential Containers feature gate.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have subscribed to the OpenShift sandboxed containers Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>cc-feature-gate.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: osc-feature-gates
  namespace: openshift-sandboxed-containers-operator
data:
  confidential: "true"</programlisting>
</listitem>
<listitem>
<simpara>Create the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f cc-feature-gate.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-route_ibm-cc">
<title>Creating the route for Trustee</title>
<simpara>You can create a secure route with edge TLS termination for Trustee. External ingress traffic reaches the router pods as HTTPS and passes on to the Trustee pods as HTTP.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the Confidential compute attestation Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an edge route by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create route edge --service=kbs-service --port kbs-port \
  -n trustee-operator-system</programlisting>
<note>
<simpara>Note: Currently, only a route with a valid CA-signed certificate is supported. You cannot use a route with self-signed certificate.</simpara>
</note>
</listitem>
<listitem>
<simpara>Set the <literal>TRUSTEE_HOST</literal> variable by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ TRUSTEE_HOST=$(oc get route -n trustee-operator-system kbs-service \
  -o jsonpath={.spec.host})</programlisting>
</listitem>
<listitem>
<simpara>Verify the route by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $TRUSTEE_HOST</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">kbs-service-trustee-operator-system.apps.memvjias.eastus.aroapp.io</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-peer-pods-config-map_ibm-cc">
<title>Updating the peer pods config map</title>
<simpara>You must update the peer pods config map for Confidential Containers.</simpara>
<note>
<simpara>Set Secure Boot to <literal>true</literal> to enable it by default. The default value is <literal>false</literal>, which presents a security risk.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>peer-pods-cm.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "libvirt"
  DISABLECVM: "false"</programlisting>
</listitem>
<listitem>
<simpara>Create the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f peer-pods-cm.yaml</programlisting>
</listitem>
<listitem>
<simpara>Restart the <literal>peerpodconfig-ctrl-caa-daemon</literal> daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="deleting-cr-cli_ibm-cc">
<title>Deleting the KataConfig custom resource</title>
<simpara>You can delete the <literal>KataConfig</literal> custom resource (CR) by using the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>KataConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete kataconfig example-kataconfig</programlisting>
</listitem>
<listitem>
<simpara>Verify that the custom resource was deleted by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kataconfig example-kataconfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">No example-kataconfig instances exist</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-peer-pods-secret_ibm-cc">
<title>Updating the peer pods secret</title>
<simpara>You must update the peer pods secret for Confidential Containers.</simpara>
<simpara>The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.</simpara>
<simpara>By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><literal>REDHAT_OFFLINE_TOKEN</literal>. You have generated this token to download the RHEL image at <link xlink:href="https://access.redhat.com/management/api">Red Hat API Tokens</link>.</simpara>
</listitem>
<listitem>
<simpara><literal>HKD_CRT</literal>. The Host Key Document (HKD) certificate enables secure execution on IBM Z®. For more information, see <link xlink:href="https://www.ibm.com/docs/en/linux-on-systems?topic=linux-obtain-host-key-document">Obtaining a host key document from Resource Link</link> in the IBM documentation.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>peer-pods-secret.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  REDHAT_OFFLINE_TOKEN: "&lt;rh_offline_token&gt;" <co xml:id="CO40-1"/>
  HKD_CRT: "&lt;hkd_crt_value&gt;" <co xml:id="CO40-2"/></programlisting>
<calloutlist>
<callout arearefs="CO40-1">
<para>Specify the Red Hat offline token, which is required for the Operator-built image.</para>
</callout>
<callout arearefs="CO40-2">
<para>Specify the HKD certificate value to enable IBM Secure Execution for the Operator-built image.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the secret by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f peer-pods-secret.yaml</programlisting>
</listitem>
<listitem>
<simpara>Restart the <literal>peerpodconfig-ctrl-caa-daemon</literal> daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-kataconfig-cr-cli_ibm-cc">
<title>Re-creating the KataConfig custom resource</title>
<simpara>You must re-create the <literal>KataConfig</literal> custom resource (CR) for Confidential Containers.</simpara>
<important>
<simpara>Creating the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard disk drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>example-kataconfig.yaml</literal> manifest file according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  enablePeerPods: true
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <co xml:id="CO41-1"/></programlisting>
<calloutlist>
<callout arearefs="CO41-1">
<para>Optional: If you have applied node labels to install <literal>kata-remote</literal> on specific nodes, specify the key and value, for example, <literal>cc: 'true'</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>KataConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-kataconfig.yaml</programlisting>
<simpara>The new <literal>KataConfig</literal> CR is created and installs <literal>kata-remote</literal> as a runtime class on the worker nodes.</simpara>
<simpara>Wait for the <literal>kata-remote</literal> installation to complete and the worker nodes to reboot before verifying the installation.</simpara>
</listitem>
<listitem>
<simpara>Monitor the installation progress by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</programlisting>
<simpara>When the status of all workers under <literal>kataNodes</literal> is <literal>installed</literal> and the condition <literal>InProgress</literal> is <literal>False</literal> without specifying a reason, the <literal>kata-remote</literal> is installed on the cluster.</simpara>
</listitem>
<listitem>
<simpara>Verify that you have built the peer pod image and uploaded it to the libvirt volume by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe configmap peer-pods-cm -n openshift-sandboxed-containers-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Name: peer-pods-cm
Namespace: openshift-sandboxed-containers-operator
Labels: &lt;none&gt;
Annotations: &lt;none&gt;

Data
====
CLOUD_PROVIDER: libvirt
DISABLECVM: false <co xml:id="CO42-1"/>
LIBVIRT_IMAGE_ID: fa-pp-vol <co xml:id="CO42-2"/>

BinaryData
====
Events: &lt;none&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO42-1">
<para>Enables the Confidential VM during IBM Secure Execution for the Operator-built image.</para>
</callout>
<callout arearefs="CO42-2">
<para>Contains a value if you have built the peer pod image and uploaded it to the libvirt volume.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Monitor the <literal>kata-oc</literal> machine config pool progress to ensure that it is in the <literal>UPDATED</literal> state, when <literal>UPDATEDMACHINECOUNT</literal> equals <literal>MACHINECOUNT</literal>, by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch oc get mcp/kata-oc</programlisting>
</listitem>
<listitem>
<simpara>Verify the daemon set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon</programlisting>
</listitem>
<listitem>
<simpara>Verify the runtime classes by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get runtimeclass</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME             HANDLER          AGE
kata             kata             152m
kata-remote      kata-remote      152m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-trustee-auth-secret_ibm-cc">
<title>Creating the Trustee authentication secret</title>
<simpara>You must create the authentication secret for Trustee.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a private key by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openssl genpkey -algorithm ed25519 &gt; privateKey</programlisting>
</listitem>
<listitem>
<simpara>Create a public key by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openssl pkey -in privateKey -pubout -out publicKey</programlisting>
</listitem>
<listitem>
<simpara>Create a secret by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic kbs-auth-public-key --from-file=publicKey -n trustee-operator-system</programlisting>
</listitem>
<listitem>
<simpara>Verify the secret by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get secret -n trustee-operator-system</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-trustee-config-map_ibm-cc">
<title>Creating the Trustee config map</title>
<simpara>You must create the config map to configure the Trustee server.</simpara>
<note>
<simpara>The following configuration example turns off security features to enable demonstration of Technology Preview features. It is not meant for a production environment.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created a route for Trustee.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>kbs-config-cm.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: kbs-config-cm
  namespace: trustee-operator-system
data:
  kbs-config.json: |
    {
      "insecure_http" : true,
      "sockets": ["0.0.0.0:8080"],
      "auth_public_key": "/etc/auth-secret/publicKey",
      "attestation_token_config": {
        "attestation_token_type": "CoCo"
      },
      "repository_config": {
        "type": "LocalFs",
        "dir_path": "/opt/confidential-containers/kbs/repository"
      },
      "as_config": {
        "work_dir": "/opt/confidential-containers/attestation-service",
        "policy_engine": "opa",
        "attestation_token_broker": "Simple",
          "attestation_token_config": {
          "duration_min": 5
          },
        "rvps_config": {
          "store_type": "LocalJson",
          "store_config": {
            "file_path": "/opt/confidential-containers/rvps/reference-values/reference-values.json"
          }
         }
      },
      "policy_engine_config": {
        "policy_path": "/opt/confidential-containers/opa/policy.rego"
      }
    }</programlisting>
</listitem>
<listitem>
<simpara>Create the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f kbs-config-cm.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="ibm-cc-obtaining-se-header.adoc_ibm-cc">
<title>Obtaining the IBM Secure Execution header</title>
<simpara>You must obtain the IBM Secure Execution (SE) header.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have a network block storage device to store the SE header temporarily.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a temporary folder for the SE header by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ mkdir -p /tmp/ibmse/hdr</programlisting>
</listitem>
<listitem>
<simpara>Download the <literal>pvextract-hdr</literal> tool from <link xlink:href="https://github.com/ibm-s390-linux/s390-tools/blob/v2.33.1/rust/pvattest/tools/pvextract-hdr">IBM s390 Linux</link> repository by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ wget https://github.com/ibm-s390-linux/s390-tools/raw/v2.33.1/rust/pvattest/tools/pvextract-hdr -O /tmp/pvextract-hdr</programlisting>
</listitem>
<listitem>
<simpara>Make the tool executable by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ chmod +x /tmp/pvextract-hdr</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>$IMAGE_OUTPUT_DIR</literal> variable by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export IMAGE=$IMAGE_OUTPUT_DIR/se-podvm-commit-short-id.qcow2</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>$IMAGE</literal> variable by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export IMAGE=/root/rooo/se-podvm-d1fb986-dirty-s390x.qcow2</programlisting>
</listitem>
<listitem>
<simpara>Enable the <literal>nbd</literal> kernel module by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ modprobe nbd</programlisting>
</listitem>
<listitem>
<simpara>Connect the SE image as a network block device (NBD) by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ qemu-nbd --connect=/dev/nbd0 $IMAGE</programlisting>
</listitem>
<listitem>
<simpara>Create a mount directory for the SE image by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ mkdir -p /mnt/se-image/</programlisting>
</listitem>
<listitem>
<simpara>Pause the process by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sleep 1</programlisting>
</listitem>
<listitem>
<simpara>List your block devices by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ lsblk</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">nbd0                                           43:0    0  100G  0 disk
├─nbd0p1                                       43:1    0  255M  0 part
├─nbd0p2                                       43:2    0    6G  0 part
│ └─luks-e23e15fa-9c2a-45a5-9275-aae9d8e709c3 253:2    0    6G  0 crypt
└─nbd0p3                                       43:3    0 12.4G  0 part
nbd1                                           43:32   0   20G  0 disk
├─nbd1p1                                       43:33   0  255M  0 part
├─nbd1p2                                       43:34   0    6G  0 part
│ └─luks-5a540f7c-c0cb-419b-95e0-487670d91525 253:3    0    6G  0 crypt
└─nbd1p3                                       43:35   0 86.9G  0 part
nbd2                                           43:64   0    0B  0 disk
nbd3                                           43:96   0    0B  0 disk
nbd4                                           43:128  0    0B  0 disk
nbd5                                           43:160  0    0B  0 disk
nbd6                                           43:192  0    0B  0 disk
nbd7                                           43:224  0    0B  0 disk
nbd8                                           43:256  0    0B  0 disk
nbd9                                           43:288  0    0B  0 disk
nbd10                                          43:320  0    0B  0 disk</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Mount the SE image directory on an available NBD partition and extract the SE header by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ mount /dev/&lt;nbdXp1&gt; /mnt/se-image/ /tmp/pvextract-hdr \
  -o /tmp/ibmse/hdr/hdr.bin /mnt/se-image/se.img</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">SE header found at offset 0x014000
SE header written to '/tmp/ibmse/hdr/hdr.bin' (640 bytes)</programlisting>
</para>
</formalpara>
<simpara>The following error is displayed if the NBD is unavailable:</simpara>
<programlisting language="text" linenumbering="unnumbered">mount: /mnt/se-image: can't read superblock on /dev/nbd0p1</programlisting>
</listitem>
<listitem>
<simpara>Unmount the SE image directory by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ umount /mnt/se-image/</programlisting>
</listitem>
<listitem>
<simpara>Disconnect the network block storage device by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ qemu-nbd --disconnect /dev/nbd0</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="ibm-cc-obtaining-attest-fields-certs-keys_ibm-cc">
<title>Configuring the IBM Secure Execution certificates and keys</title>
<simpara>You must configure the IBM Secure Execution (SE) certificates and keys for your worker nodes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have the IP address of the bastion node.</simpara>
</listitem>
<listitem>
<simpara>You have the internal IP addresses of the worker nodes.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the attestation policy fields by performing the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Download the <literal>se_parse_hdr.py</literal> script from the OpenShift Trustee repository by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ wget https://github.com/openshift/trustee/raw/main/attestation-service/verifier/src/se/se_parse_hdr.py -O /tmp/se_parse_hdr.py</programlisting>
</listitem>
<listitem>
<simpara>Create a temporary directory for the SE Host Key Document (HKD) certificate by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ mkdir /tmp/ibmse/hkds/</programlisting>
</listitem>
<listitem>
<simpara>Copy your Host Key Document (HKD) certificate to the temporary directory by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cp ~/path/to/&lt;hkd_cert.crt&gt; /tmp/ibmse/hkds/&lt;hkd_cert.crt&gt;</programlisting>
<note>
<simpara>The HKD certificate must be the same certificate that you downloaded when you created the peer pods secret.</simpara>
</note>
</listitem>
<listitem>
<simpara>Obtain the attestation policy fields by running the <literal>se_parse_hdr.py</literal> script:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ python3 /tmp/se_parse_hdr.py /tmp/ibmse/hdr/hdr.bin /tmp/ibmse/hkds/&lt;hkd_cert.crt&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">...
  ================================================
  se.image_phkh: xxx
  se.version: 256
  se.tag: xxx
  se.attestation_phkh: xxx</programlisting>
</para>
</formalpara>
<simpara>Record these values for the SE attestation policy config map.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Obtain the certificates and certificate revocation lists (CRLs) by performing the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a temporary directory for certificates by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ mkdir /tmp/ibmse/certs</programlisting>
</listitem>
<listitem>
<simpara>Download the <literal>ibm-z-host-key-signing-gen2.crt</literal> certificate by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ wget https://www.ibm.com/support/resourcelink/api/content/public/ibm-z-host-key-signing-gen2.crt -O /tmp/ibmse/certs/ibm-z-host-key-signing-gen2.crt</programlisting>
</listitem>
<listitem>
<simpara>Download the <literal>DigiCertCA.crt</literal> certificate by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ wget https://www.ibm.com/support/resourcelink/api/content/public/DigiCertCA.crt -O /tmp/ibmse/certs/DigiCertCA.crt</programlisting>
</listitem>
<listitem>
<simpara>Create a temporary directory for the CRLs by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ mkdir /tmp/ibmse/crls</programlisting>
</listitem>
<listitem>
<simpara>Download the <literal>DigiCertTrustedRootG4.crl</literal> file by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ wget http://crl3.digicert.com/DigiCertTrustedRootG4.crl -O /tmp/ibmse/crls/DigiCertTrustedRootG4.crl</programlisting>
</listitem>
<listitem>
<simpara>Download the <literal>DigiCertTrustedG4CodeSigningRSA4096SHA3842021CA1.crl</literal> file by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ wget http://crl3.digicert.com/DigiCertTrustedG4CodeSigningRSA4096SHA3842021CA1.crl -O /tmp/ibmse/crls/DigiCertTrustedG4CodeSigningRSA4096SHA3842021CA1.crl</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Generate the RSA keys:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Generate an RSA key pair by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openssl genrsa -aes256 -passout pass:&lt;password&gt; -out /tmp/encrypt_key-psw.pem 4096 <co xml:id="CO43-1"/></programlisting>
<calloutlist>
<callout arearefs="CO43-1">
<para>Specify the RSA key password.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a temporary directory for the RSA keys by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ mkdir /tmp/ibmse/rsa</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>encrypt_key.pub</literal> key by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openssl rsa -in /tmp/encrypt_key-psw.pem -passin pass:&lt;password&gt; -pubout -out /tmp/ibmse/rsa/encrypt_key.pub</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>encrypt_key.pem</literal> key by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openssl rsa -in /tmp/encrypt_key-psw.pem -passin pass:&lt;password&gt; -out /tmp/ibmse/rsa/encrypt_key.pem</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify the structure of the <literal>/tmp/ibmse</literal> directory by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tree /tmp/ibmse</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">/tmp/ibmse
├── certs
│   ├── ibm-z-host-key-signing-gen2.crt
|   └── DigiCertCA.crt
├── crls
│   └── ibm-z-host-key-gen2.crl
│   └── DigiCertTrustedRootG4.crl
│   └── DigiCertTrustedG4CodeSigningRSA4096SHA3842021CA1.crl
├── hdr
│   └── hdr.bin
├── hkds
│   └── &lt;hkd_cert.crt&gt;
└── rsa
    ├── encrypt_key.pem
    └── encrypt_key.pub</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Copy these files to the OpenShift Container Platform worker nodes by performing the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a compressed file from the <literal>/tmp/ibmse</literal> directory by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tar -czf ibmse.tar.gz -C /tmp/ibmse</programlisting>
</listitem>
<listitem>
<simpara>Copy the <literal>.tar.gz</literal> file to the bastion node in your cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ scp /tmp/ibmse.tar.gz root@&lt;ocp_bastion_ip&gt;:/tmp <co xml:id="CO44-1"/></programlisting>
<calloutlist>
<callout arearefs="CO44-1">
<para>Specify the IP address of the bastion node.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Connect to the bastion node over SSH by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh root@&lt;ocp_bastion_ip&gt;</programlisting>
</listitem>
<listitem>
<simpara>Copy the <literal>.tar.gz</literal> file to each worker node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ scp /tmp/ibmse.tar.gz core@&lt;worker_node_ip&gt;:/tmp <co xml:id="CO45-1"/></programlisting>
<calloutlist>
<callout arearefs="CO45-1">
<para>Specify the IP address of the worker node.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Extract the <literal>.tar.gz</literal> on each worker node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh core@&lt;worker_node_ip&gt; 'sudo mkdir -p /opt/confidential-containers/ &amp;&amp; sudo tar -xzf /tmp/ibmse.tar.gz -C /opt/confidential-containers/'</programlisting>
</listitem>
<listitem>
<simpara>Update the <literal>ibmse</literal> folder permissions by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh core@&lt;worker_node_ip&gt; 'sudo chmod -R 755 /opt/confidential-containers/ibmse/'</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="ibm-cc-creating-persistent-storage-components_ibm-cc">
<title>Creating the persistent storage components</title>
<simpara>You must create persistent storage components, persistent volume (PV) and persistent volume claim (PVC) to mount the <literal>ibmse</literal> folder to the Trustee pod.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>persistent-volume.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: PersistentVolume
metadata:
  name: ibmse-pv
  namespace: trustee-operator-system
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadOnlyMany
  storageClassName: ""
  local:
    path: /opt/confidential-containers/ibmse
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: node-role.kubernetes.io/worker
              operator: Exists</programlisting>
</listitem>
<listitem>
<simpara>Create the persistent volume by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f persistent-volume.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>persistent-volume-claim.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ibmse-pvc
  namespace: trustee-operator-system
spec:
  accessModes:
    - ReadOnlyMany
  storageClassName: ""
  resources:
    requests:
      storage: 100Mi</programlisting>
</listitem>
<listitem>
<simpara>Create the persistent volume claim by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f persistent-volume-claim.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-trustee_ibm-cc">
<title>Configuring Trustee values, policies, and secrets</title>
<simpara>You can configure the following values, policies, and secrets for Trustee:</simpara>
<itemizedlist>
<listitem>
<simpara>Optional: Reference values for the Reference Value Provider Service.</simpara>
</listitem>
<listitem>
<simpara>Attestation policy for IBM Secure Execution.</simpara>
</listitem>
<listitem>
<simpara>Optional: Secret for custom keys for Trustee clients.</simpara>
</listitem>
<listitem>
<simpara>Optional: Secret for container image signature verification.</simpara>
</listitem>
<listitem>
<simpara>Container image signature verification policy. This policy is mandatory. If you do not use container image signature verification, you must create a policy that does not verify signatures.</simpara>
</listitem>
<listitem>
<simpara>Resource access policy.</simpara>
</listitem>
</itemizedlist>
<section xml:id="cc-configuring-reference-values_ibm-cc">
<title>Configuring reference values</title>
<simpara>You can configure reference values for the Reference Value Provider Service (RVPS) by specifying the trusted digests of your hardware platform.</simpara>
<simpara>The client collects measurements from the running software, the Trusted Execution Environment (TEE) hardware and firmware and it submits a quote with the claims to the Attestation Server. These measurements must match the trusted digests registered to the Trustee. This process ensures that the confidential VM (CVM) is running the expected software stack and has not been tampered with.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>rvps-configmap.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: rvps-reference-values
  namespace: trustee-operator-system
data:
  reference-values.json: |
    [ <co xml:id="CO46-1"/>
    ]</programlisting>
<calloutlist>
<callout arearefs="CO46-1">
<para>Leave this value empty.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the RVPS config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f rvps-configmap.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-attestation-policy_ibm-cc">
<title>Creating the attestation policy for IBM Secure Execution</title>
<simpara>You must create the attestation policy for IBM Secure Execution.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>attestation-policy.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: attestation-policy
  namespace: trustee-operator-system
data:
  default.rego: | <co xml:id="CO47-1"/>
    package policy
    import rego.v1
    default allow = false
    converted_version := sprintf("%v", [input["se.version"]])
    allow if {
        input["se.attestation_phkh"] == "&lt;se.attestation_phkh&gt;" <co xml:id="CO47-2"/>
        input["se.image_phkh"] == "&lt;se.image_phkh&gt;"
        input["se.tag"] == "&lt;se.tag&gt;"
        converted_version == "256"
    }</programlisting>
<calloutlist>
<callout arearefs="CO47-1">
<para>Do not modify the policy name.</para>
</callout>
<callout arearefs="CO47-2">
<para>Specify the attestation policy fields you obtained by running the <literal>se_parse_hdr.py</literal> script.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the attestation policy config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f attestation-policy.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-secret-for-clients_ibm-cc">
<title>Creating a secret with custom keys for clients</title>
<simpara>You can create a secret that contains one or more custom keys for Trustee clients.</simpara>
<simpara>In this example, the <literal>kbsres1</literal> secret has two entries (<literal>key1</literal>, <literal>key2</literal>), which the clients retrieve. You can add additional secrets according to your requirements by using the same format.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created one or more custom keys.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a secret for the custom keys according to the following example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply secret generic kbsres1 \
  --from-literal key1=&lt;custom_key1&gt; \ <co xml:id="CO48-1"/>
  --from-literal key2=&lt;custom_key2&gt; \
  -n trustee-operator-system</programlisting>
<calloutlist>
<callout arearefs="CO48-1">
<para>Specify a custom key.</para>
</callout>
</calloutlist>
<simpara>The <literal>kbsres1</literal> secret is specified in the <literal>spec.kbsSecretResources</literal> key of the <literal>KbsConfig</literal> custom resource.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="cc-creating-secret-signed-container-images_ibm-cc">
<title>Creating a secret for container image signature verification</title>
<simpara>If you use container image signature verification, you must create a secret that contains the public container image signing key.</simpara>
<simpara>The Key Broker Service on the Trustee cluster uses the secret to verify the signature, ensuring that only trusted and authenticated container images are deployed in your environment.</simpara>
<simpara>You can use <link xlink:href="https://developers.redhat.com/products/trusted-artifact-signer/overview">Red Hat Trusted Artifact Signer</link> or other tools to sign container images.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a secret for container image signature verification by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply secret generic &lt;type&gt; \ <co xml:id="CO49-1"/>
  --from-file=&lt;tag&gt;=./&lt;public_key_file&gt; \ <co xml:id="CO49-2"/>
  -n trustee-operator-system</programlisting>
<calloutlist>
<callout arearefs="CO49-1">
<para>Specify the KBS secret type, for example, <literal>img-sig</literal>.</para>
</callout>
<callout arearefs="CO49-2">
<para>Specify the secret tag, for example, <literal>pub-key</literal>, and the public container image signing key.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Record the <literal>&lt;type&gt;</literal> value. You must add this value to the <literal>spec.kbsSecretResources</literal> key when you create the <literal>KbsConfig</literal> custom resource.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-container-image-sig-policy_ibm-cc">
<title>Creating the container image signature verification policy</title>
<simpara>You create the container image signature verification policy because signature verification is always enabled. If this policy is missing, the pods will not start.</simpara>
<simpara>If you are not using container image signature verification, you create the policy without signature verification.</simpara>
<simpara>For more information, see <link xlink:href="https://github.com/containers/image/blob/main/docs/containers-policy.json.5.md">containers-policy.json 5</link>.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>security-policy-config.json</literal> file according to the following examples:</simpara>
<itemizedlist>
<listitem>
<simpara>Without signature verification:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "default": [
  {
    "type": "insecureAcceptAnything"
  }],
  "transports": {}
}</programlisting>
</listitem>
<listitem>
<simpara>With signature verification:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "default": [
      {
      "type": "insecureAcceptAnything"
      }
  ],
  "transports": {
      "&lt;transport&gt;": { <co xml:id="CO50-1"/>
          "&lt;registry&gt;/&lt;image&gt;": <co xml:id="CO50-2"/>
          [
              {
                  "type": "sigstoreSigned",
                  "keyPath": "kbs:///default/&lt;type&gt;/&lt;tag&gt;" <co xml:id="CO50-3"/>
              }
          ]
      }
  }
}</programlisting>
<calloutlist>
<callout arearefs="CO50-1">
<para>Specify the image repository for <literal>transport</literal>, for example, <literal>"docker":</literal>. For more information, see <link xlink:href="https://github.com/containers/image/blob/main/docs/containers-transports.5.md">containers-transports 5</link>.</para>
</callout>
<callout arearefs="CO50-2">
<para>Specify the container registry and image, for example, "quay.io/my-image".</para>
</callout>
<callout arearefs="CO50-3">
<para>Specify the type and tag of the container image signature verification secret that you created, for example, <literal>img-sig/pub-key</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Create the security policy by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply secret generic security-policy \
  --from-file=osc=./&lt;security-policy-config.json&gt; \
  -n trustee-operator-system</programlisting>
<simpara>Do not alter the secret type, <literal>security-policy</literal>, or the key, <literal>osc</literal>.</simpara>
<simpara>The <literal>security-policy</literal> secret is specified in the <literal>spec.kbsSecretResources</literal> key of the <literal>KbsConfig</literal> custom resource.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-creating-resource-access-policy_ibm-cc">
<title>Creating the resource access policy</title>
<simpara>You configure the resource access policy for the Trustee policy engine. This policy determines which resources Trustee can access.</simpara>
<note>
<simpara>The Trustee policy engine is different from the Attestation Service policy engine, which determines the validity of TEE evidence.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>resourcepolicy-configmap.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: resource-policy
  namespace: trustee-operator-system
data:
  policy.rego: | <co xml:id="CO51-1"/>
    package policy <co xml:id="CO51-2"/>
    path := split(data["resource-path"], "/")
    default allow = false
    allow {
      count(path) == 3
      input["tee"] == "se"
    }</programlisting>
<calloutlist>
<callout arearefs="CO51-1">
<para>The name of the resource policy, <literal>policy.rego</literal>, must match the resource policy defined in the Trustee config map.</para>
</callout>
<callout arearefs="CO51-2">
<para>The resource policy follows the <link xlink:href="https://www.openpolicyagent.org/docs/latest/policy-language/">Open Policy Agent</link> specification. This example allows the retrieval of all resources when the TEE is not the sample attester.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the resource policy config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f resourcepolicy-configmap.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="cc-creating-kbsconfig-cr_ibm-cc">
<title>Creating the KbsConfig custom resource</title>
<simpara>You create the <literal>KbsConfig</literal> custom resource (CR) to launch Trustee.</simpara>
<simpara>Then, you check the Trustee pods and pod logs to verify the configuration.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>kbsconfig-cr.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: confidentialcontainers.org/v1alpha1
kind: KbsConfig
metadata:
  labels:
    app.kubernetes.io/name: kbsconfig
    app.kubernetes.io/instance: kbsconfig
    app.kubernetes.io/part-of: trustee-operator
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/created-by: trustee-operator
  name: kbsconfig
  namespace: trustee-operator-system
spec:
  kbsConfigMapName: kbs-config-cm
  kbsAuthSecretName: kbs-auth-public-key
  kbsDeploymentType: AllInOneDeployment
  kbsRvpsRefValuesConfigMapName: rvps-reference-values
  kbsSecretResources: ["kbsres1", "security-policy", "&lt;type&gt;"] <co xml:id="CO52-1"/>
  kbsResourcePolicyConfigMapName: resource-policy
  kbsAttestationPolicyConfigMapName: attestation-policy
  kbsServiceType: NodePort
  ibmSEConfigSpec:
    certStorePvc: ibmse-pvc</programlisting>
<calloutlist>
<callout arearefs="CO52-1">
<para>Specify the <literal>type</literal> value of the container image signature verification secret you created, for example, <literal>img-sig</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>KbsConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f kbsconfig-cr.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cc-verifing-trustee-config_ibm-cc">
<title>Verifying the Trustee configuration</title>
<simpara>You verify the Trustee configuration by checking the Trustee pods and logs.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Set the default project by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project trustee-operator-system</programlisting>
</listitem>
<listitem>
<simpara>Check the Trustee pods by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n trustee-operator-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">NAME                                                   READY   STATUS    RESTARTS   AGE
trustee-deployment-8585f98449-9bbgl                    1/1     Running   0          22m
trustee-operator-controller-manager-5fbd44cd97-55dlh   2/2     Running   0          59m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Set the <literal>POD_NAME</literal> environmental variable by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ POD_NAME=$(oc get pods -l app=kbs -o jsonpath='{.items[0].metadata.name}' -n trustee-operator-system)</programlisting>
</listitem>
<listitem>
<simpara>Check the pod logs by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n trustee-operator-system $POD_NAME</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">[2024-05-30T13:44:24Z INFO  kbs] Using config file /etc/kbs-config/kbs-config.json
[2024-05-30T13:44:24Z WARN  attestation_service::rvps] No RVPS address provided and will launch a built-in rvps
[2024-05-30T13:44:24Z INFO  attestation_service::token::simple] No Token Signer key in config file, create an ephemeral key and without CA pubkey cert
[2024-05-30T13:44:24Z INFO  api_server] Starting HTTPS server at [0.0.0.0:8080]
[2024-05-30T13:44:24Z INFO  actix_server::builder] starting 12 workers
[2024-05-30T13:44:24Z INFO  actix_server::server] Tokio runtime found; starting in existing Tokio runtime</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Expose the <literal>ibmse-pvc</literal> persistent volume claim to the Trustee pods by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch deployment trustee-deployment \
  --namespace=trustee-operator-system --type=json \
  -p='[{"op": "remove", "path": "/spec/template/spec/volumes/5/persistentVolumeClaim/readOnly"}]'</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>kbs-service</literal> is exposed on a node port by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get svc kbs-service -n trustee-operator-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">NAME          TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kbs-service   NodePort   198.51.100.54   &lt;none&gt;        8080:31862/TCP   23h</programlisting>
</para>
</formalpara>
<simpara>The <literal>kbs-service</literal> URL is <literal>https://&lt;worker_node_ip&gt;:&lt;node_port&gt;</literal>, for example, <literal>https://172.16.0.56:31862</literal>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="verifying-attestation-process_ibm-cc">
<title>Verifying the attestation process</title>
<simpara>You verify the attestation process by creating a test pod and retrieving its secret.
The pod image deploys the KBS client, a tool for testing the Key Broker Service and basic attestation flows.</simpara>
<important>
<simpara>This procedure is an example to verify that attestation is working. Do not write sensitive data to standard I/O because the data can be captured by using a memory dump. Only data written to memory is encrypted.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created a route if the Trustee server and the test pod are not running in the same cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>verification-pod.yaml</literal> manifest file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: kbs-client
spec:
  containers:
  - name: kbs-client
    image: quay.io/confidential-containers/kbs-client:latest
    imagePullPolicy: IfNotPresent
    command:
      - sleep
      - "360000"
    env:
      - name: RUST_LOG
        value:  none</programlisting>
</listitem>
<listitem>
<simpara>Create the pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f verification-pod.yaml</programlisting>
</listitem>
<listitem>
<simpara>Copy the <literal>https.crt</literal> file to the <literal>kbs-client</literal> pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc cp https.crt kbs-client:/</programlisting>
</listitem>
<listitem>
<simpara>Fetch the pod secret by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -it kbs-client -- kbs-client --cert-file https.crt \
  --url https://kbs-service:8080 get-resource \
  --path default/kbsres1/key1</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">res1val1</programlisting>
</para>
</formalpara>
<simpara>The Trustee server returns the secret only if the attestation is successful.</simpara>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="monitoring">
<title>Monitoring</title>
<simpara>You can use the OpenShift Container Platform web console to monitor metrics related to the health status of your sandboxed workloads and nodes.</simpara>
<simpara>OpenShift sandboxed containers has a pre-configured dashboard available in the OpenShift Container Platform web console. Administrators can also access and query raw metrics through Prometheus.</simpara>
<section xml:id="about-metrics_monitoring">
<title>About metrics</title>
<simpara>OpenShift sandboxed containers metrics enable administrators to monitor how their sandboxed containers are running. You can query for these metrics in Metrics UI In the OpenShift Container Platform web console.</simpara>
<simpara>OpenShift sandboxed containers metrics are collected for the following categories:</simpara>
<variablelist>
<varlistentry>
<term>Kata agent metrics</term>
<listitem>
<simpara>Kata agent metrics display information about the kata agent process running in the VM embedded in your sandboxed containers. These metrics include data from <literal>/proc/&lt;pid&gt;/[io, stat, status]</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Kata guest operating system metrics</term>
<listitem>
<simpara>Kata guest operating system metrics display data from the guest operating system running in your sandboxed containers. These metrics include data from <literal>/proc/[stats, diskstats, meminfo, vmstats]</literal> and <literal>/proc/net/dev</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Hypervisor metrics</term>
<listitem>
<simpara>Hypervisor metrics display data regarding the hypervisor running the VM embedded in your sandboxed containers. These metrics mainly include data from <literal>/proc/&lt;pid&gt;/[io, stat, status]</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Kata monitor metrics</term>
<listitem>
<simpara>Kata monitor is the process that gathers metric data and makes it available to Prometheus. The kata monitor metrics display detailed information about the resource usage of the kata-monitor process itself. These metrics also include counters from Prometheus data collection.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Kata containerd shim v2 metrics</term>
<listitem>
<simpara>Kata containerd shim v2 metrics display detailed information about the kata shim process. These metrics include data from <literal>/proc/&lt;pid&gt;/[io, stat, status]</literal> and detailed resource usage metrics.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="viewing-metrics_monitoring">
<title>Viewing metrics</title>
<simpara>You can access the metrics for OpenShift sandboxed containers in the <emphasis role="strong">Metrics</emphasis> page In the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role or with view permissions for all projects.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Observe</emphasis> → <emphasis role="strong">Metrics</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the input field, enter the query for the metric you want to observe.</simpara>
<simpara>All kata-related metrics begin with <emphasis role="strong">kata</emphasis>. Typing <emphasis role="strong">kata</emphasis> displays a list of all available kata metrics.</simpara>
</listitem>
</orderedlist>
<simpara>The metrics from your query are visualized on the page.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/monitoring/index#querying-metrics.html">Querying metrics</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/support/index#gathering-cluster-data.html">Gathering data about your cluster</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="uninstalling">
<title>Uninstalling</title>
<simpara>You can uninstall OpenShift sandboxed containers and remove the Confidential Containers environment.</simpara>
<section xml:id="uninstalling-ocs">
<title>Uninstalling OpenShift sandboxed containers</title>
<simpara>You can uninstall OpenShift sandboxed containers by using the OpenShift Container Platform web console or the command line.</simpara>
<simpara>You uninstall OpenShift sandboxed containers by performing the following tasks:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Delete the workload pods.</simpara>
</listitem>
<listitem>
<simpara>Delete the <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>Uninstall the OpenShift sandboxed containers Operator.</simpara>
</listitem>
<listitem>
<simpara>Delete the <literal>KataConfig</literal> custom resource definition.</simpara>
</listitem>
</orderedlist>
<section xml:id="uninstalling-osc-by-using-web-console">
<title>Uninstalling OpenShift sandboxed containers by using the web console</title>
<simpara>You can uninstall OpenShift sandboxed containers by using the OpenShift Container Platform web console.</simpara>
<section xml:id="deleting-workload-pods-web_uninstalling-osc-web">
<title>Deleting workload pods</title>
<simpara>You can delete the OpenShift sandboxed containers workload pods by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have a list of pods that use the OpenShift sandboxed containers runtime class.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Workloads</emphasis> → <emphasis role="strong">Pods</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the name of the pod that you want to delete in the <emphasis role="strong">Search by name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Click the pod name to open it.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Details</emphasis> page, check that <literal>kata</literal> or <literal>kata-remote</literal> is displayed for <emphasis role="strong">Runtime class</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Options</emphasis> menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Delete Pod</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Delete</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="deleting-cr-web_uninstalling-osc-web">
<title>Deleting the KataConfig custom resource</title>
<simpara>You can delete the <literal>KataConfig</literal> custom resource (CR) by using the web console.</simpara>
<simpara>Deleting the <literal>KataConfig</literal> CR removes and uninstalls the <literal>kata</literal> runtime and its related resources from your cluster.</simpara>
<important>
<simpara>Deleting the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have deleted all running pods that use <literal>kata</literal> as the <literal>runtimeClass</literal>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter <literal>OpenShift sandboxed containers Operator</literal> in the <emphasis role="strong">Search by name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Click the Operator to open it and then click the <emphasis role="strong">KataConfig</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Options</emphasis> menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Delete <literal>KataConfig</literal></emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Delete</emphasis> in the confirmation window.</simpara>
</listitem>
</orderedlist>
<simpara>Wait for the <literal>kata</literal> runtime and resources to uninstall and for the worker nodes to reboot before continuing to the next step.</simpara>
</section>
<section xml:id="uninstalling-operator-web_uninstalling-osc-web">
<title>Uninstalling the OpenShift sandboxed containers Operator</title>
<simpara>You can uninstall the OpenShift sandboxed containers Operator by using OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have deleted the <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter <literal>OpenShift sandboxed containers Operator</literal> in the <emphasis role="strong">Search by name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>On the right side of the <emphasis role="strong">Operator Details</emphasis> page, select <emphasis role="strong">Uninstall Operator</emphasis> from the <emphasis role="strong">Actions</emphasis> list.</simpara>
<simpara>An <emphasis role="strong">Uninstall Operator?</emphasis> dialog box is displayed.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Uninstall</emphasis> to remove the Operator, Operator deployments, and pods.</simpara>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Administration</emphasis> → <emphasis role="strong">Namespaces</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter <literal>openshift-sandboxed-containers-operator</literal> in the <emphasis role="strong">Search by name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Options</emphasis> menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Delete Namespace</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the confirmation dialog, enter <literal>openshift-sandboxed-containers-operator</literal> and click <emphasis role="strong">Delete</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="deleting-crd-web_uninstalling-osc-web">
<title>Deleting the KataConfig CRD</title>
<simpara>You can delete the <literal>KataConfig</literal> custom resource definition (CRD) by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have deleted the <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>You have uninstalled the OpenShift sandboxed containers Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the web console, navigate to <emphasis role="strong">Administration</emphasis> → <emphasis role="strong">CustomResourceDefinitions</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the <literal>KataConfig</literal> name in the <emphasis role="strong">Search by name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Options</emphasis> menu and select <emphasis role="strong">Delete CustomResourceDefinition</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Delete</emphasis> in the confirmation window.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="uninstalling-osc-by-using-cli">
<title>Uninstalling OpenShift sandboxed containers by using the CLI</title>
<simpara>You can uninstall OpenShift sandboxed containers by using the command-line interface (CLI).</simpara>
<section xml:id="deleting-workload-pods-cli_uninstalling-osc-cli">
<title>Deleting workload pods</title>
<simpara>You can delete the OpenShift sandboxed containers workload pods by using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have the JSON processor (<literal>jq</literal>) utility installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Search for the pods by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -A -o json | jq -r '.items[] | \
  select(.spec.runtimeClassName == "&lt;runtime&gt;").metadata.name' <co xml:id="CO53-1"/></programlisting>
<calloutlist>
<callout arearefs="CO53-1">
<para>Specify <literal>kata</literal> for bare metal deployments. Specify <literal>kata-remote</literal> for AWS, Azure, IBM Z®, and IBM® LinuxONE.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Delete each pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete pod &lt;pod&gt;</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="deleting-cr-cli_uninstalling-osc-cli">
<title>Deleting the KataConfig custom resource</title>
<simpara>You can delete the <literal>KataConfig</literal> custom resource (CR) by using the command line.</simpara>
<simpara>Deleting the <literal>KataConfig</literal> CR removes the runtime and its related resources from your cluster.</simpara>
<important>
<simpara>Deleting the <literal>KataConfig</literal> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>A larger OpenShift Container Platform deployment with a greater number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Activation of the BIOS and Diagnostics utility.</simpara>
</listitem>
<listitem>
<simpara>Deployment on a hard drive rather than an SSD.</simpara>
</listitem>
<listitem>
<simpara>Deployment on physical nodes such as bare metal, rather than on virtual nodes.</simpara>
</listitem>
<listitem>
<simpara>A slow CPU and network.</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>KataConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete kataconfig example-kataconfig</programlisting>
<simpara>The OpenShift sandboxed containers Operator removes all resources that were initially created to enable the runtime on your cluster.</simpara>
<important>
<simpara>When you delete the <literal>KataConfig</literal> CR, the CLI stops responding until all worker nodes reboot. You must for the deletion process to complete before performing the verification.</simpara>
</important>
</listitem>
<listitem>
<simpara>Verify that the custom resource was deleted by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kataconfig example-kataconfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">No example-kataconfig instances exist</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="uninstalling-operator-cli_uninstalling-osc-cli">
<title>Uninstalling the OpenShift sandboxed containers Operator</title>
<simpara>You can uninstall the OpenShift sandboxed containers Operator by using the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have deleted the OpenShift sandboxed containers workload pods.</simpara>
</listitem>
<listitem>
<simpara>You have deleted <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the subscription by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete subscription sandboxed-containers-operator -n openshift-sandboxed-containers-operator</programlisting>
</listitem>
<listitem>
<simpara>Delete the namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete namespace openshift-sandboxed-containers-operator</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="deleting-crd-cli_uninstalling-osc-cli">
<title>Deleting the KataConfig CRD</title>
<simpara>You can delete the <literal>KataConfig</literal> custom resource definition (CRD) by using the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have deleted the <literal>KataConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>You have uninstalled the OpenShift sandboxed containers Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>KataConfig</literal> CRD by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete crd kataconfigs.kataconfiguration.openshift.io</programlisting>
</listitem>
<listitem>
<simpara>Verify that the CRD was deleted by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get crd kataconfigs.kataconfiguration.openshift.io</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Unknown CRD kataconfigs.kataconfiguration.openshift.io</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="removing-cc-environment">
<title>Removing the Confidential Containers environment</title>
<simpara>You can remove the Confidential Containers environment by using the OpenShift Container Platform web console or the command line.</simpara>
<simpara>You remove the Confidential Containers environment by performing the following tasks:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Delete the <literal>KbsConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>Uninstall the Confidential compute attestation Operator.</simpara>
</listitem>
<listitem>
<simpara>Delete the <literal>KbsConfig</literal> custom resource definition.</simpara>
</listitem>
</orderedlist>
<section xml:id="removing-cc-environment-web">
<title>Removing the Confidential Containers environment by using the web console</title>
<simpara>You can remove the Confidential Containers environment by using the OpenShift Container Platform web console.</simpara>
<section xml:id="deleting-cr-web_removing-cc-web">
<title>Deleting the KbsConfig custom resource</title>
<simpara>You can delete the <literal>KbsConfig</literal> custom resource (CR) by using the web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have uninstalled OpenShift sandboxed containers.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter <literal>Confidential compute attestation</literal> in the <emphasis role="strong">Search by name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Click the Operator to open it and then click the <emphasis role="strong">KbsConfig</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Options</emphasis> menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Delete <literal>KbsConfig</literal></emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Delete</emphasis> in the confirmation window.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="uninstalling-operator-web_removing-cc-web">
<title>Uninstalling the Confidential compute attestation Operator</title>
<simpara>You can uninstall the Confidential compute attestation Operator by using OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have deleted the <literal>KbsConfig</literal> custom resource.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter <literal>Confidential compute attestation</literal> in the <emphasis role="strong">Search by name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>On the right side of the <emphasis role="strong">Operator Details</emphasis> page, select <emphasis role="strong">Uninstall Operator</emphasis> from the <emphasis role="strong">Actions</emphasis> list.</simpara>
<simpara>An <emphasis role="strong">Uninstall Operator?</emphasis> dialog box is displayed.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Uninstall</emphasis> to remove the Operator, Operator deployments, and pods.</simpara>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Administration</emphasis> → <emphasis role="strong">Namespaces</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter <literal>trustee-operator-system</literal> in the <emphasis role="strong">Search by name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Options</emphasis> menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Delete Namespace</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the confirmation dialog, enter <literal>trustee-operator-system</literal> and click <emphasis role="strong">Delete</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="deleting-crd-web_removing-cc-web">
<title>Deleting the KbsConfig CRD</title>
<simpara>You can delete the <literal>KbsConfig</literal> custom resource definition (CRD) by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have deleted the <literal>KbsConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>You have uninstalled the Confidential compute attestation Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the web console, navigate to <emphasis role="strong">Administration</emphasis> → <emphasis role="strong">CustomResourceDefinitions</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the <literal>KbsConfig</literal> name in the <emphasis role="strong">Search by name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Options</emphasis> menu and select <emphasis role="strong">Delete CustomResourceDefinition</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Delete</emphasis> in the confirmation window.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="removing-cc-environment-cli">
<title>Removing the Confidential Containers environment by using the CLI</title>
<simpara>You can remove the Confidential Containers environment by using the command-line interface (CLI).</simpara>
<section xml:id="deleting-cr-cli_removing-cc-cli">
<title>Deleting the KbsConfig custom resource</title>
<simpara>You can delete the <literal>KbsConfig</literal> custom resource (CR) by using the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have uninstalled OpenShift sandboxed containers.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>KbsConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete kbsconfig kbsconfig</programlisting>
</listitem>
<listitem>
<simpara>Verify that the custom resource was deleted by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kbsconfig kbsconfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">No kbsconfig instances exist</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="uninstalling-operator-cli_removing-cc-cli">
<title>Uninstalling the Confidential compute attestation Operator</title>
<simpara>You can uninstall the Confidential compute attestation Operator by using the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have deleted the <literal>KbsConfig</literal> custom resource.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the subscription by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete subscription trustee-operator -n trustee-operator-system</programlisting>
</listitem>
<listitem>
<simpara>Delete the namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete namespace trustee-operator-system</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="deleting-crd-cli_removing-cc-cli">
<title>Deleting the KbsConfig CRD</title>
<simpara>You can delete the <literal>KbsConfig</literal> custom resource definition (CRD) by using the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have deleted the <literal>KbsConfig</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>You have uninstalled the Confidential compute attestation Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>KbsConfig</literal> CRD by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete crd kbsconfigs.confidentialcontainers.org</programlisting>
</listitem>
<listitem>
<simpara>Verify that the CRD was deleted by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get crd kbsconfigs.confidentialcontainers.org</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Unknown CRD kbsconfigs.confidentialcontainers.org</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
</chapter>
<chapter xml:id="upgrading">
<title>Upgrading</title>
<simpara>The upgrade of the OpenShift sandboxed containers components consists of the following three steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Upgrade OpenShift Container Platform to update the <literal>Kata</literal> runtime and its dependencies.</simpara>
</listitem>
<listitem>
<simpara>Upgrade the OpenShift sandboxed containers Operator to update the Operator subscription.</simpara>
</listitem>
</orderedlist>
<simpara>You can upgrade OpenShift Container Platform before or after the OpenShift sandboxed containers Operator upgrade, with the one exception noted below. Always apply the <literal>KataConfig</literal> patch immediately after upgrading OpenShift sandboxed containers Operator.</simpara>
<section xml:id="upgrading-resources">
<title>Upgrading resources</title>
<simpara>The OpenShift sandboxed containers resources are deployed onto the cluster using Red Hat Enterprise Linux CoreOS (RHCOS) extensions.</simpara>
<simpara>The RHCOS extension <literal>sandboxed containers</literal> contains the required components to run OpenShift sandboxed containers, such as the Kata containers runtime, the hypervisor QEMU, and other dependencies. You upgrade the extension by upgrading the cluster to a new release of OpenShift Container Platform.</simpara>
<simpara>For more information about upgrading OpenShift Container Platform, see <link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/updating/index">Updating Clusters</link>.</simpara>
</section>
<section xml:id="upgrading-operator">
<title>Upgrading the Operator</title>
<simpara>Use Operator Lifecycle Manager (OLM) to upgrade the OpenShift sandboxed containers Operator either manually or automatically. Selecting between manual or automatic upgrade during the initial deployment determines the future upgrade mode. For manual upgrades, the OpenShift Container Platform web console shows the available updates that can be installed by the cluster administrator.</simpara>
<simpara>For more information about upgrading the OpenShift sandboxed containers Operator in Operator Lifecycle Manager (OLM), see <link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-upgrading-operators">Updating installed Operators</link>.</simpara>
</section>
</chapter>
<chapter xml:id="troubleshooting">
<title>Troubleshooting</title>
<simpara>When troubleshooting OpenShift sandboxed containers, you can open a support case and provide debugging information using the <literal>must-gather</literal> tool.</simpara>
<simpara>If you are a cluster administrator, you can also review logs on your own, enabling a more detailed level of logs.</simpara>
<section xml:id="collect_data_rh_support">
<title>Collecting data for Red Hat Support</title>
<simpara>When opening a support case, it is helpful to provide debugging information about your cluster to Red Hat Support.</simpara>
<simpara>The <literal>must-gather</literal> tool enables you to collect diagnostic information about your OpenShift Container Platform cluster, including virtual machines and other data related to OpenShift sandboxed containers.</simpara>
<simpara>For prompt support, supply diagnostic information for both OpenShift Container Platform and OpenShift sandboxed containers.</simpara>
<bridgehead xml:id="using-must-gather_troubleshooting" renderas="sect3">Using the must-gather tool</bridgehead>
<simpara>The <literal>oc adm must-gather</literal> CLI command collects the information from your cluster that is most likely needed for debugging issues, including:</simpara>
<itemizedlist>
<listitem>
<simpara>Resource definitions</simpara>
</listitem>
<listitem>
<simpara>Service logs</simpara>
</listitem>
</itemizedlist>
<simpara>By default, the <literal>oc adm must-gather</literal> command uses the default plugin image and writes into <literal>./must-gather.local</literal>.</simpara>
<simpara>Alternatively, you can collect specific information by running the command with the appropriate arguments as described in the following sections:</simpara>
<itemizedlist>
<listitem>
<simpara>To collect data related to one or more specific features, use the <literal>--image</literal> argument with an image, as listed in a following section.</simpara>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather --image=registry.redhat.io/openshift-sandboxed-containers/osc-must-gather-rhel9:1.8.0</programlisting>
</listitem>
<listitem>
<simpara>To collect the audit logs, use the <literal>-- /usr/bin/gather_audit_logs</literal> argument, as described in a following section.</simpara>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather -- /usr/bin/gather_audit_logs</programlisting>
<note>
<simpara>Audit logs are not collected as part of the default set of information to reduce the size of the files.</simpara>
</note>
</listitem>
</itemizedlist>
<simpara>When you run <literal>oc adm must-gather</literal>, a new pod with a random name is created in a new project on the cluster. The data is collected on that pod and saved in a new directory that starts with <literal>must-gather.local</literal>. This directory is created in the current working directory.</simpara>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAMESPACE                      NAME                 READY   STATUS      RESTARTS      AGE
...
openshift-must-gather-5drcj    must-gather-bklx4    2/2     Running     0             72s
openshift-must-gather-5drcj    must-gather-s8sdh    2/2     Running     0             72s
...</programlisting>
<simpara>Optionally, you can run the <literal>oc adm must-gather</literal> command in a specific namespace by using the <literal>--run-namespace</literal> option.</simpara>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather --run-namespace &lt;namespace&gt; --image=registry.redhat.io/openshift-sandboxed-containers/osc-must-gather-rhel9:1.8.0</programlisting>
</section>
<section xml:id="collecting-log-data">
<title>Collecting log data</title>
<simpara>The following features and objects are associated with OpenShift sandboxed containers:</simpara>
<itemizedlist>
<listitem>
<simpara>All namespaces and their child objects that belong to OpenShift sandboxed containers resources</simpara>
</listitem>
<listitem>
<simpara>All OpenShift sandboxed containers custom resource definitions (CRDs)</simpara>
</listitem>
</itemizedlist>
<simpara>You can collect the following component logs for each pod running with the <literal>kata</literal> runtime:</simpara>
<itemizedlist>
<listitem>
<simpara>Kata agent logs</simpara>
</listitem>
<listitem>
<simpara>Kata runtime logs</simpara>
</listitem>
<listitem>
<simpara>QEMU logs</simpara>
</listitem>
<listitem>
<simpara>Audit logs</simpara>
</listitem>
<listitem>
<simpara>CRI-O logs</simpara>
</listitem>
</itemizedlist>
<section xml:id="enabling-debug-logs-crio_troubleshooting">
<title>Enabling debug logs for CRI-O runtime</title>
<simpara>You can enable debug logs by updating the <literal>logLevel</literal> field in the <literal>KataConfig</literal> CR. This changes the log level in the CRI-O runtime for the worker nodes running OpenShift sandboxed containers.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Change the <literal>logLevel</literal> field in your existing <literal>KataConfig</literal> CR to <literal>debug</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch kataconfig &lt;kataconfig&gt; --type merge --patch '{"spec":{"logLevel":"debug"}}'</programlisting>
</listitem>
<listitem>
<simpara>Monitor the <literal>kata-oc</literal> machine config pool until the value of <literal>UPDATED</literal> is <literal>True</literal>, indicating that all worker nodes are updated:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp kata-oc</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME     CONFIG                 UPDATED  UPDATING  DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT  DEGRADEDMACHINECOUNT  AGE
kata-oc  rendered-kata-oc-169   False    True      False     3             1                  1                    0                     9h</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Start a debug session with a node in the machine config pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Change the root directory to <literal>/host</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Verify the changes in the <literal>crio.conf</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># crio config | egrep 'log_level</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">log_level = "debug"</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="viewing-debug-logs-components_troubleshooting">
<title>Viewing debug logs for components</title>
<simpara>Cluster administrators can use the debug logs to troubleshoot issues. The logs for each node are printed to the node journal.</simpara>
<simpara>You can review the logs for the following OpenShift sandboxed containers components:</simpara>
<itemizedlist>
<listitem>
<simpara>Kata agent</simpara>
</listitem>
<listitem>
<simpara>Kata runtime (<literal>containerd-shim-kata-v2</literal>)</simpara>
</listitem>
<listitem>
<simpara><literal>virtiofsd</literal></simpara>
</listitem>
</itemizedlist>
<simpara>QEMU only generates warning and error logs. These warnings and errors print to the node journal in both the Kata runtime logs and the CRI-O logs with an extra <literal>qemuPid</literal> field.</simpara>
<formalpara>
<title>Example of QEMU logs</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Mar 11 11:57:28 openshift-worker-0 kata[2241647]: time="2023-03-11T11:57:28.587116986Z" level=info msg="Start logging QEMU (qemuPid=2241693)" name=containerd-shim-v2 pid=2241647 sandbox=d1d4d68efc35e5ccb4331af73da459c13f46269b512774aa6bde7da34db48987 source=virtcontainers/hypervisor subsystem=qemu

Mar 11 11:57:28 openshift-worker-0 kata[2241647]: time="2023-03-11T11:57:28.607339014Z" level=error msg="qemu-kvm: -machine q35,accel=kvm,kernel_irqchip=split,foo: Expected '=' after parameter 'foo'" name=containerd-shim-v2 pid=2241647 qemuPid=2241693 sandbox=d1d4d68efc35e5ccb4331af73da459c13f46269b512774aa6bde7da34db48987 source=virtcontainers/hypervisor subsystem=qemu

Mar 11 11:57:28 openshift-worker-0 kata[2241647]: time="2023-03-11T11:57:28.60890737Z" level=info msg="Stop logging QEMU (qemuPid=2241693)" name=containerd-shim-v2 pid=2241647 sandbox=d1d4d68efc35e5ccb4331af73da459c13f46269b512774aa6bde7da34db48987 source=virtcontainers/hypervisor subsystem=qemu</programlisting>
</para>
</formalpara>
<simpara>The Kata runtime prints <literal>Start logging QEMU</literal> when QEMU starts, and <literal>Stop Logging QEMU</literal> when QEMU stops. The error appears in between these two log messages with the <literal>qemuPid</literal> field. The actual error message from QEMU appears in red.</simpara>
<simpara>The console of the QEMU guest is printed to the node journal as well. You can view the guest console logs together with the Kata agent logs.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To review the Kata agent logs and guest console logs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;nodename&gt; -- journalctl -D /host/var/log/journal -t kata -g “reading guest console”</programlisting>
</listitem>
<listitem>
<simpara>To review the Kata runtime logs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;nodename&gt; -- journalctl -D /host/var/log/journal -t kata</programlisting>
</listitem>
<listitem>
<simpara>To review the <literal>virtiofsd</literal> logs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;nodename&gt; -- journalctl -D /host/var/log/journal -t virtiofsd</programlisting>
</listitem>
<listitem>
<simpara>To review the QEMU logs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;nodename&gt; -- journalctl -D /host/var/log/journal -t kata -g "qemuPid=\d+"</programlisting>
</listitem>
</itemizedlist>
<bridgehead xml:id="additional_resources_4" role="_additional-resources" renderas="sect2" remap="_additional_resources_4">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/support/index#support_gathering_data_gathering-cluster-data">Gathering data about your cluster</link> in the OpenShift Container Platform documentation</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<appendix xml:id="kataconfig-status-messages">
<title>KataConfig status messages</title>
<simpara>The following table displays the status messages for the <literal>KataConfig</literal> custom resource (CR) for a cluster with two worker nodes.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>KataConfig</literal> status messages</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Status</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Initial installation</emphasis></simpara><simpara>When a <literal>KataConfig</literal> CR is created and starts installing <literal>kata-remote</literal> on both workers, the following status is displayed for a few seconds.</simpara></entry>
<entry align="left" valign="top"><programlisting language="yaml" linenumbering="unnumbered"> conditions:
    message: Performing initial installation of kata-remote on cluster
    reason: Installing
    status: 'True'
    type: InProgress
 kataNodes:
   nodeCount: 0
   readyNodeCount: 0</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Installing</emphasis></simpara><simpara>Within a few seconds the status changes.</simpara></entry>
<entry align="left" valign="top"><programlisting language="yaml" linenumbering="unnumbered"> kataNodes:
   nodeCount: 2
   readyNodeCount: 0
   waitingToInstall:
   - worker-0
   - worker-1</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Installing</emphasis> (Worker-1 installation starting)</simpara><simpara>For a short period of time, the status changes, signifying that one node has initiated the installation of <literal>kata-remote</literal>, while the other is in a waiting state. This is because only one node can be unavailable at any given time. The <literal>nodeCount</literal> remains at 2 because both nodes will eventually receive <literal>kata-remote</literal>, but the <literal>readyNodeCount</literal> is currently 0 as neither of them has reached that state yet.</simpara></entry>
<entry align="left" valign="top"><programlisting language="yaml" linenumbering="unnumbered"> kataNodes:
   installing:
   - worker-1
   nodeCount: 2
   readyNodeCount: 0
   waitingToInstall:
   - worker-0</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Installing</emphasis> (Worker-1 installed, worker-0 installation started)</simpara><simpara>After some time, <literal>worker-1</literal> will complete its installation, causing a change in the status. The <literal>readyNodeCount</literal> is updated to 1, indicating that <literal>worker-1</literal> is now prepared to execute <literal>kata-remote</literal> workloads.
You cannot schedule or run <literal>kata-remote</literal> workloads until the runtime class is created at the end of the installation process.</simpara></entry>
<entry align="left" valign="top"><programlisting language="yaml" linenumbering="unnumbered"> kataNodes:
   installed:
   - worker-1
   installing:
   - worker-0
   nodeCount: 2
   readyNodeCount: 1</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Installed</emphasis></simpara><simpara>When installed, both workers are listed as installed, and the <literal>InProgress</literal> condition transitions to <literal>False</literal> without specifying a reason, indicating the successful installation of <literal>kata-remote</literal> on the cluster.</simpara></entry>
<entry align="left" valign="top"><programlisting language="yaml" linenumbering="unnumbered"> conditions:
    message: ""
    reason: ""
    status: 'False'
    type: InProgress
 kataNodes:
   installed:
   - worker-0
   - worker-1
   nodeCount: 2
   readyNodeCount: 2</programlisting></entry>
</row>
</tbody>
</tgroup>
</table>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Status</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Initial uninstall</emphasis></simpara><simpara>If <literal>kata-remote</literal> is installed on both workers, and you delete the <literal>KataConfig</literal> to remove <literal>kata-remote</literal> from the cluster, both workers briefly enter a waiting state for a few seconds.</simpara></entry>
<entry align="left" valign="top"><programlisting language="yaml" linenumbering="unnumbered"> conditions:
    message: Removing kata-remote from cluster
    reason: Uninstalling
    status: 'True'
    type: InProgress
 kataNodes:
   nodeCount: 0
   readyNodeCount: 0
   waitingToUninstall:
   - worker-0
   - worker-1</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Uninstalling</emphasis></simpara><simpara>After a few seconds, one of the workers starts uninstalling.</simpara></entry>
<entry align="left" valign="top"><programlisting language="yaml" linenumbering="unnumbered"> kataNodes:
   nodeCount: 0
   readyNodeCount: 0
   uninstalling:
   - worker-1
   waitingToUninstall:
   - worker-0</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Uninstalling</emphasis></simpara><simpara>Worker-1 finishes and worker-0 starts uninstalling.</simpara></entry>
<entry align="left" valign="top"><programlisting language="yaml" linenumbering="unnumbered"> kataNodes:
   nodeCount: 0
   readyNodeCount: 0
   uninstalling:
   - worker-0</programlisting></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<note>
<simpara>The <literal>reason</literal> field can also report the following causes:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Failed</literal>: This is reported if the node cannot finish its transition. The <literal>status</literal> reports <literal>True</literal> and the <literal>message</literal> is <literal>Node &lt;node_name&gt; Degraded: &lt;error_message_from_the_node&gt;</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>BlockedByExistingKataPods</literal>: This is reported if there are pods running on a cluster that use the <literal>kata-remote</literal> runtime while <literal>kata-remote</literal> is being uninstalled.  The <literal>status</literal> field is <literal>False</literal> and the <literal>message</literal> is <literal>Existing pods using "kata-remote" RuntimeClass found. Please delete the pods manually for KataConfig deletion to proceed</literal>. There could also be a technical error message reported like <literal>Failed to list kata pods: &lt;error_message&gt;</literal> if communication with the cluster control plane fails.</simpara>
</listitem>
</itemizedlist>
</note>
</appendix>
</book>