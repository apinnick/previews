<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" class="chrometwo"><head><title>User guide</title><link rel="stylesheet" type="text/css" href="Common_Content/css/default.css"/><meta name="generator" content="publican v4.3.2"/><meta name="description" content="Deploying OpenShift sandboxed containers in OpenShift Container Platform on bare metal, public cloud, and IBM platforms."/><link rel="next" href="#idm46763391618752" title="Preface"/><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><script type="text/javascript" src="Common_Content/scripts/jquery-1.7.1.min.js"> </script><script type="text/javascript" src="Common_Content/scripts/utils.js"> </script><script type="text/javascript" src="Common_Content/scripts/highlight.js/highlight.pack.js"> </script></head><body><div id="chrometwo"><div id="main"><div xml:lang="en-US" class="book" id="idm46763389393392"><div class="titlepage"><div><div class="producttitle"><span class="productname">OpenShift sandboxed containers</span> <span class="productnumber">1.8</span></div><div><h1 class="title">User guide</h1></div><div><h2 class="subtitle">Deploying sandboxed containers in OpenShift Container Platform</h2></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat Customer Content Services</span></div></div><div><a href="#idm46763383936928">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				Deploying OpenShift sandboxed containers in OpenShift Container Platform on bare metal, public cloud, and IBM platforms.
			</div></div></div></div><hr/></div><div class="toc"><ul class="toc"><li><span class="preface"><a href="#idm46763391618752">Preface</a></span></li><li><span class="chapter"><a href="#about-osc">1. About OpenShift sandboxed containers</a></span><ul><li><span class="section"><a href="#osc-features_about-osc">1.1. Features</a></span></li><li><span class="section"><a href="#compatiblity-platforms_about-osc">1.2. Compatibility with OpenShift Container Platform</a></span></li><li><span class="section"><a href="#about-node-eligibility-checks_about-osc">1.3. Node eligibility checks</a></span></li><li><span class="section"><a href="#common-terms_about-osc">1.4. Common terms</a></span></li><li><span class="section"><a href="#osc-operator_about-osc">1.5. OpenShift sandboxed containers Operator</a></span></li><li><span class="section"><a href="#about-confidential-containers_about-osc">1.6. About Confidential Containers</a></span></li><li><span class="section"><a href="#ocp-virt-and-osc_about-osc">1.7. OpenShift Virtualization</a></span></li><li><span class="section"><a href="#block_volume_support_about-osc">1.8. Block volume support</a></span></li><li><span class="section"><a href="#fips-compliance_about-osc">1.9. FIPS compliance</a></span></li></ul></li><li><span class="chapter"><a href="#deploying-osc-bare-metal">2. Deploying OpenShift sandboxed containers on bare metal</a></span><ul><li><span class="section"><a href="#osc-resource-requirements_deploying-bare-metal">2.1. OpenShift sandboxed containers resource requirements</a></span></li><li><span class="section"><a href="#deploying-osc-web_metal-web">2.2. Deploying OpenShift sandboxed containers by using the web console</a></span><ul><li><span class="section"><a href="#installing-operator-web-console_metal-web">2.2.1. Installing the OpenShift sandboxed containers Operator</a></span></li><li><span class="section"><a href="#creating-kataconfig-cr-web_metal-web">2.2.2. Creating the KataConfig custom resource</a></span></li><li><span class="section"><a href="#configuring-workload-objects_metal-web">2.2.3. Configuring workload objects</a></span></li></ul></li><li><span class="section"><a href="#deploying-osc-cli_metal-cli">2.3. Deploying OpenShift sandboxed containers by using the command line</a></span><ul><li><span class="section"><a href="#installing-operator-cli_metal-cli">2.3.1. Installing the OpenShift sandboxed containers Operator</a></span></li><li><span class="section"><a href="#optional_configurations">2.3.2. Optional configurations</a></span></li><li><span class="section"><a href="#creating-kataconfig-cr-cli_metal-cli">2.3.3. Creating the KataConfig custom resource</a></span></li><li><span class="section"><a href="#modifying-pod-overhead.adoc_metal-cli">2.3.4. Modifying pod overhead</a></span></li><li><span class="section"><a href="#configuring-workload-objects_metal-cli">2.3.5. Configuring workload objects</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#deploying-aws">3. Deploying OpenShift sandboxed containers on AWS</a></span><ul><li><span class="section"><a href="#peer-pod-resource-requirements_aws">3.1. Peer pod resource requirements</a></span></li><li><span class="section"><a href="#deploying-osc-web_aws-web">3.2. Deploying OpenShift sandboxed containers by using the web console</a></span><ul><li><span class="section"><a href="#installing-operator-web-console_aws-web">3.2.1. Installing the OpenShift sandboxed containers Operator</a></span></li><li><span class="section"><a href="#aws-enabling-ports_aws-web">3.2.2. Enabling ports for AWS</a></span></li><li><span class="section"><a href="#creating-peer-pods-secret_aws-web">3.2.3. Creating the peer pods secret</a></span></li><li><span class="section"><a href="#creating-peer-pods-config-map_aws-web">3.2.4. Creating the peer pods config map</a></span></li><li><span class="section"><a href="#creating-kataconfig-cr-web_aws-web">3.2.5. Creating the KataConfig custom resource</a></span></li><li><span class="section"><a href="#configuring-workload-objects_aws-web">3.2.6. Configuring workload objects</a></span></li></ul></li><li><span class="section"><a href="#deploying-osc-cli_aws-cli">3.3. Deploying OpenShift sandboxed containers by using the command line</a></span><ul><li><span class="section"><a href="#installing-operator-cli_aws-cli">3.3.1. Installing the OpenShift sandboxed containers Operator</a></span></li><li><span class="section"><a href="#modifying-peer-pod-vm-limit_aws-cli">3.3.2. Modifying the number of peer pod VMs per node</a></span></li><li><span class="section"><a href="#aws-enabling-ports_aws-cli">3.3.3. Enabling ports for AWS</a></span></li><li><span class="section"><a href="#creating-peer-pods-secret_aws-cli">3.3.4. Creating the peer pods secret</a></span></li><li><span class="section"><a href="#creating-peer-pods-config-map_aws-cli">3.3.5. Creating the peer pods config map</a></span></li><li><span class="section"><a href="#creating-kataconfig-cr-cli_aws-cli">3.3.6. Creating the KataConfig custom resource</a></span></li><li><span class="section"><a href="#configuring-workload-objects_aws-cli">3.3.7. Configuring workload objects</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#deploying-osc-on-azure">4. Deploying OpenShift sandboxed containers on Azure</a></span><ul><li><span class="section"><a href="#peer-pod-resource-requirements_azure">4.1. Peer pod resource requirements</a></span></li><li><span class="section"><a href="#deploying-osc-web_azure-web">4.2. Deploying OpenShift sandboxed containers by using the web console</a></span><ul><li><span class="section"><a href="#installing-operator-web-console_azure-web">4.2.1. Installing the OpenShift sandboxed containers Operator</a></span></li><li><span class="section"><a href="#creating-peer-pods-secret_azure-web">4.2.2. Creating the peer pods secret</a></span></li><li><span class="section"><a href="#creating-peer-pods-config-map_azure-web">4.2.3. Creating the peer pods config map</a></span></li><li><span class="section"><a href="#creating-ssh-key-secret_azure-web">4.2.4. Creating the Azure secret</a></span></li><li><span class="section"><a href="#creating-kataconfig-cr-web_azure-web">4.2.5. Creating the KataConfig custom resource</a></span></li><li><span class="section"><a href="#configuring-workload-objects_azure-web">4.2.6. Configuring workload objects</a></span></li></ul></li><li><span class="section"><a href="#deploying-osc-cli_azure-cli">4.3. Deploying OpenShift sandboxed containers by using the command line</a></span><ul><li><span class="section"><a href="#installing-operator-cli_azure-cli">4.3.1. Installing the OpenShift sandboxed containers Operator</a></span></li><li><span class="section"><a href="#modifying-peer-pod-vm-limit_azure-cli">4.3.2. Modifying the number of peer pod VMs per node</a></span></li><li><span class="section"><a href="#creating-peer-pods-secret_azure-cli">4.3.3. Creating the peer pods secret</a></span></li><li><span class="section"><a href="#creating-peer-pods-config-map_azure-cli">4.3.4. Creating the peer pods config map</a></span></li><li><span class="section"><a href="#creating-ssh-key-secret_azure-cli">4.3.5. Creating the Azure secret</a></span></li><li><span class="section"><a href="#creating-kataconfig-cr-cli_azure-cli">4.3.6. Creating the KataConfig custom resource</a></span></li><li><span class="section"><a href="#configuring-workload-objects_azure-cli">4.3.7. Configuring workload objects</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#deploying-cc_azure-cc">5. Deploying Confidential Containers on Azure</a></span><ul><li><span class="section"><a href="#cc-installing-cc-operator-cli_azure-cc">5.1. Installing the Confidential compute attestation Operator</a></span></li><li><span class="section"><a href="#cc-enabling-feature-gate_azure-cc">5.2. Enabling the Confidential Containers feature gate</a></span></li><li><span class="section"><a href="#cc-creating-route_azure-cc">5.3. Creating the route for Trustee</a></span></li><li><span class="section"><a href="#creating-peer-pods-config-map_azure-cc">5.4. Updating the peer pods config map</a></span></li><li><span class="section"><a href="#deleting-cr-cli_azure-cc">5.5. Deleting the KataConfig custom resource</a></span></li><li><span class="section"><a href="#creating-kataconfig-cr-cli_azure-cc">5.6. Re-creating the KataConfig custom resource</a></span></li><li><span class="section"><a href="#cc-creating-trustee-auth-secret_azure-cc">5.7. Creating the Trustee authentication secret</a></span></li><li><span class="section"><a href="#cc-creating-trustee-config-map_azure-cc">5.8. Creating the Trustee config map</a></span></li><li><span class="section"><a href="#configuring-trustee_azure-cc">5.9. Configuring Trustee values, policies, and secrets</a></span><ul><li><span class="section"><a href="#cc-configuring-reference-values_azure-cc">5.9.1. Configuring reference values</a></span></li><li><span class="section"><a href="#cc-creating-attestation-policy_azure-cc">5.9.2. Creating an attestation policy</a></span></li><li><span class="section"><a href="#cc-configuring-pccs-for-tdx_azure-cc">5.9.3. Configuring PCCS for TDX</a></span></li><li><span class="section"><a href="#cc-creating-secret-for-clients_azure-cc">5.9.4. Creating a secret with custom keys for clients</a></span></li><li><span class="section"><a href="#cc-creating-secret-signed-container-images_azure-cc">5.9.5. Creating a secret for container image signature verification</a></span></li><li><span class="section"><a href="#cc-creating-container-image-sig-policy_azure-cc">5.9.6. Creating the container image signature verification policy</a></span></li><li><span class="section"><a href="#cc-creating-resource-access-policy_azure-cc">5.9.7. Creating the resource access policy</a></span></li></ul></li><li><span class="section"><a href="#cc-creating-kbsconfig-cr_azure-cc">5.10. Creating the KbsConfig custom resource</a></span></li><li><span class="section"><a href="#cc-verifing-trustee-config_azure-cc">5.11. Verifying the Trustee configuration</a></span></li><li><span class="section"><a href="#verifying-attestation-process_azure-cc">5.12. Verifying the attestation process</a></span></li></ul></li><li><span class="chapter"><a href="#deploying-osc-ibm">6. Deploying OpenShift sandboxed containers on IBM Z and IBM LinuxONE</a></span><ul><li><span class="section"><a href="#peer-pod-resource-requirements_ibm-osc">6.1. Peer pod resource requirements</a></span></li><li><span class="section"><a href="#deploying-osc-cli_ibm-osc">6.2. Deploying OpenShift sandboxed containers on IBM Z and IBM LinuxONE</a></span><ul><li><span class="section"><a href="#installing-operator-cli_ibm-osc">6.2.1. Installing the OpenShift sandboxed containers Operator</a></span></li><li><span class="section"><a href="#modifying-peer-pod-vm-limit_ibm-osc">6.2.2. Modifying the number of peer pod VMs per node</a></span></li><li><span class="section"><a href="#ibm-configuring-libvirt_ibm-osc">6.2.3. Configuring the libvirt volume</a></span></li><li><span class="section"><a href="#ibm-embedding-podvm-image_ibm-osc">6.2.4. Creating a custom peer pod VM image</a></span></li><li><span class="section"><a href="#creating-peer-pods-secret_ibm-osc">6.2.5. Creating the peer pods secret</a></span></li><li><span class="section"><a href="#creating-peer-pods-config-map_ibm-osc">6.2.6. Creating the peer pods config map</a></span></li><li><span class="section"><a href="#creating-libvirt-config-map_ibm-osc">6.2.7. Creating the peer pod VM image config map</a></span></li><li><span class="section"><a href="#creating-ssh-key-secret_ibm-osc">6.2.8. Creating the KVM host secret</a></span></li><li><span class="section"><a href="#creating-kataconfig-cr-cli_ibm-osc">6.2.9. Creating the KataConfig custom resource</a></span></li><li><span class="section"><a href="#configuring-workload-objects_ibm-osc">6.2.10. Configuring workload objects</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#deploying-cc_ibm-cc">7. Deploying Confidential Containers on IBM Z and IBM LinuxONE</a></span><ul><li><span class="section"><a href="#cc-installing-cc-operator-cli_ibm-cc">7.1. Installing the Confidential compute attestation Operator</a></span></li><li><span class="section"><a href="#cc-enabling-feature-gate_ibm-cc">7.2. Enabling the Confidential Containers feature gate</a></span></li><li><span class="section"><a href="#cc-creating-route_ibm-cc">7.3. Creating the route for Trustee</a></span></li><li><span class="section"><a href="#creating-peer-pods-config-map_ibm-cc">7.4. Updating the peer pods config map</a></span></li><li><span class="section"><a href="#deleting-cr-cli_ibm-cc">7.5. Deleting the KataConfig custom resource</a></span></li><li><span class="section"><a href="#creating-peer-pods-secret_ibm-cc">7.6. Updating the peer pods secret</a></span></li><li><span class="section"><a href="#creating-kataconfig-cr-cli_ibm-cc">7.7. Re-creating the KataConfig custom resource</a></span></li><li><span class="section"><a href="#cc-creating-trustee-auth-secret_ibm-cc">7.8. Creating the Trustee authentication secret</a></span></li><li><span class="section"><a href="#cc-creating-trustee-config-map_ibm-cc">7.9. Creating the Trustee config map</a></span></li><li><span class="section"><a href="#ibm-cc-obtaining-se-header.adoc_ibm-cc">7.10. Obtaining the IBM Secure Execution header</a></span></li><li><span class="section"><a href="#ibm-cc-obtaining-attest-fields-certs-keys_ibm-cc">7.11. Configuring the IBM Secure Execution certificates and keys</a></span></li><li><span class="section"><a href="#ibm-cc-creating-persistent-storage-components_ibm-cc">7.12. Creating the persistent storage components</a></span></li><li><span class="section"><a href="#configuring-trustee_ibm-cc">7.13. Configuring Trustee values, policies, and secrets</a></span><ul><li><span class="section"><a href="#cc-configuring-reference-values_ibm-cc">7.13.1. Configuring reference values</a></span></li><li><span class="section"><a href="#cc-creating-attestation-policy_ibm-cc">7.13.2. Creating the attestation policy for IBM Secure Execution</a></span></li><li><span class="section"><a href="#cc-creating-secret-for-clients_ibm-cc">7.13.3. Creating a secret with custom keys for clients</a></span></li><li><span class="section"><a href="#cc-creating-secret-signed-container-images_ibm-cc">7.13.4. Creating a secret for container image signature verification</a></span></li><li><span class="section"><a href="#cc-creating-container-image-sig-policy_ibm-cc">7.13.5. Creating the container image signature verification policy</a></span></li><li><span class="section"><a href="#cc-creating-resource-access-policy_ibm-cc">7.13.6. Creating the resource access policy</a></span></li></ul></li><li><span class="section"><a href="#cc-creating-kbsconfig-cr_ibm-cc">7.14. Creating the KbsConfig custom resource</a></span></li><li><span class="section"><a href="#cc-verifing-trustee-config_ibm-cc">7.15. Verifying the Trustee configuration</a></span></li><li><span class="section"><a href="#verifying-attestation-process_ibm-cc">7.16. Verifying the attestation process</a></span></li></ul></li><li><span class="chapter"><a href="#monitoring">8. Monitoring</a></span><ul><li><span class="section"><a href="#about-metrics_monitoring">8.1. About metrics</a></span></li><li><span class="section"><a href="#viewing-metrics_monitoring">8.2. Viewing metrics</a></span></li></ul></li><li><span class="chapter"><a href="#uninstalling">9. Uninstalling</a></span><ul><li><span class="section"><a href="#uninstalling-ocs">9.1. Uninstalling OpenShift sandboxed containers</a></span><ul><li><span class="section"><a href="#uninstalling-osc-by-using-web-console">9.1.1. Uninstalling OpenShift sandboxed containers by using the web console</a></span></li><li><span class="section"><a href="#uninstalling-osc-by-using-cli">9.1.2. Uninstalling OpenShift sandboxed containers by using the CLI</a></span></li></ul></li><li><span class="section"><a href="#removing-cc-environment">9.2. Removing the Confidential Containers environment</a></span><ul><li><span class="section"><a href="#removing-cc-environment-web">9.2.1. Removing the Confidential Containers environment by using the web console</a></span></li><li><span class="section"><a href="#removing-cc-environment-cli">9.2.2. Removing the Confidential Containers environment by using the CLI</a></span></li></ul></li></ul></li><li><span class="chapter"><a href="#upgrading">10. Upgrading</a></span><ul><li><span class="section"><a href="#upgrading-resources">10.1. Upgrading resources</a></span></li><li><span class="section"><a href="#upgrading-operator">10.2. Upgrading the Operator</a></span></li></ul></li><li><span class="chapter"><a href="#troubleshooting">11. Troubleshooting</a></span><ul><li><span class="section"><a href="#collect_data_rh_support">11.1. Collecting data for Red Hat Support</a></span></li><li><span class="section"><a href="#collecting-log-data">11.2. Collecting log data</a></span><ul><li><span class="section"><a href="#enabling-debug-logs-crio_troubleshooting">11.2.1. Enabling debug logs for CRI-O runtime</a></span></li><li><span class="section"><a href="#viewing-debug-logs-components_troubleshooting">11.2.2. Viewing debug logs for components</a></span></li></ul></li></ul></li><li><span class="appendix"><a href="#kataconfig-status-messages">A. KataConfig status messages</a></span></li></ul></div><section class="preface" id="idm46763391618752"><div class="titlepage"><div><div><h1 class="title">Preface</h1></div></div></div><h2 id="providing-feedback-on-red-hat-documentation">Providing feedback on Red Hat documentation</h2><p>
			You can provide feedback or report an error by submitting the <span class="strong strong"><strong>Create Issue</strong></span> form in Jira. The Jira issue will be created in the Red Hat Hybrid Cloud Infrastructure Jira project, where you can track the progress of your feedback.
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Ensure that you are logged in to Jira. If you do not have a Jira account, you must create a <a class="link" href="https://issues.redhat.com">Red Hat Jira account</a>.
				</li><li class="listitem">
					Launch the <a class="link" href="https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12341520&amp;summary=Documentation+feedback&amp;issuetype=1&amp;description=Details:%0A%0ADocumentation+URL:%0A%0A&amp;priority=10200&amp;labels=hcidocs-feedback&amp;components=12393342"><span class="strong strong"><strong>Create Issue</strong></span> form</a>.
				</li><li class="listitem"><p class="simpara">
					Complete the <span class="strong strong"><strong>Summary</strong></span>, <span class="strong strong"><strong>Description</strong></span>, and <span class="strong strong"><strong>Reporter</strong></span> fields.
				</p><p class="simpara">
					In the <span class="strong strong"><strong>Description</strong></span> field, include the documentation URL, chapter or section number, and a detailed description of the issue.
				</p></li><li class="listitem">
					Click <span class="strong strong"><strong>Create</strong></span>.
				</li></ol></div></section><section class="chapter" id="about-osc"><div class="titlepage"><div><div><h1 class="title">Chapter 1. About OpenShift sandboxed containers</h1></div></div></div><p>
			OpenShift sandboxed containers for OpenShift Container Platform integrates Kata Containers as an optional runtime, providing enhanced security and isolation by running containerized applications in lightweight virtual machines. This integration provides a more secure runtime environment for sensitive workloads without significant changes to existing OpenShift workflows. This runtime supports containers in dedicated virtual machines (VMs), providing improved workload isolation.
		</p><section class="section" id="osc-features_about-osc"><div class="titlepage"><div><div><h2 class="title">1.1. Features</h2></div></div></div><p>
				OpenShift sandboxed containers provides the following features:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Run privileged or untrusted workloads</span></dt><dd><p class="simpara">
							You can safely run workloads that require specific privileges, without the risk of compromising cluster nodes by running privileged containers. Workloads that require special privileges include the following:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Workloads that require special capabilities from the kernel, beyond the default ones granted by standard container runtimes such as CRI-O, for example to access low-level networking features.
								</li><li class="listitem">
									Workloads that need elevated root privileges, for example to access a specific physical device. With OpenShift sandboxed containers, it is possible to pass only a specific device through to the virtual machines (VM), ensuring that the workload cannot access or misconfigure the rest of the system.
								</li><li class="listitem"><p class="simpara">
									Workloads for installing or using <code class="literal">set-uid</code> root binaries. These binaries grant special privileges and, as such, can present a security risk. With OpenShift sandboxed containers, additional privileges are restricted to the virtual machines, and grant no special access to the cluster nodes.
								</p><p class="simpara">
									Some workloads require privileges specifically for configuring the cluster nodes. Such workloads should still use privileged containers, because running on a virtual machine would prevent them from functioning.
								</p></li></ul></div></dd><dt><span class="term">Ensure isolation for sensitive workloads</span></dt><dd>
							The OpenShift sandboxed containers for Red Hat OpenShift Container Platform integrates Kata Containers as an optional runtime, providing enhanced security and isolation by running containerized applications in lightweight virtual machines. This integration provides a more secure runtime environment for sensitive workloads without significant changes to existing OpenShift workflows. This runtime supports containers in dedicated virtual machines (VMs), providing improved workload isolation.
						</dd><dt><span class="term">Ensure kernel isolation for each workload</span></dt><dd>
							You can run workloads that require custom kernel tuning (such as <code class="literal">sysctl</code>, scheduler changes, or cache tuning) and the creation of custom kernel modules (such as <code class="literal">out of tree</code> or special arguments).
						</dd><dt><span class="term">Share the same workload across tenants</span></dt><dd>
							You can run workloads that support many users (tenants) from different organizations sharing the same OpenShift Container Platform cluster. The system also supports running third-party workloads from multiple vendors, such as container network functions (CNFs) and enterprise applications. Third-party CNFs, for example, may not want their custom settings interfering with packet tuning or with <code class="literal">sysctl</code> variables set by other applications. Running inside a completely isolated kernel is helpful in preventing "noisy neighbor" configuration problems.
						</dd><dt><span class="term">Ensure proper isolation and sandboxing for testing software</span></dt><dd>
							You can run containerized workloads with known vulnerabilities or handle issues in an existing application. This isolation enables administrators to give developers administrative control over pods, which is useful when the developer wants to test or validate configurations beyond those an administrator would typically grant. Administrators can, for example, safely and securely delegate kernel packet filtering (eBPF) to developers. eBPF requires <code class="literal">CAP_ADMIN</code> or <code class="literal">CAP_BPF</code> privileges, and is therefore not allowed under a standard CRI-O configuration, as this would grant access to every process on the Container Host worker node. Similarly, administrators can grant access to intrusive tools such as <code class="literal">SystemTap</code>, or support the loading of custom kernel modules during their development.
						</dd><dt><span class="term">Ensure default resource containment through VM boundaries</span></dt><dd>
							By default, OpenShift sandboxed containers manages resources such as CPU, memory, storage, and networking in a robust and secure way. Since OpenShift sandboxed containers deploys on VMs, additional layers of isolation and security give a finer-grained access control to the resource. For example, an errant container will not be able to assign more memory than is available to the VM. Conversely, a container that needs dedicated access to a network card or to a disk can take complete control over that device without getting any access to other devices.
						</dd></dl></div></section><section class="section" id="compatiblity-platforms_about-osc"><div class="titlepage"><div><div><h2 class="title">1.2. Compatibility with OpenShift Container Platform</h2></div></div></div><p>
				The required functionality for the OpenShift Container Platform platform is supported by two main components:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Kata runtime: This includes Red Hat Enterprise Linux CoreOS (RHCOS) and <a class="link" href="https://access.redhat.com/support/policy/updates/openshift/">updates</a> with every OpenShift Container Platform release.
					</li><li class="listitem">
						OpenShift sandboxed containers Operator: Install the Operator using either the web console or OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><p>
				The OpenShift sandboxed containers Operator is a <a class="link" href="https://access.redhat.com/support/policy/updates/openshift_operators#rolling-stream">Rolling Stream Operator</a>, which means the latest version is the only supported version. It works with all currently supported versions of OpenShift Container Platform. For more information, see <a class="link" href="https://access.redhat.com/support/policy/updates/openshift/">OpenShift Container Platform Life Cycle Policy</a> for additional details.
			</p><p>
				The Operator depends on the features that come with the RHCOS host and the environment it runs in.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					You must install Red Hat Enterprise Linux CoreOS (RHCOS) on the worker nodes. RHEL nodes are not supported.
				</p></div></div><p>
				The following compatibility matrix for OpenShift sandboxed containers and OpenShift Container Platform releases identifies compatible features and environments.
			</p><div class="table" id="idm46763389758608"><p class="title"><strong>Table 1.1. Supported architectures</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="bottom" id="idm46763389409760" scope="col">Architecture</th><th align="left" valign="bottom" id="idm46763389408672" scope="col">OpenShift Container Platform version</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm46763389409760">
							<p>
								x86_64
							</p>
							</td><td align="left" valign="top" headers="idm46763389408672">
							<p>
								4.8 or later
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46763389409760">
							<p>
								s390x
							</p>
							</td><td align="left" valign="top" headers="idm46763389408672">
							<p>
								4.14 or later
							</p>
							</td></tr></tbody></table></div></div><p>
				There are two ways to deploy Kata containers runtime:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Bare metal
					</li><li class="listitem">
						Peer pods
					</li></ul></div><p>
				Peer pods technology for the deployment of OpenShift sandboxed containers in public clouds was available as Developer Preview in OpenShift sandboxed containers 1.5 and OpenShift Container Platform 4.14.
			</p><p>
				With the release of OpenShift sandboxed containers 1.7, the Operator requires OpenShift Container Platform version 4.15 or later.
			</p><div class="table" id="idm46763393513088"><p class="title"><strong>Table 1.2. Feature availability by OpenShift version</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"/><col style="width: 25%; " class="col_2"/><col style="width: 25%; " class="col_3"/><col style="width: 25%; " class="col_4"/></colgroup><thead><tr><th align="left" valign="bottom" id="idm46763389450592" scope="col">Feature</th><th align="left" valign="bottom" id="idm46763391363488" scope="col">Deployment method</th><th align="left" valign="bottom" id="idm46763391362400" scope="col">OpenShift Container Platform 4.15</th><th align="left" valign="bottom" id="idm46763391361296" scope="col">OpenShift Container Platform 4.16</th></tr></thead><tbody><tr><td rowspan="2" align="left" valign="top" headers="idm46763389450592">
							<p>
								Confidential Containers
							</p>
							</td><td align="left" valign="top" headers="idm46763391363488">
							<p>
								Bare metal
							</p>
							</td><td align="left" valign="top" headers="idm46763391362400"> </td><td align="left" valign="top" headers="idm46763391361296"> </td></tr><tr><td align="left" valign="top" headers="idm46763391363488">
							<p>
								Peer pods
							</p>
							</td><td align="left" valign="top" headers="idm46763391362400">
							<p>
								Technology Preview
							</p>
							</td><td align="left" valign="top" headers="idm46763391361296">
							<p>
								Technology Preview <sup>[1]</sup>
							</p>
							</td></tr><tr><td rowspan="2" align="left" valign="top" headers="idm46763389450592">
							<p>
								GPU support <sup>[2]</sup>
							</p>
							</td><td align="left" valign="top" headers="idm46763391363488">
							<p>
								Bare metal
							</p>
							</td><td align="left" valign="top" headers="idm46763391362400"> </td><td align="left" valign="top" headers="idm46763391361296"> </td></tr><tr><td align="left" valign="top" headers="idm46763391363488">
							<p>
								Peer pods
							</p>
							</td><td align="left" valign="top" headers="idm46763391362400">
							<p>
								Developer Preview
							</p>
							</td><td align="left" valign="top" headers="idm46763391361296">
							<p>
								Developer Preview
							</p>
							</td></tr></tbody></table></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Technology Preview of Confidential Containers has been available since OpenShift sandboxed containers 1.7.0.
					</li><li class="listitem">
						GPU functionality is not available on IBM Z.
					</li></ol></div><div class="table" id="idm46763389183728"><p class="title"><strong>Table 1.3. Supported cloud platforms for OpenShift sandboxed containers</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"/><col style="width: 33%; " class="col_2"/><col style="width: 33%; " class="col_3"/></colgroup><thead><tr><th align="left" valign="bottom" id="idm46763392078192" scope="col">Platform</th><th align="left" valign="bottom" id="idm46763392077104" scope="col">GPU</th><th align="left" valign="bottom" id="idm46763392076016" scope="col">Confidential Containers</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm46763392078192">
							<p>
								AWS Cloud Computing Services
							</p>
							</td><td align="left" valign="top" headers="idm46763392077104">
							<p>
								Developer Preview
							</p>
							</td><td align="left" valign="top" headers="idm46763392076016"> </td></tr><tr><td align="left" valign="top" headers="idm46763392078192">
							<p>
								Microsoft Azure Cloud Computing Services
							</p>
							</td><td align="left" valign="top" headers="idm46763392077104">
							<p>
								Developer Preview
							</p>
							</td><td align="left" valign="top" headers="idm46763392076016">
							<p>
								Technology Preview <sup>[1]</sup>
							</p>
							</td></tr></tbody></table></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Technology Preview of Confidential Containers has been available since OpenShift sandboxed containers 1.7.0.
					</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://access.redhat.com/support/offerings/devpreview">Developer Preview Support Scope</a>
					</li><li class="listitem">
						<a class="link" href="https://access.redhat.com/support/offerings/techpreview">Technology Preview Features - Scope of Support</a>
					</li></ul></div></section><section class="section" id="about-node-eligibility-checks_about-osc"><div class="titlepage"><div><div><h2 class="title">1.3. Node eligibility checks</h2></div></div></div><p id="node-eligibility-checks">
				You can verify that your bare-metal cluster nodes support OpenShift sandboxed containers by running a node eligibility check. The most common reason for node ineligibility is lack of virtualization support. If you run sandboxed workloads on ineligible nodes, you will experience errors.
			</p><div class="orderedlist"><p class="title"><strong>High-level workflow</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Install the Node Feature Discovery Operator.
					</li><li class="listitem">
						Create the <code class="literal">NodeFeatureDiscovery</code> custom resource (CR).
					</li><li class="listitem">
						Enable node eligibility checks when you create the <code class="literal">Kataconfig</code> CR. You can run node eligibility checks on all worker nodes or on selected nodes.
					</li></ol></div><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/specialized_hardware_and_driver_enablement/index#about-node-feature-discovery-operator_node-feature-discovery-operator">Installing the Node Feature Discovery Operator</a>
					</li></ul></div></section><section class="section" id="common-terms_about-osc"><div class="titlepage"><div><div><h2 class="title">1.4. Common terms</h2></div></div></div><p>
				The following terms are used throughout the documentation.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Sandbox</span></dt><dd><p class="simpara">
							A sandbox is an isolated environment where programs can run. In a sandbox, you can run untested or untrusted programs without risking harm to the host machine or the operating system.
						</p><p class="simpara">
							In the context of OpenShift sandboxed containers, sandboxing is achieved by running workloads in a different kernel using virtualization, providing enhanced control over the interactions between multiple workloads that run on the same host.
						</p></dd><dt><span class="term">Pod</span></dt><dd><p class="simpara">
							A pod is a construct that is inherited from Kubernetes and OpenShift Container Platform. It represents resources where containers can be deployed. Containers run inside of pods, and pods are used to specify resources that can be shared between multiple containers.
						</p><p class="simpara">
							In the context of OpenShift sandboxed containers, a pod is implemented as a virtual machine. Several containers can run in the same pod on the same virtual machine.
						</p></dd><dt><span class="term">OpenShift sandboxed containers Operator</span></dt><dd>
							The OpenShift sandboxed containers Operator manages the lifecycle of sandboxed containers on a cluster. You can use the OpenShift sandboxed containers Operator to perform tasks such as the installation and removal of sandboxed containers, software updates, and status monitoring.
						</dd><dt><span class="term">Kata Containers</span></dt><dd>
							Kata Containers is a core upstream project that is used to build OpenShift sandboxed containers. OpenShift sandboxed containers integrate Kata Containers with OpenShift Container Platform.
						</dd><dt><span class="term">KataConfig</span></dt><dd>
							<code class="literal">KataConfig</code> objects represent configurations of sandboxed containers. They store information about the state of the cluster, such as the nodes on which the software is deployed.
						</dd><dt><span class="term">Runtime class</span></dt><dd>
							A <code class="literal">RuntimeClass</code> object describes which runtime can be used to run a given workload. A runtime class that is named <code class="literal">kata</code> is installed and deployed by the OpenShift sandboxed containers Operator. The runtime class contains information about the runtime that describes resources that the runtime needs to operate, such as the <a class="link" href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</a>.
						</dd></dl></div><div class="variablelist" id="peer-pods"><dl class="variablelist"><dt><span class="term">Peer pod</span></dt><dd><p class="simpara">
							A peer pod in OpenShift sandboxed containers extends the concept of a standard pod. Unlike a standard sandboxed container, where the virtual machine is created on the worker node itself, in a peer pod, the virtual machine is created through a remote hypervisor using any supported hypervisor or cloud provider API.
						</p><p class="simpara">
							The peer pod acts as a regular pod on the worker node, with its corresponding VM running elsewhere. The remote location of the VM is transparent to the user and is specified by the runtime class in the pod specification. The peer pod design circumvents the need for nested virtualization.
						</p></dd><dt><span class="term">IBM Secure Execution</span></dt><dd>
							IBM Secure Execution for Linux is an advanced security feature introduced with IBM z15® and LinuxONE III. This feature extends the protection provided by pervasive encryption. IBM Secure Execution safeguards data at rest, in transit, and in use. It enables secure deployment of workloads and ensures data protection throughout its lifecycle. For more information, see <a class="link" href="https://www.ibm.com/docs/en/linux-on-systems?topic=management-secure-execution">Introducing IBM Secure Execution for Linux</a>.
						</dd><dt><span class="term">Confidential Containers</span></dt><dd><p class="simpara">
							Confidential Containers protects containers and data by verifying that your workload is running in a Trusted Execution Environment (TEE). You can deploy this feature to safeguard the privacy of big data analytics and machine learning inferences.
						</p><p class="simpara">
							<span class="strong strong"><strong>Trustee</strong></span> is a component of Confidential Containers. Trustee is an attestation service that verifies the trustworthiness of the location where you plan to run your workload or where you plan to send confidential information. Trustee includes components deployed on a trusted side and used to verify whether the remote workload is running in a Trusted Execution Environment (TEE). Trustee is flexible and can be deployed in several different configurations to support a wide variety of applications and hardware platforms.
						</p></dd><dt><span class="term">Confidential compute attestation Operator</span></dt><dd>
							The Confidential compute attestation Operator manages the installation, lifecycle, and configuration of Confidential Containers.
						</dd></dl></div></section><section class="section" id="osc-operator_about-osc"><div class="titlepage"><div><div><h2 class="title">1.5. OpenShift sandboxed containers Operator</h2></div></div></div><p>
				The OpenShift sandboxed containers Operator encapsulates all of the components from Kata containers. It manages installation, lifecycle, and configuration tasks.
			</p><p>
				The OpenShift sandboxed containers Operator is packaged in the <a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/operator_sdk/index#osdk-working-bundle-images.html">Operator bundle format</a> as two container images:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The bundle image contains metadata and is required to make the operator OLM-ready.
					</li><li class="listitem">
						The second container image contains the actual controller that monitors and manages the <code class="literal">KataConfig</code> resource.
					</li></ul></div><p>
				The OpenShift sandboxed containers Operator is based on the Red Hat Enterprise Linux CoreOS (RHCOS) extensions concept. RHCOS extensions are a mechanism to install optional OpenShift Container Platform software. The OpenShift sandboxed containers Operator uses this mechanism to deploy sandboxed containers on a cluster.
			</p><p>
				The sandboxed containers RHCOS extension contains RPMs for Kata, QEMU, and its dependencies. You can enable them by using the <code class="literal">MachineConfig</code> resources that the Machine Config Operator provides.
			</p><div class="itemizedlist"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/post_installation_configuration/index#rhcos-add-extensions_post-install-machine-configuration-tasks">Adding extensions to RHCOS</a>
					</li></ul></div></section><section class="section" id="about-confidential-containers_about-osc"><div class="titlepage"><div><div><h2 class="title">1.6. About Confidential Containers</h2></div></div></div><p>
				Confidential Containers provides a confidential computing environment to protect containers and data by leveraging <a class="link" href="https://en.wikipedia.org/wiki/Trusted_execution_environment">Trusted Execution Environments</a>.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Confidential Containers on Microsoft Azure Cloud Computing Services, IBM Z®, and IBM® LinuxONE is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p>
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
				</p></div></div><p>
				You can sign container images by using a tool such as <a class="link" href="https://developers.redhat.com/products/trusted-artifact-signer/overview">Red Hat Trusted Artifact Signer</a>. Then, you create a container image signature verification policy.
			</p><p>
				The Trustee Operator verifies the signatures, ensuring that only trusted and authenticated container images are deployed in your environment.
			</p><p>
				For more information, see <a class="link" href="https://www.redhat.com/en/blog/exploring-openshift-confidential-containers-solution">Exploring the OpenShift Confidential Containers solution</a>.
			</p></section><section class="section" id="ocp-virt-and-osc_about-osc"><div class="titlepage"><div><div><h2 class="title">1.7. OpenShift Virtualization</h2></div></div></div><p>
				You can deploy OpenShift sandboxed containers on clusters with OpenShift Virtualization.
			</p><p>
				To run OpenShift Virtualization and OpenShift sandboxed containers at the same time, your virtual machines must be live migratable so that they do not block node reboots. See <a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/virtualization/index#virt-about-live-migration">About live migration</a> in the OpenShift Virtualization documentation for details.
			</p></section><section class="section" id="block_volume_support_about-osc"><div class="titlepage"><div><div><h2 class="title">1.8. Block volume support</h2></div></div></div><p>
				OpenShift Container Platform can statically provision raw block volumes. These volumes do not have a file system, and can provide performance benefits for applications that either write to the disk directly or implement their own storage service.
			</p><p>
				You can use a local block device as persistent volume (PV) storage with OpenShift sandboxed containers. This block device can be provisioned by using the Local Storage Operator (LSO).
			</p><p>
				The Local Storage Operator is not installed in OpenShift Container Platform by default. See <a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/storage/configuring-persistent-storage#local-storage-install_persistent-storage-local">Installing the Local Storage Operator</a> for installation instructions.
			</p><p>
				You can provision raw block volumes for OpenShift sandboxed containers by specifying <code class="literal">volumeMode: Block</code> in the PV specification.
			</p><div class="formalpara"><p class="title"><strong>Block volume example</strong></p><p>
					
<pre class="programlisting language-yaml">apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-disks"
  namespace: "openshift-local-storage"
spec:
  nodeSelector:
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-0
  storageClassDevices:
    - storageClassName: "local-sc"
      forceWipeDevicesAndDestroyAllData: false
      volumeMode: Block <span id="CO1-1"/><span class="callout">1</span>
      devicePaths:
        - /path/to/device <span id="CO1-2"/><span class="callout">2</span></pre>
				</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						Set <code class="literal">volumeMode</code> to <code class="literal">Block</code> to indicate that this PV is a raw block volume.
					</div></dd><dt><a href="#CO1-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						Replace this value with the filepath to your <code class="literal">LocalVolume</code> resource <code class="literal">by-id</code>. PVs are created for these local disks when the provisioner is deployed successfully. You must also use this path to label the node that uses the block device when deploying OpenShift sandboxed containers.
					</div></dd></dl></div></section><section class="section" id="fips-compliance_about-osc"><div class="titlepage"><div><div><h2 class="title">1.9. FIPS compliance</h2></div></div></div><p>
				OpenShift Container Platform is designed for Federal Information Processing Standards (FIPS) 140-2 and 140-3. When running Red Hat Enterprise Linux (RHEL) or Red Hat Enterprise Linux CoreOS (RHCOS) booted in FIPS mode, OpenShift Container Platform core components use the RHEL cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the <code class="literal">x86_64</code>, <code class="literal">ppc64le</code>, and <code class="literal">s390x</code> architectures.
			</p><p>
				For more information about the NIST validation program, see <a class="link" href="https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules">Cryptographic Module Validation Program</a>. For the latest NIST status for the individual versions of RHEL cryptographic libraries that have been submitted for validation, see <a class="link" href="https://access.redhat.com/articles/2918071#fips-140-2-and-fips-140-3-2">Compliance Activities and Government Standards</a>.
			</p><p>
				OpenShift sandboxed containers can be used on FIPS enabled clusters.
			</p><p>
				When running in FIPS mode, OpenShift sandboxed containers components, VMs, and VM images are adapted to comply with FIPS.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					FIPS compliance for OpenShift sandboxed containers only applies to the <code class="literal">kata</code> runtime class. The peer pod runtime class, <code class="literal">kata-remote</code>, is not yet fully supported and has not been tested for FIPS compliance.
				</p></div></div><p>
				FIPS compliance is one of the most critical components required in highly secure environments, to ensure that only supported cryptographic technologies are allowed on nodes.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The use of FIPS Validated / Modules in Process cryptographic libraries is only supported on OpenShift Container Platform deployments on the <code class="literal">x86_64</code> architecture.
				</p></div></div><p>
				To understand Red Hat’s view of OpenShift Container Platform compliance frameworks, refer to the Risk Management and Regulatory Readiness chapter of the <a class="link" href="https://access.redhat.com/articles/5059881">OpenShift Security Guide Book</a>.
			</p></section></section><section class="chapter" id="deploying-osc-bare-metal"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Deploying OpenShift sandboxed containers on bare metal</h1></div></div></div><p>
			You can deploy OpenShift sandboxed containers on an on-premise bare-metal cluster with Red Hat Enterprise Linux CoreOS (RHCOS) installed on the worker nodes.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						RHEL nodes are not supported.
					</li><li class="listitem">
						Nested virtualization is not supported.
					</li></ul></div></div></div><p>
			You can use any installation method including <a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/installing/index#installing-bare-metal">user-provisioned</a>, <a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/installing/index#deploying-installer-provisioned-clusters-on-bare-metal">installer-provisioned</a>, or <a class="link" href="https://access.redhat.com/documentation/en-us/assisted_installer_for_openshift_container_platform">Assisted Installer</a> to deploy your cluster.
		</p><p>
			You can also install OpenShift sandboxed containers on Amazon Web Services (AWS) bare-metal instances. Bare-metal instances offered by other cloud providers are not supported.
		</p><div class="itemizedlist"><p class="title"><strong>Cluster requirements</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
					You have installed Red Hat OpenShift Container Platform 4.14 or later on the cluster where you are installing the OpenShift sandboxed containers Operator.
				</li><li class="listitem">
					Your cluster has at least one worker node.
				</li></ul></div><section class="section" id="osc-resource-requirements_deploying-bare-metal"><div class="titlepage"><div><div><h2 class="title">2.1. OpenShift sandboxed containers resource requirements</h2></div></div></div><p>
				You must ensure that your cluster has sufficient resources.
			</p><p>
				OpenShift sandboxed containers lets users run workloads on their OpenShift Container Platform clusters inside a sandboxed runtime (Kata). Each pod is represented by a virtual machine (VM). Each VM runs in a QEMU process and hosts a <code class="literal">kata-agent</code> process that acts as a supervisor for managing container workloads, and the processes running in those containers. Two additional processes add more overhead:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">containerd-shim-kata-v2</code> is used to communicate with the pod.
					</li><li class="listitem">
						<code class="literal">virtiofsd</code> handles host file system access on behalf of the guest.
					</li></ul></div><p>
				Each VM is configured with a default amount of memory. Additional memory is hot-plugged into the VM for containers that explicitly request memory.
			</p><p>
				A container running without a memory resource consumes free memory until the total memory used by the VM reaches the default allocation. The guest and its I/O buffers also consume memory.
			</p><p>
				If a container is given a specific amount of memory, then that memory is hot-plugged into the VM before the container starts.
			</p><p>
				When a memory limit is specified, the workload is terminated if it consumes more memory than the limit. If no memory limit is specified, the kernel running on the VM might run out of memory. If the kernel runs out of memory, it might terminate other processes on the VM.
			</p><div class="formalpara"><p class="title"><strong>Default memory sizes</strong></p><p>
					The following table lists some the default values for resource allocation.
				</p></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm46763389297728" scope="col">Resource</th><th align="left" valign="top" id="idm46763389296640" scope="col">Value</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm46763389297728">
							<p>
								Memory allocated by default to a virtual machine
							</p>
							</td><td align="left" valign="top" headers="idm46763389296640">
							<p>
								2Gi
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46763389297728">
							<p>
								Guest Linux kernel memory usage at boot
							</p>
							</td><td align="left" valign="top" headers="idm46763389296640">
							<p>
								~110Mi
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46763389297728">
							<p>
								Memory used by the QEMU process (excluding VM memory)
							</p>
							</td><td align="left" valign="top" headers="idm46763389296640">
							<p>
								~30Mi
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46763389297728">
							<p>
								Memory used by the <code class="literal">virtiofsd</code> process (excluding VM I/O buffers)
							</p>
							</td><td align="left" valign="top" headers="idm46763389296640">
							<p>
								~10Mi
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46763389297728">
							<p>
								Memory used by the <code class="literal">containerd-shim-kata-v2</code> process
							</p>
							</td><td align="left" valign="top" headers="idm46763389296640">
							<p>
								~20Mi
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46763389297728">
							<p>
								File buffer cache data after running <code class="literal">dnf install</code> on Fedora
							</p>
							</td><td align="left" valign="top" headers="idm46763389296640">
							<p>
								~300Mi* <sup>[1]</sup>
							</p>
							</td></tr></tbody></table></div><p>
				File buffers appear and are accounted for in multiple locations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						In the guest where it appears as file buffer cache.
					</li><li class="listitem">
						In the <code class="literal">virtiofsd</code> daemon that maps allowed user-space file I/O operations.
					</li><li class="listitem">
						In the QEMU process as guest memory.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Total memory usage is properly accounted for by the memory utilization metrics, which only count that memory once.
				</p></div></div><p>
				<a class="link" href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">Pod overhead</a> describes the amount of system resources that a pod on a node uses. You can get the current pod overhead for the Kata runtime by using <code class="literal">oc describe runtimeclass kata</code> as shown below.
			</p><div class="formalpara"><p class="title"><strong>Example</strong></p><p>
					
<pre class="programlisting language-terminal">$ oc describe runtimeclass kata</pre>
				</p></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
					
<pre class="programlisting language-terminal">kind: RuntimeClass
apiVersion: node.k8s.io/v1
metadata:
  name: kata
overhead:
  podFixed:
    memory: "500Mi"
    cpu: "500m"</pre>
				</p></div><p>
				You can change the pod overhead by changing the <code class="literal">spec.overhead</code> field for a <code class="literal">RuntimeClass</code>. For example, if the configuration that you run for your containers consumes more than 350Mi of memory for the QEMU process and guest kernel data, you can alter the <code class="literal">RuntimeClass</code> overhead to suit your needs.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The specified default overhead values are supported by Red Hat. Changing default overhead values is not supported and can result in technical issues.
				</p></div></div><p>
				When performing any kind of file system I/O in the guest, file buffers are allocated in the guest kernel. The file buffers are also mapped in the QEMU process on the host, as well as in the <code class="literal">virtiofsd</code> process.
			</p><p>
				For example, if you use 300Mi of file buffer cache in the guest, both QEMU and <code class="literal">virtiofsd</code> appear to use 300Mi additional memory. However, the same memory is used in all three cases. Therefore, the total memory usage is only 300Mi, mapped in three different places. This is correctly accounted for when reporting the memory utilization metrics.
			</p></section><section class="section" id="deploying-osc-web_metal-web"><div class="titlepage"><div><div><h2 class="title">2.2. Deploying OpenShift sandboxed containers by using the web console</h2></div></div></div><p>
				You can deploy OpenShift sandboxed containers on bare metal by using the OpenShift Container Platform web console to perform the following tasks:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Install the OpenShift sandboxed containers Operator.
					</li><li class="listitem">
						Optional: Install the Node Feature Discovery (NFD) Operator to configure node eligibility checks. For more information, see <a class="link" href="#about-node-eligibility-checks_about-osc" title="1.3. Node eligibility checks">node eligibility checks</a> and the <a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/specialized_hardware_and_driver_enablement/index#about-node-feature-discovery-operator_node-feature-discovery-operator">NFD Operator documentation</a>.
					</li><li class="listitem">
						Create the <code class="literal">KataConfig</code> custom resource.
					</li><li class="listitem">
						Configure the OpenShift sandboxed containers workload objects.
					</li></ol></div><section class="section" id="installing-operator-web-console_metal-web"><div class="titlepage"><div><div><h3 class="title">2.2.1. Installing the OpenShift sandboxed containers Operator</h3></div></div></div><p>
					You can install the OpenShift sandboxed containers Operator by using the OpenShift Container Platform web console.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Filter by keyword</strong></span> field, type <code class="literal">OpenShift sandboxed containers</code>.
						</li><li class="listitem">
							Select the <span class="strong strong"><strong>OpenShift sandboxed containers Operator</strong></span> tile and click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Install Operator</strong></span> page, select <span class="strong strong"><strong>stable</strong></span> from the list of available <span class="strong strong"><strong>Update Channel</strong></span> options.
						</li><li class="listitem"><p class="simpara">
							Verify that <span class="strong strong"><strong>Operator recommended Namespace</strong></span> is selected for <span class="strong strong"><strong>Installed Namespace</strong></span>. This installs the Operator in the mandatory <code class="literal">openshift-sandboxed-containers-operator</code> namespace. If this namespace does not yet exist, it is automatically created.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Attempting to install the OpenShift sandboxed containers Operator in a namespace other than <code class="literal">openshift-sandboxed-containers-operator</code> causes the installation to fail.
							</p></div></div></li><li class="listitem">
							Verify that <span class="strong strong"><strong>Automatic</strong></span> is selected for <span class="strong strong"><strong>Approval Strategy</strong></span>. <span class="strong strong"><strong>Automatic</strong></span> is the default value, and enables automatic updates to OpenShift sandboxed containers when a new z-stream release is available.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem">
							Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> to verify that the Operator is installed.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</a>.
						</li><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</a> for disconnected environments.
						</li></ul></div></section><section class="section" id="creating-kataconfig-cr-web_metal-web"><div class="titlepage"><div><div><h3 class="title">2.2.2. Creating the KataConfig custom resource</h3></div></div></div><p>
					You must create the <code class="literal">KataConfig</code> custom resource (CR) to install <code class="literal">kata</code> as a <code class="literal">RuntimeClass</code> on your worker nodes.
				</p><p>
					The <code class="literal">kata</code> runtime class is installed on all worker nodes by default. If you want to install <code class="literal">kata</code> on specific nodes, you can add labels to those nodes and then define the label in the <code class="literal">KataConfig</code> CR.
				</p><p>
					OpenShift sandboxed containers installs <code class="literal">kata</code> as a <span class="emphasis"><em>secondary, optional</em></span> runtime on the cluster and not as the primary runtime.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Creating the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. The following factors might increase the reboot time:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								A larger OpenShift Container Platform deployment with a greater number of worker nodes.
							</li><li class="listitem">
								Activation of the BIOS and Diagnostics utility.
							</li><li class="listitem">
								Deployment on a hard disk drive rather than an SSD.
							</li><li class="listitem">
								Deployment on physical nodes such as bare metal, rather than on virtual nodes.
							</li><li class="listitem">
								A slow CPU and network.
							</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Optional: You have installed the Node Feature Discovery Operator if you want to enable node eligibility checks.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
						</li><li class="listitem">
							Select the OpenShift sandboxed containers Operator.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>KataConfig</strong></span> tab, click <span class="strong strong"><strong>Create KataConfig</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Enter the following details:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>Name</strong></span>: Optional: The default name is <code class="literal">example-kataconfig</code>.
								</li><li class="listitem">
									<span class="strong strong"><strong>Labels</strong></span>: Optional: Enter any relevant, identifying attributes to the <code class="literal">KataConfig</code> resource. Each label represents a key-value pair.
								</li><li class="listitem">
									<span class="strong strong"><strong>checkNodeEligibility</strong></span>: Optional: Select to use the Node Feature Discovery Operator (NFD) to detect node eligibility.
								</li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>kataConfigPoolSelector</strong></span>. Optional: To install <code class="literal">kata</code> on selected nodes, add a match expression for the labels on the selected nodes:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Expand the <span class="strong strong"><strong>kataConfigPoolSelector</strong></span> area.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>kataConfigPoolSelector</strong></span> area, expand <span class="strong strong"><strong>matchExpressions</strong></span>. This is a list of label selector requirements.
										</li><li class="listitem">
											Click <span class="strong strong"><strong>Add matchExpressions</strong></span>.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>Key</strong></span> field, enter the label key the selector applies to.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>Operator</strong></span> field, enter the key’s relationship to the label values. Valid operators are <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, and <code class="literal">DoesNotExist</code>.
										</li><li class="listitem">
											Expand the <span class="strong strong"><strong>Values</strong></span> area and then click <span class="strong strong"><strong>Add value</strong></span>.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>Value</strong></span> field, enter <code class="literal">true</code> or <code class="literal">false</code> for <span class="strong strong"><strong>key</strong></span> label value.
										</li></ol></div></li><li class="listitem">
									<span class="strong strong"><strong>logLevel</strong></span>: Define the level of log data retrieved for nodes with the <code class="literal">kata</code> runtime class.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Create</strong></span>. The <code class="literal">KataConfig</code> CR is created and installs the <code class="literal">kata</code> runtime class on the worker nodes.
						</p><p class="simpara">
							Wait for the <code class="literal">kata</code> installation to complete and the worker nodes to reboot before verifying the installation.
						</p></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the <span class="strong strong"><strong>KataConfig</strong></span> tab, click the <code class="literal">KataConfig</code> CR to view its details.
						</li><li class="listitem"><p class="simpara">
							Click the <span class="strong strong"><strong>YAML</strong></span> tab to view the <code class="literal">status</code> stanza.
						</p><p class="simpara">
							The <code class="literal">status</code> stanza contains the <code class="literal">conditions</code> and <code class="literal">kataNodes</code> keys. The value of <code class="literal">status.kataNodes</code> is an array of nodes, each of which lists nodes in a particular state of <code class="literal">kata</code> installation. A message appears each time there is an update.
						</p></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Reload</strong></span> to refresh the YAML.
						</p><p class="simpara">
							When all workers in the <code class="literal">status.kataNodes</code> array display the values <code class="literal">installed</code> and <code class="literal">conditions.InProgress: False</code> with no specified reason, the <code class="literal">kata</code> is installed on the cluster.
						</p></li></ol></div><h5 id="additional_resources">Additional resources</h5><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="#kataconfig-status-messages" title="Appendix A. KataConfig status messages">KataConfig status messages</a>
						</li></ul></div></section><section class="section" id="configuring-workload-objects_metal-web"><div class="titlepage"><div><div><h3 class="title">2.2.3. Configuring workload objects</h3></div></div></div><p>
					You must configure OpenShift sandboxed containers workload objects by setting <code class="literal">kata</code> as the runtime class for the following pod-templated objects:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">Pod</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicaSet</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicationController</code> objects
						</li><li class="listitem">
							<code class="literal">StatefulSet</code> objects
						</li><li class="listitem">
							<code class="literal">Deployment</code> objects
						</li><li class="listitem">
							<code class="literal">DeploymentConfig</code> objects
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created the <code class="literal">KataConfig</code> custom resource (CR).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Workloads</strong></span> → workload type, for example, <span class="strong strong"><strong>Pods</strong></span>.
						</li><li class="listitem">
							On the workload type page, click an object to view its details.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>YAML</strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							Add <code class="literal">spec.runtimeClassName: kata</code> to the manifest of each pod-templated workload object as in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata
# ...</pre><p class="simpara">
							OpenShift Container Platform creates the workload object and begins scheduling it.
						</p></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Inspect the <code class="literal">spec.runtimeClassName</code> field of a pod-templated object. If the value is <code class="literal">kata</code>, then the workload is running on OpenShift sandboxed containers, using peer pods.
						</li></ul></div></section></section><section class="section" id="deploying-osc-cli_metal-cli"><div class="titlepage"><div><div><h2 class="title">2.3. Deploying OpenShift sandboxed containers by using the command line</h2></div></div></div><p>
				You can deploy OpenShift sandboxed containers on bare metal by using the command line interface (CLI) to perform the following tasks:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Install the OpenShift sandboxed containers Operator.
					</li><li class="listitem"><p class="simpara">
						After installing the Operator, you can configure the following options:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Configure a block storage device.
							</li><li class="listitem"><p class="simpara">
								Install the Node Feature Discovery (NFD) Operator to configure node eligibility checks. For more information, see <a class="link" href="#about-node-eligibility-checks_about-osc" title="1.3. Node eligibility checks">node eligibility checks</a> and the <a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/specialized_hardware_and_driver_enablement/index#about-node-feature-discovery-operator_node-feature-discovery-operator">NFD Operator documentation</a>.
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										Create a <code class="literal">NodeFeatureDiscovery</code> custom resource.
									</li></ul></div></li></ul></div></li><li class="listitem">
						Create the <code class="literal">KataConfig</code> custom resource.
					</li><li class="listitem">
						Optional: Modify the pod overhead.
					</li><li class="listitem">
						Configure the OpenShift sandboxed containers workload objects.
					</li></ol></div><section class="section" id="installing-operator-cli_metal-cli"><div class="titlepage"><div><div><h3 class="title">2.3.1. Installing the OpenShift sandboxed containers Operator</h3></div></div></div><p>
					You can install the OpenShift sandboxed containers Operator by using the CLI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-namespace.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sandboxed-containers-operator</pre></li><li class="listitem"><p class="simpara">
							Create the namespace by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-namespace.yaml</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-operatorgroup.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sandboxed-containers-operator-group
  namespace: openshift-sandboxed-containers-operator
spec:
  targetNamespaces:
  - openshift-sandboxed-containers-operator</pre></li><li class="listitem"><p class="simpara">
							Create the operator group by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-operatorgroup.yaml</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-subscription.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sandboxed-containers-operator
  namespace: openshift-sandboxed-containers-operator
spec:
  channel: stable
  installPlanApproval: Automatic
  name: sandboxed-containers-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: sandboxed-containers-operator.v1.8.0</pre></li><li class="listitem"><p class="simpara">
							Create the subscription by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-subscription.yaml</pre></li><li class="listitem"><p class="simpara">
							Verify that the Operator is correctly installed by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get csv -n openshift-sandboxed-containers-operator</pre><p class="simpara">
							This command can take several minutes to complete.
						</p></li><li class="listitem"><p class="simpara">
							Watch the process by running the following command:
						</p><pre class="programlisting language-terminal">$ watch oc get csv -n openshift-sandboxed-containers-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                             DISPLAY                                  VERSION             REPLACES                   PHASE
openshift-sandboxed-containers   openshift-sandboxed-containers-operator  1.8.0    1.7.0        Succeeded</pre>
							</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</a>.
						</li><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</a> for disconnected environments.
						</li></ul></div></section><section class="section" id="optional_configurations"><div class="titlepage"><div><div><h3 class="title">2.3.2. Optional configurations</h3></div></div></div><p>
					You can configure the following options after you install the OpenShift sandboxed containers Operator.
				</p><section class="section" id="using-osc-local-volume_metal-cli"><div class="titlepage"><div><div><h4 class="title">2.3.2.1. Provisioning local block volumes</h4></div></div></div><p>
						You can use local block volumes with OpenShift sandboxed containers. You must first provision the local block volumes by using the Local Storage Operator (LSO). Then you must enable the nodes with the local block volumes to run OpenShift sandboxed containers workloads.
					</p><p>
						You can provision local block volumes for OpenShift sandboxed containers by using the Local Storage Operator (LSO). The local volume provisioner looks for any block volume devices at the paths specified in the defined resource.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the Local Storage Operator.
							</li><li class="listitem"><p class="simpara">
								You have a local disk that meets the following conditions:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										It is attached to a node.
									</li><li class="listitem">
										It is not mounted.
									</li><li class="listitem">
										It does not contain partitions.
									</li></ul></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create the local volume resource. This resource must define the nodes and paths to the local volumes.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Do not use different storage class names for the same device. Doing so creates multiple persistent volumes (PVs).
								</p></div></div><div class="formalpara"><p class="title"><strong>Example: Block</strong></p><p>
									
<pre class="programlisting language-yaml">apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-disks"
  namespace: "openshift-local-storage" <span id="CO2-1"/><span class="callout">1</span>
spec:
  nodeSelector: <span id="CO2-2"/><span class="callout">2</span>
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - ip-10-0-136-143
          - ip-10-0-140-255
          - ip-10-0-144-180
  storageClassDevices:
    - storageClassName: "local-sc" <span id="CO2-3"/><span class="callout">3</span>
      forceWipeDevicesAndDestroyAllData: false <span id="CO2-4"/><span class="callout">4</span>
      volumeMode: Block
      devicePaths: <span id="CO2-5"/><span class="callout">5</span>
        - /path/to/device <span id="CO2-6"/><span class="callout">6</span></pre>
								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The namespace where the Local Storage Operator is installed.
									</div></dd><dt><a href="#CO2-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Optional: A node selector containing a list of nodes where the local storage volumes are attached. This example uses the node hostnames, obtained from <code class="literal">oc get node</code>. If a value is not defined, then the Local Storage Operator will attempt to find matching disks on all available nodes.
									</div></dd><dt><a href="#CO2-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The name of the storage class to use when creating persistent volume objects.
									</div></dd><dt><a href="#CO2-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										This setting defines whether or not to call <code class="literal">wipefs</code>, which removes partition table signatures (magic strings) making the disk ready to use for Local Storage Operator provisioning. No other data besides signatures is erased. The default is "false" (<code class="literal">wipefs</code> is not invoked). Setting <code class="literal">forceWipeDevicesAndDestroyAllData</code> to "true" can be useful in scenarios where previous data can remain on disks that need to be re-used. In these scenarios, setting this field to true eliminates the need for administrators to erase the disks manually.
									</div></dd><dt><a href="#CO2-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										The path containing a list of local storage devices to choose from. You must use this path when enabling a node with a local block device to run OpenShift sandboxed containers workloads.
									</div></dd><dt><a href="#CO2-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Replace this value with the filepath to your <code class="literal">LocalVolume</code> resource <code class="literal">by-id</code>, such as <code class="literal">/dev/disk/by-id/wwn</code>. PVs are created for these local disks when the provisioner is deployed successfully.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create the local volume resource in your OpenShift Container Platform cluster. Specify the file you just created:
							</p><pre class="programlisting language-terminal">$ oc apply -f &lt;local-volume&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
								Verify that the provisioner was created and that the corresponding daemon sets were created:
							</p><pre class="programlisting language-terminal">$ oc get all -n openshift-local-storage</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                                          READY   STATUS    RESTARTS   AGE
pod/diskmaker-manager-9wzms                   1/1     Running   0          5m43s
pod/diskmaker-manager-jgvjp                   1/1     Running   0          5m43s
pod/diskmaker-manager-tbdsj                   1/1     Running   0          5m43s
pod/local-storage-operator-7db4bd9f79-t6k87   1/1     Running   0          14m

NAME                                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
service/local-storage-operator-metrics   ClusterIP   172.30.135.36   &lt;none&gt;        8383/TCP,8686/TCP   14m

NAME                               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/diskmaker-manager   3         3         3       3            3           &lt;none&gt;          5m43s

NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/local-storage-operator   1/1     1            1           14m

NAME                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/local-storage-operator-7db4bd9f79   1         1         1       14m</pre>
								</p></div><p class="simpara">
								Note the <code class="literal">desired</code> and <code class="literal">current</code> number of daemon set processes. A <code class="literal">desired</code> count of <code class="literal">0</code> indicates that the label selectors were invalid.
							</p></li><li class="listitem"><p class="simpara">
								Verify that the persistent volumes were created:
							</p><pre class="programlisting language-terminal">$ oc get pv</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
local-pv-1cec77cf   100Gi      RWO            Delete           Available           local-sc                88m
local-pv-2ef7cd2a   100Gi      RWO            Delete           Available           local-sc                82m
local-pv-3fa1c73    100Gi      RWO            Delete           Available           local-sc                48m</pre>
								</p></div></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Editing the <code class="literal">LocalVolume</code> object does not change existing persistent volumes because doing so might result in a destructive operation.
						</p></div></div></section><section class="section" id="deploy-nodes-block-device_metal-cli"><div class="titlepage"><div><div><h4 class="title">2.3.2.2. Enabling nodes to use a local block device</h4></div></div></div><p>
						You can configure nodes with a local block device to run OpenShift sandboxed containers workloads at the paths specified in the defined volume resource.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You provisioned a block device using the Local Storage Operator (LSO).
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Enable each node with a local block device to run OpenShift sandboxed containers workloads by running the following command:
							</p><pre class="programlisting language-terminal">$ oc debug node/worker-0 -- chcon -vt container_file_t /host/path/to/device</pre><p class="simpara">
								The <code class="literal">/path/to/device</code> must be the same path you defined when creating the local storage resource.
							</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">system_u:object_r:container_file_t:s0 /host/path/to/device</pre>
								</p></div></li></ul></div></section><section class="section" id="creating-nfd-cr_metal-cli"><div class="titlepage"><div><div><h4 class="title">2.3.2.3. Creating a NodeFeatureDiscovery custom resource</h4></div></div></div><p>
						You create a <code class="literal">NodeFeatureDiscovery</code> custom resource (CR) to define the configuration parameters that the Node Feature Discovery (NFD) Operator checks to determine that the worker nodes can support OpenShift sandboxed containers.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							To install the <code class="literal">kata</code> runtime on only selected worker nodes that you know are eligible, apply the <code class="literal">feature.node.kubernetes.io/runtime.kata=true</code> label to the selected nodes and set <code class="literal">checkNodeEligibility: true</code> in the <code class="literal">KataConfig</code> CR.
						</p><p>
							To install the <code class="literal">kata</code> runtime on all worker nodes, set <code class="literal">checkNodeEligibility: false</code> in the <code class="literal">KataConfig</code> CR.
						</p><p>
							In both these scenarios, you do not need to create the <code class="literal">NodeFeatureDiscovery</code> CR. You should only apply the <code class="literal">feature.node.kubernetes.io/runtime.kata=true</code> label manually if you are sure that the node is eligible to run OpenShift sandboxed containers.
						</p></div></div><p>
						The following procedure applies the <code class="literal">feature.node.kubernetes.io/runtime.kata=true</code> label to all eligible nodes and configures the <code class="literal">KataConfig</code> resource to check for node eligibility.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the NFD Operator.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create an <code class="literal">nfd.yaml</code> manifest file according to the following example:
							</p><pre class="programlisting language-yaml">apiVersion: nfd.openshift.io/v1
kind: NodeFeatureDiscovery
metadata:
  name: nfd-kata
  namespace: openshift-nfd
spec:
  workerConfig:
    configData: |
      sources:
        custom:
          - name: "feature.node.kubernetes.io/runtime.kata"
            matchOn:
              - cpuId: ["SSE4", "VMX"]
                loadedKMod: ["kvm", "kvm_intel"]
              - cpuId: ["SSE4", "SVM"]
                loadedKMod: ["kvm", "kvm_amd"]
# ...</pre></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">NodeFeatureDiscovery</code> CR:
							</p><pre class="programlisting language-terminal">$ oc create -f nfd.yaml</pre><p class="simpara">
								The <code class="literal">NodeFeatureDiscovery</code> CR applies the <code class="literal">feature.node.kubernetes.io/runtime.kata=true</code> label to all qualifying worker nodes.
							</p></li></ol></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a <code class="literal">kata-config.yaml</code> manifest file according to the following example:
							</p><pre class="programlisting language-yaml">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  checkNodeEligibility: true</pre></li><li class="listitem"><p class="simpara">
								Create the <code class="literal">KataConfig</code> CR:
							</p><pre class="programlisting language-terminal">$ oc create -f kata-config.yaml</pre></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Verify that qualifying nodes in the cluster have the correct label applied:
							</p><pre class="programlisting language-terminal">$ oc get nodes --selector='feature.node.kubernetes.io/runtime.kata=true'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-terminal">NAME                           STATUS                     ROLES    AGE     VERSION
compute-3.example.com          Ready                      worker   4h38m   v1.25.0
compute-2.example.com          Ready                      worker   4h35m   v1.25.0</pre>
								</p></div></li></ul></div></section></section><section class="section" id="creating-kataconfig-cr-cli_metal-cli"><div class="titlepage"><div><div><h3 class="title">2.3.3. Creating the KataConfig custom resource</h3></div></div></div><p>
					You must create the <code class="literal">KataConfig</code> custom resource (CR) to install <code class="literal">kata</code> as a runtime class on your worker nodes.
				</p><p>
					Creating the <code class="literal">KataConfig</code> CR triggers the OpenShift sandboxed containers Operator to do the following: * Install the needed RHCOS extensions, such as QEMU and <code class="literal">kata-containers</code>, on your RHCOS node. * Ensure that the <a class="link" href="https://github.com/cri-o/cri-o">CRI-O</a> runtime is configured with the correct runtime handlers. * Create a <code class="literal">RuntimeClass</code> CR named <code class="literal">kata</code> with a default configuration. This enables users to configure workloads to use <code class="literal">kata</code> as the runtime by referencing the CR in the <code class="literal">RuntimeClassName</code> field. This CR also specifies the resource overhead for the runtime.
				</p><p>
					OpenShift sandboxed containers installs <code class="literal">kata</code> as a <span class="emphasis"><em>secondary, optional</em></span> runtime on the cluster and not as the primary runtime.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Creating the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								A larger OpenShift Container Platform deployment with a greater number of worker nodes.
							</li><li class="listitem">
								Activation of the BIOS and Diagnostics utility.
							</li><li class="listitem">
								Deployment on a hard disk drive rather than an SSD.
							</li><li class="listitem">
								Deployment on physical nodes such as bare metal, rather than on virtual nodes.
							</li><li class="listitem">
								A slow CPU and network.
							</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Optional: You have installed the Node Feature Discovery Operator if you want to enable node eligibility checks.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">example-kataconfig.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  checkNodeEligibility: false <span id="CO3-1"/><span class="callout">1</span>
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <span id="CO3-2"/><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional: Set`checkNodeEligibility` to <code class="literal">true</code> to run node eligibility checks.
								</div></dd><dt><a href="#CO3-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Optional: If you have applied node labels to install OpenShift sandboxed containers on specific nodes, specify the key and value.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">KataConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f example-kataconfig.yaml</pre><p class="simpara">
							The new <code class="literal">KataConfig</code> CR is created and installs <code class="literal">kata</code> as a runtime class on the worker nodes.
						</p><p class="simpara">
							Wait for the <code class="literal">kata</code> installation to complete and the worker nodes to reboot before verifying the installation.
						</p></li><li class="listitem"><p class="simpara">
							Monitor the installation progress by running the following command:
						</p><pre class="programlisting language-terminal">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</pre><p class="simpara">
							When the status of all workers under <code class="literal">kataNodes</code> is <code class="literal">installed</code> and the condition <code class="literal">InProgress</code> is <code class="literal">False</code> without specifying a reason, the <code class="literal">kata</code> is installed on the cluster.
						</p></li></ol></div></section><section class="section" id="modifying-pod-overhead.adoc_metal-cli"><div class="titlepage"><div><div><h3 class="title">2.3.4. Modifying pod overhead</h3></div></div></div><p>
					<a class="link" href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">Pod overhead</a> describes the amount of system resources that a pod on a node uses. You can modify the pod overhead by changing the <code class="literal">spec.overhead</code> field for a <code class="literal">RuntimeClass</code> custom resource. For example, if the configuration that you run for your containers consumes more than 350Mi of memory for the QEMU process and guest kernel data, you can alter the <code class="literal">RuntimeClass</code> overhead to suit your needs.
				</p><p>
					When performing any kind of file system I/O in the guest, file buffers are allocated in the guest kernel. The file buffers are also mapped in the QEMU process on the host, as well as in the <code class="literal">virtiofsd</code> process.
				</p><p>
					For example, if you use 300Mi of file buffer cache in the guest, both QEMU and <code class="literal">virtiofsd</code> appear to use 300Mi additional memory. However, the same memory is being used in all three cases. Therefore, the total memory usage is only 300Mi, mapped in three different places. This is correctly accounted for when reporting the memory utilization metrics.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The default values are supported by Red Hat. Changing default overhead values is not supported and can result in technical issues.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the <code class="literal">RuntimeClass</code> object by running the following command:
						</p><pre class="programlisting language-terminal">$ oc describe runtimeclass kata</pre></li><li class="listitem"><p class="simpara">
							Update the <code class="literal">overhead.podFixed.memory</code> and <code class="literal">cpu</code> values and save as the file as <code class="literal">runtimeclass.yaml</code>:
						</p><pre class="programlisting language-yaml">kind: RuntimeClass
apiVersion: node.k8s.io/v1
metadata:
  name: kata
overhead:
  podFixed:
    memory: "500Mi"
    cpu: "500m"</pre></li><li class="listitem"><p class="simpara">
							Apply the changes by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f runtimeclass.yaml</pre></li></ol></div></section><section class="section" id="configuring-workload-objects_metal-cli"><div class="titlepage"><div><div><h3 class="title">2.3.5. Configuring workload objects</h3></div></div></div><p>
					You must configure OpenShift sandboxed containers workload objects by setting <code class="literal">kata</code> as the runtime class for the following pod-templated objects:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">Pod</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicaSet</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicationController</code> objects
						</li><li class="listitem">
							<code class="literal">StatefulSet</code> objects
						</li><li class="listitem">
							<code class="literal">Deployment</code> objects
						</li><li class="listitem">
							<code class="literal">DeploymentConfig</code> objects
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created the <code class="literal">KataConfig</code> custom resource (CR).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add <code class="literal">spec.runtimeClassName: kata</code> to the manifest of each pod-templated workload object as in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata
# ...</pre><p class="simpara">
							OpenShift Container Platform creates the workload object and begins scheduling it.
						</p></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Inspect the <code class="literal">spec.runtimeClassName</code> field of a pod-templated object. If the value is <code class="literal">kata</code>, then the workload is running on OpenShift sandboxed containers, using peer pods.
						</li></ul></div></section></section></section><section class="chapter" id="deploying-aws"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Deploying OpenShift sandboxed containers on AWS</h1></div></div></div><p>
			You can deploy OpenShift sandboxed containers on AWS Cloud Computing Services by using the OpenShift Container Platform web console or the command line interface (CLI).
		</p><p>
			OpenShift sandboxed containers deploys peer pods. The peer pod design circumvents the need for nested virtualization. For more information, see <a class="link" href="#peer-pods">peer pods</a>.
		</p><div class="itemizedlist"><p class="title"><strong>Cluster requirements</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
					You have installed Red Hat OpenShift Container Platform 4.14 or later on the cluster where you are installing the OpenShift sandboxed containers Operator.
				</li><li class="listitem">
					Your cluster has at least one worker node.
				</li></ul></div><section class="section" id="peer-pod-resource-requirements_aws"><div class="titlepage"><div><div><h2 class="title">3.1. Peer pod resource requirements</h2></div></div></div><p>
				You must ensure that your cluster has sufficient resources.
			</p><p>
				Peer pod virtual machines (VMs) require resources in two locations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The worker node. The worker node stores metadata, Kata shim resources (<code class="literal">containerd-shim-kata-v2</code>), remote-hypervisor resources (<code class="literal">cloud-api-adaptor</code>), and the tunnel setup between the worker nodes and the peer pod VM.
					</li><li class="listitem">
						The cloud instance. This is the actual peer pod VM running in the cloud.
					</li></ul></div><p>
				The CPU and memory resources used in the Kubernetes worker node are handled by the <a class="link" href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</a> included in the RuntimeClass (<code class="literal">kata-remote</code>) definition used for creating peer pods.
			</p><p>
				The total number of peer pod VMs running in the cloud is defined as Kubernetes Node extended resources. This limit is per node and is set by the <code class="literal">limit</code> attribute in the <code class="literal">peerpodConfig</code> custom resource (CR).
			</p><p>
				The <code class="literal">peerpodConfig</code> CR, named <code class="literal">peerpodconfig-openshift</code>, is created when you create the <code class="literal">kataConfig</code> CR and enable peer pods, and is located in the <code class="literal">openshift-sandboxed-containers-operator</code> namespace.
			</p><p>
				The following <code class="literal">peerpodConfig</code> CR example displays the default <code class="literal">spec</code> values:
			</p><pre class="programlisting language-yaml">apiVersion: confidentialcontainers.org/v1alpha1
kind: PeerPodConfig
metadata:
  name: peerpodconfig-openshift
  namespace: openshift-sandboxed-containers-operator
spec:
  cloudSecretName: peer-pods-secret
  configMapName: peer-pods-cm
  limit: "10" <span id="CO4-1"/><span class="callout">1</span>
  nodeSelector:
    node-role.kubernetes.io/kata-oc: ""</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						The default limit is 10 VMs per node.
					</div></dd></dl></div><p>
				The extended resource is named <code class="literal">kata.peerpods.io/vm</code>, and enables the Kubernetes scheduler to handle capacity tracking and accounting.
			</p><p>
				You can edit the limit per node based on the requirements for your environment after you install the OpenShift sandboxed containers Operator.
			</p><p>
				A <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">mutating webhook</a> adds the extended resource <code class="literal">kata.peerpods.io/vm</code> to the pod specification. It also removes any resource-specific entries from the pod specification, if present. This enables the Kubernetes scheduler to account for these extended resources, ensuring the peer pod is only scheduled when resources are available.
			</p><p>
				The mutating webhook modifies a Kubernetes pod as follows:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The mutating webhook checks the pod for the expected <code class="literal">RuntimeClassName</code> value, specified in the <code class="literal">TARGET_RUNTIME_CLASS</code> environment variable. If the value in the pod specification does not match the value in the <code class="literal">TARGET_RUNTIME_CLASS</code>, the webhook exits without modifying the pod.
					</li><li class="listitem"><p class="simpara">
						If the <code class="literal">RuntimeClassName</code> values match, the webhook makes the following changes to the pod spec:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								The webhook removes every resource specification from the <code class="literal">resources</code> field of all containers and init containers in the pod.
							</li><li class="listitem">
								The webhook adds the extended resource (<code class="literal">kata.peerpods.io/vm</code>) to the spec by modifying the resources field of the first container in the pod. The extended resource <code class="literal">kata.peerpods.io/vm</code> is used by the Kubernetes scheduler for accounting purposes.
							</li></ol></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The mutating webhook excludes specific system namespaces in OpenShift Container Platform from mutation. If a peer pod is created in those system namespaces, then resource accounting using Kubernetes extended resources does not work unless the pod spec includes the extended resource.
				</p><p>
					As a best practice, define a cluster-wide policy to only allow peer pod creation in specific namespaces.
				</p></div></div></section><section class="section" id="deploying-osc-web_aws-web"><div class="titlepage"><div><div><h2 class="title">3.2. Deploying OpenShift sandboxed containers by using the web console</h2></div></div></div><p>
				You can deploy OpenShift sandboxed containers on AWS by using the OpenShift Container Platform web console to perform the following tasks:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Install the OpenShift sandboxed containers Operator.
					</li><li class="listitem">
						Enable ports 15150 and 9000 to allow internal communication with peer pods.
					</li><li class="listitem">
						Create the peer pods secret.
					</li><li class="listitem">
						Create the peer pods config map.
					</li><li class="listitem">
						Create the <code class="literal">KataConfig</code> custom resource.
					</li><li class="listitem">
						Configure the OpenShift sandboxed containers workload objects.
					</li></ol></div><section class="section" id="installing-operator-web-console_aws-web"><div class="titlepage"><div><div><h3 class="title">3.2.1. Installing the OpenShift sandboxed containers Operator</h3></div></div></div><p>
					You can install the OpenShift sandboxed containers Operator by using the OpenShift Container Platform web console.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Filter by keyword</strong></span> field, type <code class="literal">OpenShift sandboxed containers</code>.
						</li><li class="listitem">
							Select the <span class="strong strong"><strong>OpenShift sandboxed containers Operator</strong></span> tile and click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Install Operator</strong></span> page, select <span class="strong strong"><strong>stable</strong></span> from the list of available <span class="strong strong"><strong>Update Channel</strong></span> options.
						</li><li class="listitem"><p class="simpara">
							Verify that <span class="strong strong"><strong>Operator recommended Namespace</strong></span> is selected for <span class="strong strong"><strong>Installed Namespace</strong></span>. This installs the Operator in the mandatory <code class="literal">openshift-sandboxed-containers-operator</code> namespace. If this namespace does not yet exist, it is automatically created.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Attempting to install the OpenShift sandboxed containers Operator in a namespace other than <code class="literal">openshift-sandboxed-containers-operator</code> causes the installation to fail.
							</p></div></div></li><li class="listitem">
							Verify that <span class="strong strong"><strong>Automatic</strong></span> is selected for <span class="strong strong"><strong>Approval Strategy</strong></span>. <span class="strong strong"><strong>Automatic</strong></span> is the default value, and enables automatic updates to OpenShift sandboxed containers when a new z-stream release is available.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem">
							Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> to verify that the Operator is installed.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</a>.
						</li><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</a> for disconnected environments.
						</li></ul></div></section><section class="section" id="aws-enabling-ports_aws-web"><div class="titlepage"><div><div><h3 class="title">3.2.2. Enabling ports for AWS</h3></div></div></div><p>
					You must enable ports 15150 and 9000 to allow internal communication with peer pods running on AWS.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift sandboxed containers Operator.
						</li><li class="listitem">
							You have installed the AWS command line tool.
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Log in to your OpenShift Container Platform cluster and retrieve the instance ID:
						</p><pre class="programlisting language-terminal">$ INSTANCE_ID=$(oc get nodes -l 'node-role.kubernetes.io/worker' \
  -o jsonpath='{.items[0].spec.providerID}' | sed 's#[^ ]*/##g')</pre></li><li class="listitem"><p class="simpara">
							Retrieve the AWS region:
						</p><pre class="programlisting language-terminal">$ AWS_REGION=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.aws.region}')</pre></li><li class="listitem"><p class="simpara">
							Retrieve the security group IDs and store them in an array:
						</p><pre class="programlisting language-terminal">$ AWS_SG_IDS=($(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} \
  --query 'Reservations[*].Instances[*].SecurityGroups[*].GroupId' \
  --output text --region $AWS_REGION))</pre></li><li class="listitem"><p class="simpara">
							For each security group ID, authorize the peer pods shim to access kata-agent communication, and set up the peer pods tunnel:
						</p><pre class="programlisting language-terminal">$ for AWS_SG_ID in "${AWS_SG_IDS[@]}"; do \
  aws ec2 authorize-security-group-ingress --group-id $AWS_SG_ID --protocol tcp --port 15150 --source-group $AWS_SG_ID --region $AWS_REGION \
  aws ec2 authorize-security-group-ingress --group-id $AWS_SG_ID --protocol tcp --port 9000 --source-group $AWS_SG_ID --region $AWS_REGION \
done</pre></li></ol></div><p>
					The ports are now enabled.
				</p></section><section class="section" id="creating-peer-pods-secret_aws-web"><div class="titlepage"><div><div><h3 class="title">3.2.3. Creating the peer pods secret</h3></div></div></div><p>
					You must create the peer pods secret for OpenShift sandboxed containers.
				</p><p>
					The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.
				</p><p>
					By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							You have the following values generated by using the AWS console:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">AWS_ACCESS_KEY_ID</code>
								</li><li class="listitem">
									<code class="literal">AWS_SECRET_ACCESS_KEY</code>
								</li></ul></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
						</li><li class="listitem">
							Click the OpenShift sandboxed containers Operator tile.
						</li><li class="listitem">
							Click the Import icon (<span class="strong strong"><strong>+</strong></span>) on the top right corner.
						</li><li class="listitem"><p class="simpara">
							In the <span class="strong strong"><strong>Import YAML</strong></span> window, paste the following YAML manifest:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  AWS_ACCESS_KEY_ID: "&lt;aws_access_key&gt;" <span id="CO5-1"/><span class="callout">1</span>
  AWS_SECRET_ACCESS_KEY: "&lt;aws_secret_access_key&gt;" <span id="CO5-2"/><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_ACCESS_KEY_ID</code> value.
								</div></dd><dt><a href="#CO5-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_SECRET_ACCESS_KEY</code> value.
								</div></dd></dl></div></li><li class="listitem">
							Click <span class="strong strong"><strong>Save</strong></span> to apply the changes.
						</li><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span> to verify the peer pods secret.
						</li></ol></div></section><section class="section" id="creating-peer-pods-config-map_aws-web"><div class="titlepage"><div><div><h3 class="title">3.2.4. Creating the peer pods config map</h3></div></div></div><p>
					You must create the peer pods config map for OpenShift sandboxed containers.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have your Amazon Machine Image (AMI) ID if you are not using the default AMI ID based on your cluster credentials.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the following values from your AWS instance:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve and record the instance ID:
								</p><pre class="programlisting language-terminal">$ INSTANCE_ID=$(oc get nodes -l 'node-role.kubernetes.io/worker' -o jsonpath='{.items[0].spec.providerID}' | sed 's#[^ ]*/##g')</pre><p class="simpara">
									This is used to retrieve other values for the secret object.
								</p></li><li class="listitem"><p class="simpara">
									Retrieve and record the AWS region:
								</p><pre class="programlisting language-terminal">$ AWS_REGION=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.aws.region}') &amp;&amp; echo "AWS_REGION: \"$AWS_REGION\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the AWS subnet ID:
								</p><pre class="programlisting language-terminal">$ AWS_SUBNET_ID=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].SubnetId' --region ${AWS_REGION} --output text) &amp;&amp; echo "AWS_SUBNET_ID: \"$AWS_SUBNET_ID\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the AWS VPC ID:
								</p><pre class="programlisting language-terminal">$ AWS_VPC_ID=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].VpcId' --region ${AWS_REGION} --output text) &amp;&amp; echo "AWS_VPC_ID: \"$AWS_VPC_ID\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the AWS security group IDs:
								</p><pre class="programlisting language-terminal">$ AWS_SG_IDS=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].SecurityGroups[*].GroupId' --region  $AWS_REGION --output json | jq -r '.[][][]' | paste -sd ",") &amp;&amp; echo "AWS_SG_IDS: \"$AWS_SG_IDS\""</pre></li></ol></div></li><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
						</li><li class="listitem">
							Select the OpenShift sandboxed containers Operator from the list of operators.
						</li><li class="listitem">
							Click the Import icon (<span class="strong strong"><strong>+</strong></span>) in the top right corner.
						</li><li class="listitem"><p class="simpara">
							In the <span class="strong strong"><strong>Import YAML</strong></span> window, paste the following YAML manifest:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "aws"
  VXLAN_PORT: "9000"
  PODVM_INSTANCE_TYPE: "t3.medium" <span id="CO6-1"/><span class="callout">1</span>
  PODVM_INSTANCE_TYPES: "t2.small,t2.medium,t3.large" <span id="CO6-2"/><span class="callout">2</span>
  PROXY_TIMEOUT: "5m"
  PODVM_AMI_ID: "&lt;podvm_ami_id&gt;" <span id="CO6-3"/><span class="callout">3</span>
  AWS_REGION: "&lt;aws_region&gt;" <span id="CO6-4"/><span class="callout">4</span>
  AWS_SUBNET_ID: "&lt;aws_subnet_id&gt;" <span id="CO6-5"/><span class="callout">5</span>
  AWS_VPC_ID: "&lt;aws_vpc_id&gt;" <span id="CO6-6"/><span class="callout">6</span>
  AWS_SG_IDS: "&lt;aws_sg_ids&gt;" <span id="CO6-7"/><span class="callout">7</span>
  DISABLECVM: "true"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the default instance type that is used when a type is not defined in the workload.
								</div></dd><dt><a href="#CO6-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Lists all of the instance types you can specify when creating the pod. This allows you to define smaller instance types for workloads that need less memory and fewer CPUs or larger instance types for larger workloads.
								</div></dd><dt><a href="#CO6-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Optional: By default, this value is populated when you run the <code class="literal">KataConfig</code> CR, using an AMI ID based on your cluster credentials. If you create your own AMI, specify the correct AMI ID.
								</div></dd><dt><a href="#CO6-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_REGION</code> value you retrieved.
								</div></dd><dt><a href="#CO6-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_SUBNET_ID</code> value you retrieved.
								</div></dd><dt><a href="#CO6-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_VPC_ID</code> value you retrieved.
								</div></dd><dt><a href="#CO6-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_SG_IDS</code> value you retrieved.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Save</strong></span> to apply the changes.
						</p><pre class="programlisting language-terminal">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</pre></li><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>ConfigMaps</strong></span> to view the new config map.
						</li></ol></div></section><section class="section" id="creating-kataconfig-cr-web_aws-web"><div class="titlepage"><div><div><h3 class="title">3.2.5. Creating the KataConfig custom resource</h3></div></div></div><p>
					You must create the <code class="literal">KataConfig</code> custom resource (CR) to install <code class="literal">kata-remote</code> as a <code class="literal">RuntimeClass</code> on your worker nodes.
				</p><p>
					The <code class="literal">kata-remote</code> runtime class is installed on all worker nodes by default. If you want to install <code class="literal">kata-remote</code> on specific nodes, you can add labels to those nodes and then define the label in the <code class="literal">KataConfig</code> CR.
				</p><p>
					OpenShift sandboxed containers installs <code class="literal">kata-remote</code> as a <span class="emphasis"><em>secondary, optional</em></span> runtime on the cluster and not as the primary runtime.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Creating the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. The following factors might increase the reboot time:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								A larger OpenShift Container Platform deployment with a greater number of worker nodes.
							</li><li class="listitem">
								Activation of the BIOS and Diagnostics utility.
							</li><li class="listitem">
								Deployment on a hard disk drive rather than an SSD.
							</li><li class="listitem">
								Deployment on physical nodes such as bare metal, rather than on virtual nodes.
							</li><li class="listitem">
								A slow CPU and network.
							</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Optional: You have installed the Node Feature Discovery Operator if you want to enable node eligibility checks.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
						</li><li class="listitem">
							Select the OpenShift sandboxed containers Operator.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>KataConfig</strong></span> tab, click <span class="strong strong"><strong>Create KataConfig</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Enter the following details:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>Name</strong></span>: Optional: The default name is <code class="literal">example-kataconfig</code>.
								</li><li class="listitem">
									<span class="strong strong"><strong>Labels</strong></span>: Optional: Enter any relevant, identifying attributes to the <code class="literal">KataConfig</code> resource. Each label represents a key-value pair.
								</li><li class="listitem">
									<span class="strong strong"><strong>enablePeerPods</strong></span>: Select for public cloud, IBM Z®, and IBM® LinuxONE deployments.
								</li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>kataConfigPoolSelector</strong></span>. Optional: To install <code class="literal">kata-remote</code> on selected nodes, add a match expression for the labels on the selected nodes:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Expand the <span class="strong strong"><strong>kataConfigPoolSelector</strong></span> area.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>kataConfigPoolSelector</strong></span> area, expand <span class="strong strong"><strong>matchExpressions</strong></span>. This is a list of label selector requirements.
										</li><li class="listitem">
											Click <span class="strong strong"><strong>Add matchExpressions</strong></span>.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>Key</strong></span> field, enter the label key the selector applies to.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>Operator</strong></span> field, enter the key’s relationship to the label values. Valid operators are <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, and <code class="literal">DoesNotExist</code>.
										</li><li class="listitem">
											Expand the <span class="strong strong"><strong>Values</strong></span> area and then click <span class="strong strong"><strong>Add value</strong></span>.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>Value</strong></span> field, enter <code class="literal">true</code> or <code class="literal">false</code> for <span class="strong strong"><strong>key</strong></span> label value.
										</li></ol></div></li><li class="listitem">
									<span class="strong strong"><strong>logLevel</strong></span>: Define the level of log data retrieved for nodes with the <code class="literal">kata-remote</code> runtime class.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Create</strong></span>. The <code class="literal">KataConfig</code> CR is created and installs the <code class="literal">kata-remote</code> runtime class on the worker nodes.
						</p><p class="simpara">
							Wait for the <code class="literal">kata-remote</code> installation to complete and the worker nodes to reboot before verifying the installation.
						</p></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the <span class="strong strong"><strong>KataConfig</strong></span> tab, click the <code class="literal">KataConfig</code> CR to view its details.
						</li><li class="listitem"><p class="simpara">
							Click the <span class="strong strong"><strong>YAML</strong></span> tab to view the <code class="literal">status</code> stanza.
						</p><p class="simpara">
							The <code class="literal">status</code> stanza contains the <code class="literal">conditions</code> and <code class="literal">kataNodes</code> keys. The value of <code class="literal">status.kataNodes</code> is an array of nodes, each of which lists nodes in a particular state of <code class="literal">kata-remote</code> installation. A message appears each time there is an update.
						</p></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Reload</strong></span> to refresh the YAML.
						</p><p class="simpara">
							When all workers in the <code class="literal">status.kataNodes</code> array display the values <code class="literal">installed</code> and <code class="literal">conditions.InProgress: False</code> with no specified reason, the <code class="literal">kata-remote</code> is installed on the cluster.
						</p></li></ol></div><h5 id="additional_resources_2">Additional resources</h5><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="#kataconfig-status-messages" title="Appendix A. KataConfig status messages">KataConfig status messages</a>
						</li></ul></div><h5 id="verifying-pod-vm-image-creation_aws-web">Verifying the pod VM image</h5><p>
					After <code class="literal">kata-remote</code> is installed on your cluster, the OpenShift sandboxed containers Operator creates a pod VM image, which is used to create peer pods. This process can take a long time because the image is created on the cloud instance. You can verify that the pod VM image was created successfully by checking the config map that you created for the cloud provider.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>ConfigMaps</strong></span>.
						</li><li class="listitem">
							Click the provider config map to view its details.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>YAML</strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							Check the <code class="literal">status</code> stanza of the YAML file.
						</p><p class="simpara">
							If the <code class="literal">PODVM_AMI_ID</code> parameter is populated, the pod VM image was created successfully.
						</p></li></ol></div><div class="orderedlist"><p class="title"><strong>Troubleshooting</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Retrieve the events log by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get events -n openshift-sandboxed-containers-operator --field-selector involvedObject.name=osc-podvm-image-creation</pre></li><li class="listitem"><p class="simpara">
							Retrieve the job log by running the following command:
						</p><pre class="programlisting language-terminal">$ oc logs -n openshift-sandboxed-containers-operator jobs/osc-podvm-image-creation</pre></li></ol></div><p>
					If you cannot resolve the issue, submit a Red Hat Support case and attach the output of both logs.
				</p></section><section class="section" id="configuring-workload-objects_aws-web"><div class="titlepage"><div><div><h3 class="title">3.2.6. Configuring workload objects</h3></div></div></div><p>
					You must configure OpenShift sandboxed containers workload objects by setting <code class="literal">kata-remote</code> as the runtime class for the following pod-templated objects:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">Pod</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicaSet</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicationController</code> objects
						</li><li class="listitem">
							<code class="literal">StatefulSet</code> objects
						</li><li class="listitem">
							<code class="literal">Deployment</code> objects
						</li><li class="listitem">
							<code class="literal">DeploymentConfig</code> objects
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.
					</p></div></div><p>
					You can define whether the workload should be deployed using the default instance type, which you defined in the config map, by adding an annotation to the YAML file.
				</p><p>
					If you do not want to define the instance type manually, you can add an annotation to use an automatic instance type, based on the memory available.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created the <code class="literal">KataConfig</code> custom resource (CR).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Workloads</strong></span> → workload type, for example, <span class="strong strong"><strong>Pods</strong></span>.
						</li><li class="listitem">
							On the workload type page, click an object to view its details.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>YAML</strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							Add <code class="literal">spec.runtimeClassName: kata-remote</code> to the manifest of each pod-templated workload object as in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata-remote
# ...</pre></li><li class="listitem"><p class="simpara">
							Add an annotation to the pod-templated object to use a manually defined instance type or an automatic instance type:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To use a manually defined instance type, add the following annotation:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;object&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.machine_type: "t3.medium" <span id="CO7-1"/><span class="callout">1</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the instance type that you defined in the config map.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To use an automatic instance type, add the following annotations:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;Pod&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.default_vcpus: &lt;vcpus&gt;
    io.katacontainers.config.hypervisor.default_memory: &lt;memory&gt;
# ...</pre><p class="simpara">
									Define the amount of memory available for the workload to use. The workload will run on an automatic instance type based on the amount of memory available.
								</p></li></ul></div></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Save</strong></span> to apply the changes.
						</p><p class="simpara">
							OpenShift Container Platform creates the workload object and begins scheduling it.
						</p></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Inspect the <code class="literal">spec.runtimeClassName</code> field of a pod-templated object. If the value is <code class="literal">kata-remote</code>, then the workload is running on OpenShift sandboxed containers, using peer pods.
						</li></ul></div></section></section><section class="section" id="deploying-osc-cli_aws-cli"><div class="titlepage"><div><div><h2 class="title">3.3. Deploying OpenShift sandboxed containers by using the command line</h2></div></div></div><p>
				You can deploy OpenShift sandboxed containers on AWS by using the command line interface (CLI) to perform the following tasks:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Install the OpenShift sandboxed containers Operator.
					</li><li class="listitem">
						Optional: Change the number of virtual machines running on each worker node.
					</li><li class="listitem">
						Enable ports 15150 and 9000 to allow internal communication with peer pods.
					</li><li class="listitem">
						Create the peer pods secret.
					</li><li class="listitem">
						Create the peer pods config map.
					</li><li class="listitem">
						Create the <code class="literal">KataConfig</code> custom resource.
					</li><li class="listitem">
						Configure the OpenShift sandboxed containers workload objects.
					</li></ol></div><section class="section" id="installing-operator-cli_aws-cli"><div class="titlepage"><div><div><h3 class="title">3.3.1. Installing the OpenShift sandboxed containers Operator</h3></div></div></div><p>
					You can install the OpenShift sandboxed containers Operator by using the CLI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-namespace.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sandboxed-containers-operator</pre></li><li class="listitem"><p class="simpara">
							Create the namespace by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-namespace.yaml</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-operatorgroup.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sandboxed-containers-operator-group
  namespace: openshift-sandboxed-containers-operator
spec:
  targetNamespaces:
  - openshift-sandboxed-containers-operator</pre></li><li class="listitem"><p class="simpara">
							Create the operator group by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-operatorgroup.yaml</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-subscription.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sandboxed-containers-operator
  namespace: openshift-sandboxed-containers-operator
spec:
  channel: stable
  installPlanApproval: Automatic
  name: sandboxed-containers-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: sandboxed-containers-operator.v1.8.0</pre></li><li class="listitem"><p class="simpara">
							Create the subscription by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-subscription.yaml</pre></li><li class="listitem"><p class="simpara">
							Verify that the Operator is correctly installed by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get csv -n openshift-sandboxed-containers-operator</pre><p class="simpara">
							This command can take several minutes to complete.
						</p></li><li class="listitem"><p class="simpara">
							Watch the process by running the following command:
						</p><pre class="programlisting language-terminal">$ watch oc get csv -n openshift-sandboxed-containers-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                             DISPLAY                                  VERSION             REPLACES                   PHASE
openshift-sandboxed-containers   openshift-sandboxed-containers-operator  1.8.0    1.7.0        Succeeded</pre>
							</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</a>.
						</li><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</a> for disconnected environments.
						</li></ul></div></section><section class="section" id="modifying-peer-pod-vm-limit_aws-cli"><div class="titlepage"><div><div><h3 class="title">3.3.2. Modifying the number of peer pod VMs per node</h3></div></div></div><p>
					You can change the limit of peer pod virtual machines (VMs) per node by editing the <code class="literal">peerpodConfig</code> custom resource (CR).
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the current limit by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
-o jsonpath='{.spec.limit}{"\n"}'</pre></li><li class="listitem"><p class="simpara">
							Modify the <code class="literal">limit</code> attribute of the <code class="literal">peerpodConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc patch peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
--type merge --patch '{"spec":{"limit":"&lt;value&gt;"}}' <span id="CO8-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace &lt;value&gt; with the limit you want to define.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="aws-enabling-ports_aws-cli"><div class="titlepage"><div><div><h3 class="title">3.3.3. Enabling ports for AWS</h3></div></div></div><p>
					You must enable ports 15150 and 9000 to allow internal communication with peer pods running on AWS.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift sandboxed containers Operator.
						</li><li class="listitem">
							You have installed the AWS command line tool.
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Log in to your OpenShift Container Platform cluster and retrieve the instance ID:
						</p><pre class="programlisting language-terminal">$ INSTANCE_ID=$(oc get nodes -l 'node-role.kubernetes.io/worker' \
  -o jsonpath='{.items[0].spec.providerID}' | sed 's#[^ ]*/##g')</pre></li><li class="listitem"><p class="simpara">
							Retrieve the AWS region:
						</p><pre class="programlisting language-terminal">$ AWS_REGION=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.aws.region}')</pre></li><li class="listitem"><p class="simpara">
							Retrieve the security group IDs and store them in an array:
						</p><pre class="programlisting language-terminal">$ AWS_SG_IDS=($(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} \
  --query 'Reservations[*].Instances[*].SecurityGroups[*].GroupId' \
  --output text --region $AWS_REGION))</pre></li><li class="listitem"><p class="simpara">
							For each security group ID, authorize the peer pods shim to access kata-agent communication, and set up the peer pods tunnel:
						</p><pre class="programlisting language-terminal">$ for AWS_SG_ID in "${AWS_SG_IDS[@]}"; do \
  aws ec2 authorize-security-group-ingress --group-id $AWS_SG_ID --protocol tcp --port 15150 --source-group $AWS_SG_ID --region $AWS_REGION \
  aws ec2 authorize-security-group-ingress --group-id $AWS_SG_ID --protocol tcp --port 9000 --source-group $AWS_SG_ID --region $AWS_REGION \
done</pre></li></ol></div><p>
					The ports are now enabled.
				</p></section><section class="section" id="creating-peer-pods-secret_aws-cli"><div class="titlepage"><div><div><h3 class="title">3.3.4. Creating the peer pods secret</h3></div></div></div><p>
					You must create the peer pods secret for OpenShift sandboxed containers.
				</p><p>
					The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.
				</p><p>
					By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							You have the following values generated by using the AWS console:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<code class="literal">AWS_ACCESS_KEY_ID</code>
								</li><li class="listitem">
									<code class="literal">AWS_SECRET_ACCESS_KEY</code>
								</li></ul></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">peer-pods-secret.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  AWS_ACCESS_KEY_ID: "&lt;aws_access_key&gt;" <span id="CO9-1"/><span class="callout">1</span>
  AWS_SECRET_ACCESS_KEY: "&lt;aws_secret_access_key&gt;" <span id="CO9-2"/><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_ACCESS_KEY_ID</code> value.
								</div></dd><dt><a href="#CO9-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_SECRET_ACCESS_KEY</code> value.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the secret by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f peer-pods-secret.yaml</pre></li><li class="listitem"><p class="simpara">
							Optional: To update an existing peer pods config map, restart the <code class="literal">peerpodconfig-ctrl-caa-daemon</code> daemon set by running the following command:
						</p><pre class="programlisting language-terminal">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</pre></li></ol></div></section><section class="section" id="creating-peer-pods-config-map_aws-cli"><div class="titlepage"><div><div><h3 class="title">3.3.5. Creating the peer pods config map</h3></div></div></div><p>
					You must create the peer pods config map for OpenShift sandboxed containers.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have your Amazon Machine Image (AMI) ID if you are not using the default AMI ID based on your cluster credentials.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the following values from your AWS instance:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve and record the instance ID:
								</p><pre class="programlisting language-terminal">$ INSTANCE_ID=$(oc get nodes -l 'node-role.kubernetes.io/worker' -o jsonpath='{.items[0].spec.providerID}' | sed 's#[^ ]*/##g')</pre><p class="simpara">
									This is used to retrieve other values for the secret object.
								</p></li><li class="listitem"><p class="simpara">
									Retrieve and record the AWS region:
								</p><pre class="programlisting language-terminal">$ AWS_REGION=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.aws.region}') &amp;&amp; echo "AWS_REGION: \"$AWS_REGION\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the AWS subnet ID:
								</p><pre class="programlisting language-terminal">$ AWS_SUBNET_ID=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].SubnetId' --region ${AWS_REGION} --output text) &amp;&amp; echo "AWS_SUBNET_ID: \"$AWS_SUBNET_ID\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the AWS VPC ID:
								</p><pre class="programlisting language-terminal">$ AWS_VPC_ID=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].VpcId' --region ${AWS_REGION} --output text) &amp;&amp; echo "AWS_VPC_ID: \"$AWS_VPC_ID\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the AWS security group IDs:
								</p><pre class="programlisting language-terminal">$ AWS_SG_IDS=$(aws ec2 describe-instances --instance-ids ${INSTANCE_ID} --query 'Reservations[*].Instances[*].SecurityGroups[*].GroupId' --region  $AWS_REGION --output json | jq -r '.[][][]' | paste -sd ",") &amp;&amp; echo "AWS_SG_IDS: \"$AWS_SG_IDS\""</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">peer-pods-cm.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "aws"
  VXLAN_PORT: "9000"
  PODVM_INSTANCE_TYPE: "t3.medium" <span id="CO10-1"/><span class="callout">1</span>
  PODVM_INSTANCE_TYPES: "t2.small,t2.medium,t3.large" <span id="CO10-2"/><span class="callout">2</span>
  PROXY_TIMEOUT: "5m"
  PODVM_AMI_ID: "&lt;podvm_ami_id&gt;" <span id="CO10-3"/><span class="callout">3</span>
  AWS_REGION: "&lt;aws_region&gt;" <span id="CO10-4"/><span class="callout">4</span>
  AWS_SUBNET_ID: "&lt;aws_subnet_id&gt;" <span id="CO10-5"/><span class="callout">5</span>
  AWS_VPC_ID: "&lt;aws_vpc_id&gt;" <span id="CO10-6"/><span class="callout">6</span>
  AWS_SG_IDS: "&lt;aws_sg_ids&gt;" <span id="CO10-7"/><span class="callout">7</span>
  DISABLECVM: "true"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Defines the default instance type that is used when a type is not defined in the workload.
								</div></dd><dt><a href="#CO10-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Lists all of the instance types you can specify when creating the pod. This allows you to define smaller instance types for workloads that need less memory and fewer CPUs or larger instance types for larger workloads.
								</div></dd><dt><a href="#CO10-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Optional: By default, this value is populated when you run the <code class="literal">KataConfig</code> CR, using an AMI ID based on your cluster credentials. If you create your own AMI, specify the correct AMI ID.
								</div></dd><dt><a href="#CO10-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_REGION</code> value you retrieved.
								</div></dd><dt><a href="#CO10-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_SUBNET_ID</code> value you retrieved.
								</div></dd><dt><a href="#CO10-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_VPC_ID</code> value you retrieved.
								</div></dd><dt><a href="#CO10-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AWS_SG_IDS</code> value you retrieved.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f peer-pods-cm.yaml</pre></li><li class="listitem"><p class="simpara">
							Optional: To update an existing peer pods config map, restart the <code class="literal">peerpodconfig-ctrl-caa-daemon</code> daemon set by running the following command:
						</p><pre class="programlisting language-terminal">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</pre></li></ol></div></section><section class="section" id="creating-kataconfig-cr-cli_aws-cli"><div class="titlepage"><div><div><h3 class="title">3.3.6. Creating the KataConfig custom resource</h3></div></div></div><p>
					You must create the <code class="literal">KataConfig</code> custom resource (CR) to install <code class="literal">kata-remote</code> as a runtime class on your worker nodes.
				</p><p>
					Creating the <code class="literal">KataConfig</code> CR triggers the OpenShift sandboxed containers Operator to do the following: * Create a <code class="literal">RuntimeClass</code> CR named <code class="literal">kata-remote</code> with a default configuration. This enables users to configure workloads to use <code class="literal">kata-remote</code> as the runtime by referencing the CR in the <code class="literal">RuntimeClassName</code> field. This CR also specifies the resource overhead for the runtime.
				</p><p>
					OpenShift sandboxed containers installs <code class="literal">kata-remote</code> as a <span class="emphasis"><em>secondary, optional</em></span> runtime on the cluster and not as the primary runtime.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Creating the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								A larger OpenShift Container Platform deployment with a greater number of worker nodes.
							</li><li class="listitem">
								Activation of the BIOS and Diagnostics utility.
							</li><li class="listitem">
								Deployment on a hard disk drive rather than an SSD.
							</li><li class="listitem">
								Deployment on physical nodes such as bare metal, rather than on virtual nodes.
							</li><li class="listitem">
								A slow CPU and network.
							</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">example-kataconfig.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  enablePeerPods: true
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <span id="CO11-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional: If you have applied node labels to install <code class="literal">kata-remote</code> on specific nodes, specify the key and value, for example, <code class="literal">osc: 'true'</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">KataConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f example-kataconfig.yaml</pre><p class="simpara">
							The new <code class="literal">KataConfig</code> CR is created and installs <code class="literal">kata-remote</code> as a runtime class on the worker nodes.
						</p><p class="simpara">
							Wait for the <code class="literal">kata-remote</code> installation to complete and the worker nodes to reboot before verifying the installation.
						</p></li><li class="listitem"><p class="simpara">
							Monitor the installation progress by running the following command:
						</p><pre class="programlisting language-terminal">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</pre><p class="simpara">
							When the status of all workers under <code class="literal">kataNodes</code> is <code class="literal">installed</code> and the condition <code class="literal">InProgress</code> is <code class="literal">False</code> without specifying a reason, the <code class="literal">kata-remote</code> is installed on the cluster.
						</p></li><li class="listitem"><p class="simpara">
							Verify the daemon set by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon</pre></li><li class="listitem"><p class="simpara">
							Verify the runtime classes by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get runtimeclass</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME             HANDLER          AGE
kata             kata             152m
kata-remote      kata-remote      152m</pre>
							</p></div></li></ol></div><h5 id="verifying-pod-vm-image-creation_aws-cli">Verifying the pod VM image</h5><p>
					After <code class="literal">kata-remote</code> is installed on your cluster, the OpenShift sandboxed containers Operator creates a pod VM image, which is used to create peer pods. This process can take a long time because the image is created on the cloud instance. You can verify that the pod VM image was created successfully by checking the config map that you created for the cloud provider.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the config map you created for the peer pods:
						</p><pre class="programlisting language-terminal">$ oc get configmap peer-pods-cm -n openshift-sandboxed-containers-operator -o yaml</pre></li><li class="listitem"><p class="simpara">
							Check the <code class="literal">status</code> stanza of the YAML file.
						</p><p class="simpara">
							If the <code class="literal">PODVM_AMI_ID</code> parameter is populated, the pod VM image was created successfully.
						</p></li></ol></div><div class="orderedlist"><p class="title"><strong>Troubleshooting</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Retrieve the events log by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get events -n openshift-sandboxed-containers-operator --field-selector involvedObject.name=osc-podvm-image-creation</pre></li><li class="listitem"><p class="simpara">
							Retrieve the job log by running the following command:
						</p><pre class="programlisting language-terminal">$ oc logs -n openshift-sandboxed-containers-operator jobs/osc-podvm-image-creation</pre></li></ol></div><p>
					If you cannot resolve the issue, submit a Red Hat Support case and attach the output of both logs.
				</p></section><section class="section" id="configuring-workload-objects_aws-cli"><div class="titlepage"><div><div><h3 class="title">3.3.7. Configuring workload objects</h3></div></div></div><p>
					You must configure OpenShift sandboxed containers workload objects by setting <code class="literal">kata-remote</code> as the runtime class for the following pod-templated objects:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">Pod</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicaSet</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicationController</code> objects
						</li><li class="listitem">
							<code class="literal">StatefulSet</code> objects
						</li><li class="listitem">
							<code class="literal">Deployment</code> objects
						</li><li class="listitem">
							<code class="literal">DeploymentConfig</code> objects
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.
					</p></div></div><p>
					You can define whether the workload should be deployed using the default instance type, which you defined in the config map, by adding an annotation to the YAML file.
				</p><p>
					If you do not want to define the instance type manually, you can add an annotation to use an automatic instance type, based on the memory available.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created the <code class="literal">KataConfig</code> custom resource (CR).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add <code class="literal">spec.runtimeClassName: kata-remote</code> to the manifest of each pod-templated workload object as in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata-remote
# ...</pre></li><li class="listitem"><p class="simpara">
							Add an annotation to the pod-templated object to use a manually defined instance type or an automatic instance type:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To use a manually defined instance type, add the following annotation:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;object&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.machine_type: "t3.medium" <span id="CO12-1"/><span class="callout">1</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO12-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the instance type that you defined in the config map.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To use an automatic instance type, add the following annotations:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;Pod&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.default_vcpus: &lt;vcpus&gt;
    io.katacontainers.config.hypervisor.default_memory: &lt;memory&gt;
# ...</pre><p class="simpara">
									Define the amount of memory available for the workload to use. The workload will run on an automatic instance type based on the amount of memory available.
								</p></li></ul></div></li><li class="listitem"><p class="simpara">
							Apply the changes to the workload object by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f &lt;object.yaml&gt;</pre><p class="simpara">
							OpenShift Container Platform creates the workload object and begins scheduling it.
						</p></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Inspect the <code class="literal">spec.runtimeClassName</code> field of a pod-templated object. If the value is <code class="literal">kata-remote</code>, then the workload is running on OpenShift sandboxed containers, using peer pods.
						</li></ul></div></section></section></section><section class="chapter" id="deploying-osc-on-azure"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Deploying OpenShift sandboxed containers on Azure</h1></div></div></div><p>
			You can deploy OpenShift sandboxed containers on Microsoft Azure Cloud Computing Services.
		</p><p>
			OpenShift sandboxed containers deploys peer pods. The peer pod design circumvents the need for nested virtualization. For more information, see <a class="link" href="#peer-pods">peer pods</a>.
		</p><div class="itemizedlist"><p class="title"><strong>Cluster requirements</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
					You have installed Red Hat OpenShift Container Platform 4.14 or later on the cluster where you are installing the OpenShift sandboxed containers Operator.
				</li><li class="listitem">
					Your cluster has at least one worker node.
				</li></ul></div><section class="section" id="peer-pod-resource-requirements_azure"><div class="titlepage"><div><div><h2 class="title">4.1. Peer pod resource requirements</h2></div></div></div><p>
				You must ensure that your cluster has sufficient resources.
			</p><p>
				Peer pod virtual machines (VMs) require resources in two locations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The worker node. The worker node stores metadata, Kata shim resources (<code class="literal">containerd-shim-kata-v2</code>), remote-hypervisor resources (<code class="literal">cloud-api-adaptor</code>), and the tunnel setup between the worker nodes and the peer pod VM.
					</li><li class="listitem">
						The cloud instance. This is the actual peer pod VM running in the cloud.
					</li></ul></div><p>
				The CPU and memory resources used in the Kubernetes worker node are handled by the <a class="link" href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</a> included in the RuntimeClass (<code class="literal">kata-remote</code>) definition used for creating peer pods.
			</p><p>
				The total number of peer pod VMs running in the cloud is defined as Kubernetes Node extended resources. This limit is per node and is set by the <code class="literal">limit</code> attribute in the <code class="literal">peerpodConfig</code> custom resource (CR).
			</p><p>
				The <code class="literal">peerpodConfig</code> CR, named <code class="literal">peerpodconfig-openshift</code>, is created when you create the <code class="literal">kataConfig</code> CR and enable peer pods, and is located in the <code class="literal">openshift-sandboxed-containers-operator</code> namespace.
			</p><p>
				The following <code class="literal">peerpodConfig</code> CR example displays the default <code class="literal">spec</code> values:
			</p><pre class="programlisting language-yaml">apiVersion: confidentialcontainers.org/v1alpha1
kind: PeerPodConfig
metadata:
  name: peerpodconfig-openshift
  namespace: openshift-sandboxed-containers-operator
spec:
  cloudSecretName: peer-pods-secret
  configMapName: peer-pods-cm
  limit: "10" <span id="CO13-1"/><span class="callout">1</span>
  nodeSelector:
    node-role.kubernetes.io/kata-oc: ""</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO13-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						The default limit is 10 VMs per node.
					</div></dd></dl></div><p>
				The extended resource is named <code class="literal">kata.peerpods.io/vm</code>, and enables the Kubernetes scheduler to handle capacity tracking and accounting.
			</p><p>
				You can edit the limit per node based on the requirements for your environment after you install the OpenShift sandboxed containers Operator.
			</p><p>
				A <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">mutating webhook</a> adds the extended resource <code class="literal">kata.peerpods.io/vm</code> to the pod specification. It also removes any resource-specific entries from the pod specification, if present. This enables the Kubernetes scheduler to account for these extended resources, ensuring the peer pod is only scheduled when resources are available.
			</p><p>
				The mutating webhook modifies a Kubernetes pod as follows:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The mutating webhook checks the pod for the expected <code class="literal">RuntimeClassName</code> value, specified in the <code class="literal">TARGET_RUNTIME_CLASS</code> environment variable. If the value in the pod specification does not match the value in the <code class="literal">TARGET_RUNTIME_CLASS</code>, the webhook exits without modifying the pod.
					</li><li class="listitem"><p class="simpara">
						If the <code class="literal">RuntimeClassName</code> values match, the webhook makes the following changes to the pod spec:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								The webhook removes every resource specification from the <code class="literal">resources</code> field of all containers and init containers in the pod.
							</li><li class="listitem">
								The webhook adds the extended resource (<code class="literal">kata.peerpods.io/vm</code>) to the spec by modifying the resources field of the first container in the pod. The extended resource <code class="literal">kata.peerpods.io/vm</code> is used by the Kubernetes scheduler for accounting purposes.
							</li></ol></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The mutating webhook excludes specific system namespaces in OpenShift Container Platform from mutation. If a peer pod is created in those system namespaces, then resource accounting using Kubernetes extended resources does not work unless the pod spec includes the extended resource.
				</p><p>
					As a best practice, define a cluster-wide policy to only allow peer pod creation in specific namespaces.
				</p></div></div></section><section class="section" id="deploying-osc-web_azure-web"><div class="titlepage"><div><div><h2 class="title">4.2. Deploying OpenShift sandboxed containers by using the web console</h2></div></div></div><p>
				You can deploy OpenShift sandboxed containers on Azure by using the OpenShift Container Platform web console to perform the following tasks:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Install the OpenShift sandboxed containers Operator.
					</li><li class="listitem">
						Create the peer pods secret.
					</li><li class="listitem">
						Create the peer pods config map.
					</li><li class="listitem">
						Create the Azure secret.
					</li><li class="listitem">
						Create the <code class="literal">KataConfig</code> custom resource.
					</li><li class="listitem">
						Configure the OpenShift sandboxed containers workload objects.
					</li></ol></div><section class="section" id="installing-operator-web-console_azure-web"><div class="titlepage"><div><div><h3 class="title">4.2.1. Installing the OpenShift sandboxed containers Operator</h3></div></div></div><p>
					You can install the OpenShift sandboxed containers Operator by using the OpenShift Container Platform web console.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>OperatorHub</strong></span>.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Filter by keyword</strong></span> field, type <code class="literal">OpenShift sandboxed containers</code>.
						</li><li class="listitem">
							Select the <span class="strong strong"><strong>OpenShift sandboxed containers Operator</strong></span> tile and click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Install Operator</strong></span> page, select <span class="strong strong"><strong>stable</strong></span> from the list of available <span class="strong strong"><strong>Update Channel</strong></span> options.
						</li><li class="listitem"><p class="simpara">
							Verify that <span class="strong strong"><strong>Operator recommended Namespace</strong></span> is selected for <span class="strong strong"><strong>Installed Namespace</strong></span>. This installs the Operator in the mandatory <code class="literal">openshift-sandboxed-containers-operator</code> namespace. If this namespace does not yet exist, it is automatically created.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Attempting to install the OpenShift sandboxed containers Operator in a namespace other than <code class="literal">openshift-sandboxed-containers-operator</code> causes the installation to fail.
							</p></div></div></li><li class="listitem">
							Verify that <span class="strong strong"><strong>Automatic</strong></span> is selected for <span class="strong strong"><strong>Approval Strategy</strong></span>. <span class="strong strong"><strong>Automatic</strong></span> is the default value, and enables automatic updates to OpenShift sandboxed containers when a new z-stream release is available.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Install</strong></span>.
						</li><li class="listitem">
							Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span> to verify that the Operator is installed.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</a>.
						</li><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</a> for disconnected environments.
						</li></ul></div></section><section class="section" id="creating-peer-pods-secret_azure-web"><div class="titlepage"><div><div><h3 class="title">4.2.2. Creating the peer pods secret</h3></div></div></div><p>
					You must create the peer pods secret for OpenShift sandboxed containers.
				</p><p>
					The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.
				</p><p>
					By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed and configured the Azure CLI tool.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Retrieve the Azure subscription ID by running the following command:
						</p><pre class="programlisting language-terminal">$ AZURE_SUBSCRIPTION_ID=$(az account list --query "[?isDefault].id" \
  -o tsv) &amp;&amp; echo "AZURE_SUBSCRIPTION_ID: \"$AZURE_SUBSCRIPTION_ID\""</pre></li><li class="listitem"><p class="simpara">
							Generate the RBAC content by running the following command:
						</p><pre class="programlisting language-terminal">$ az ad sp create-for-rbac --role Contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID \
  --query "{ client_id: appId, client_secret: password, tenant_id: tenant }"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">{
  "client_id": `AZURE_CLIENT_ID`,
  "client_secret": `AZURE_CLIENT_SECRET`,
  "tenant_id": `AZURE_TENANT_ID`
}</pre>
							</p></div></li><li class="listitem">
							Record the RBAC output to use in the <code class="literal">secret</code> object.
						</li><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
						</li><li class="listitem">
							Click the OpenShift sandboxed containers Operator tile.
						</li><li class="listitem">
							Click the Import icon (<span class="strong strong"><strong>+</strong></span>) on the top right corner.
						</li><li class="listitem"><p class="simpara">
							In the <span class="strong strong"><strong>Import YAML</strong></span> window, paste the following YAML manifest:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  AZURE_CLIENT_ID: "&lt;azure_client_id&gt;" <span id="CO14-1"/><span class="callout">1</span>
  AZURE_CLIENT_SECRET: "&lt;azure_client_secret&gt;" <span id="CO14-2"/><span class="callout">2</span>
  AZURE_TENANT_ID: "&lt;azure_tenant_id&gt;" <span id="CO14-3"/><span class="callout">3</span>
  AZURE_SUBSCRIPTION_ID: "&lt;azure_subscription_id&gt;" <span id="CO14-4"/><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO14-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_CLIENT_ID</code> value.
								</div></dd><dt><a href="#CO14-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_CLIENT_SECRET</code> value.
								</div></dd><dt><a href="#CO14-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_TENANT_ID</code> value.
								</div></dd><dt><a href="#CO14-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_SUBSCRIPTION_ID</code> value.
								</div></dd></dl></div></li><li class="listitem">
							Click <span class="strong strong"><strong>Save</strong></span> to apply the changes.
						</li><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span> to verify the peer pods secret.
						</li></ol></div></section><section class="section" id="creating-peer-pods-config-map_azure-web"><div class="titlepage"><div><div><h3 class="title">4.2.3. Creating the peer pods config map</h3></div></div></div><p>
					You must create the peer pods config map for OpenShift sandboxed containers.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the following values from your Azure instance:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve and record the Azure resource group:
								</p><pre class="programlisting language-terminal">$ AZURE_RESOURCE_GROUP=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.azure.resourceGroupName}') &amp;&amp; echo "AZURE_RESOURCE_GROUP: \"$AZURE_RESOURCE_GROUP\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the Azure VNet name:
								</p><pre class="programlisting language-terminal">$ AZURE_VNET_NAME=$(az network vnet list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Name:name}" --output tsv)</pre><p class="simpara">
									This value is used to retrieve the Azure subnet ID.
								</p></li><li class="listitem"><p class="simpara">
									Retrieve and record the Azure subnet ID:
								</p><pre class="programlisting language-terminal">$ AZURE_SUBNET_ID=$(az network vnet subnet list --resource-group ${AZURE_RESOURCE_GROUP} --vnet-name $AZURE_VNET_NAME --query "[].{Id:id} | [? contains(Id, 'worker')]" --output tsv) &amp;&amp; echo "AZURE_SUBNET_ID: \"$AZURE_SUBNET_ID\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the Azure network security group (NSG) ID:
								</p><pre class="programlisting language-terminal">$ AZURE_NSG_ID=$(az network nsg list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Id:id}" --output tsv) &amp;&amp; echo "AZURE_NSG_ID: \"$AZURE_NSG_ID\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the Azure region:
								</p><pre class="programlisting language-terminal">$ AZURE_REGION=$(az group show --resource-group ${AZURE_RESOURCE_GROUP} --query "{Location:location}" --output tsv) &amp;&amp; echo "AZURE_REGION: \"$AZURE_REGION\""</pre></li></ol></div></li><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
						</li><li class="listitem">
							Select the OpenShift sandboxed containers Operator from the list of operators.
						</li><li class="listitem">
							Click the Import icon (<span class="strong strong"><strong>+</strong></span>) in the top right corner.
						</li><li class="listitem"><p class="simpara">
							In the <span class="strong strong"><strong>Import YAML</strong></span> window, paste the following YAML manifest:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "azure"
  VXLAN_PORT: "9000"
  AZURE_INSTANCE_SIZE: "Standard_B2als_v2" <span id="CO15-1"/><span class="callout">1</span>
  AZURE_INSTANCE_SIZES: "Standard_B2als_v2,Standard_D2as_v5,Standard_D4as_v5,Standard_D2ads_v5" <span id="CO15-2"/><span class="callout">2</span>
  AZURE_SUBNET_ID: "&lt;azure_subnet_id&gt;" <span id="CO15-3"/><span class="callout">3</span>
  AZURE_NSG_ID: "&lt;azure_nsg_id&gt;" <span id="CO15-4"/><span class="callout">4</span>
  PROXY_TIMEOUT: "5m"
  AZURE_IMAGE_ID: "&lt;azure_image_id&gt;" <span id="CO15-5"/><span class="callout">5</span>
  AZURE_REGION: "&lt;azure_region&gt;" <span id="CO15-6"/><span class="callout">6</span>
  AZURE_RESOURCE_GROUP: "&lt;azure_resource_group&gt;" <span id="CO15-7"/><span class="callout">7</span>
  DISABLECVM: "true"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO15-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									This value is the default if an instance size is not defined in the workload.
								</div></dd><dt><a href="#CO15-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Lists all of the instance sizes you can specify when creating the pod. This allows you to define smaller instance sizes for workloads that need less memory and fewer CPUs or larger instance sizes for larger workloads.
								</div></dd><dt><a href="#CO15-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_SUBNET_ID</code> value that you retrieved.
								</div></dd><dt><a href="#CO15-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_NSG_ID</code> value that you retrieved.
								</div></dd><dt><a href="#CO15-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Optional: By default, this value is populated when you run the <code class="literal">KataConfig</code> CR, using an Azure image ID based on your cluster credentials. If you create your own Azure image, specify the correct image ID.
								</div></dd><dt><a href="#CO15-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_REGION</code> value you retrieved.
								</div></dd><dt><a href="#CO15-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_RESOURCE_GROUP</code> value you retrieved.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Save</strong></span> to apply the changes.
						</p><pre class="programlisting language-terminal">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</pre></li><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>ConfigMaps</strong></span> to view the new config map.
						</li></ol></div></section><section class="section" id="creating-ssh-key-secret_azure-web"><div class="titlepage"><div><div><h3 class="title">4.2.4. Creating the Azure secret</h3></div></div></div><p>
					You must create the secret for Azure.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to your OpenShift Container Platform cluster.
						</li><li class="listitem"><p class="simpara">
							Generate an SSH key pair by running the following command:
						</p><pre class="programlisting language-terminal">$ ssh-keygen -f ./id_rsa -N ""</pre></li><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Secrets</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Secrets</strong></span> page, verify that you are in the <span class="strong strong"><strong>openshift-sandboxed-containers-operator</strong></span> project.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Create</strong></span> and select <span class="strong strong"><strong>Key/value secret</strong></span>.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Secret name</strong></span> field, enter <code class="literal">ssh-key-secret</code>.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Key</strong></span> field, enter <code class="literal">id_rsa.pub</code>.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Value</strong></span> field, paste your public SSH key.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Create</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Delete the SSH keys you created:
						</p><pre class="programlisting language-terminal">$ shred --remove id_rsa.pub id_rsa</pre></li></ol></div></section><section class="section" id="creating-kataconfig-cr-web_azure-web"><div class="titlepage"><div><div><h3 class="title">4.2.5. Creating the KataConfig custom resource</h3></div></div></div><p>
					You must create the <code class="literal">KataConfig</code> custom resource (CR) to install <code class="literal">kata-remote</code> as a <code class="literal">RuntimeClass</code> on your worker nodes.
				</p><p>
					The <code class="literal">kata-remote</code> runtime class is installed on all worker nodes by default. If you want to install <code class="literal">kata-remote</code> on specific nodes, you can add labels to those nodes and then define the label in the <code class="literal">KataConfig</code> CR.
				</p><p>
					OpenShift sandboxed containers installs <code class="literal">kata-remote</code> as a <span class="emphasis"><em>secondary, optional</em></span> runtime on the cluster and not as the primary runtime.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Creating the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. The following factors might increase the reboot time:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								A larger OpenShift Container Platform deployment with a greater number of worker nodes.
							</li><li class="listitem">
								Activation of the BIOS and Diagnostics utility.
							</li><li class="listitem">
								Deployment on a hard disk drive rather than an SSD.
							</li><li class="listitem">
								Deployment on physical nodes such as bare metal, rather than on virtual nodes.
							</li><li class="listitem">
								A slow CPU and network.
							</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li><li class="listitem">
							Optional: You have installed the Node Feature Discovery Operator if you want to enable node eligibility checks.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
						</li><li class="listitem">
							Select the OpenShift sandboxed containers Operator.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>KataConfig</strong></span> tab, click <span class="strong strong"><strong>Create KataConfig</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Enter the following details:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>Name</strong></span>: Optional: The default name is <code class="literal">example-kataconfig</code>.
								</li><li class="listitem">
									<span class="strong strong"><strong>Labels</strong></span>: Optional: Enter any relevant, identifying attributes to the <code class="literal">KataConfig</code> resource. Each label represents a key-value pair.
								</li><li class="listitem">
									<span class="strong strong"><strong>enablePeerPods</strong></span>: Select for public cloud, IBM Z®, and IBM® LinuxONE deployments.
								</li><li class="listitem"><p class="simpara">
									<span class="strong strong"><strong>kataConfigPoolSelector</strong></span>. Optional: To install <code class="literal">kata-remote</code> on selected nodes, add a match expression for the labels on the selected nodes:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Expand the <span class="strong strong"><strong>kataConfigPoolSelector</strong></span> area.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>kataConfigPoolSelector</strong></span> area, expand <span class="strong strong"><strong>matchExpressions</strong></span>. This is a list of label selector requirements.
										</li><li class="listitem">
											Click <span class="strong strong"><strong>Add matchExpressions</strong></span>.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>Key</strong></span> field, enter the label key the selector applies to.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>Operator</strong></span> field, enter the key’s relationship to the label values. Valid operators are <code class="literal">In</code>, <code class="literal">NotIn</code>, <code class="literal">Exists</code>, and <code class="literal">DoesNotExist</code>.
										</li><li class="listitem">
											Expand the <span class="strong strong"><strong>Values</strong></span> area and then click <span class="strong strong"><strong>Add value</strong></span>.
										</li><li class="listitem">
											In the <span class="strong strong"><strong>Value</strong></span> field, enter <code class="literal">true</code> or <code class="literal">false</code> for <span class="strong strong"><strong>key</strong></span> label value.
										</li></ol></div></li><li class="listitem">
									<span class="strong strong"><strong>logLevel</strong></span>: Define the level of log data retrieved for nodes with the <code class="literal">kata-remote</code> runtime class.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Create</strong></span>. The <code class="literal">KataConfig</code> CR is created and installs the <code class="literal">kata-remote</code> runtime class on the worker nodes.
						</p><p class="simpara">
							Wait for the <code class="literal">kata-remote</code> installation to complete and the worker nodes to reboot before verifying the installation.
						</p></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							On the <span class="strong strong"><strong>KataConfig</strong></span> tab, click the <code class="literal">KataConfig</code> CR to view its details.
						</li><li class="listitem"><p class="simpara">
							Click the <span class="strong strong"><strong>YAML</strong></span> tab to view the <code class="literal">status</code> stanza.
						</p><p class="simpara">
							The <code class="literal">status</code> stanza contains the <code class="literal">conditions</code> and <code class="literal">kataNodes</code> keys. The value of <code class="literal">status.kataNodes</code> is an array of nodes, each of which lists nodes in a particular state of <code class="literal">kata-remote</code> installation. A message appears each time there is an update.
						</p></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Reload</strong></span> to refresh the YAML.
						</p><p class="simpara">
							When all workers in the <code class="literal">status.kataNodes</code> array display the values <code class="literal">installed</code> and <code class="literal">conditions.InProgress: False</code> with no specified reason, the <code class="literal">kata-remote</code> is installed on the cluster.
						</p></li></ol></div><h5 id="additional_resources_3">Additional resources</h5><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="#kataconfig-status-messages" title="Appendix A. KataConfig status messages">KataConfig status messages</a>
						</li></ul></div><h5 id="verifying-pod-vm-image-creation_azure-web">Verifying the pod VM image</h5><p>
					After <code class="literal">kata-remote</code> is installed on your cluster, the OpenShift sandboxed containers Operator creates a pod VM image, which is used to create peer pods. This process can take a long time because the image is created on the cloud instance. You can verify that the pod VM image was created successfully by checking the config map that you created for the cloud provider.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>ConfigMaps</strong></span>.
						</li><li class="listitem">
							Click the provider config map to view its details.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>YAML</strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							Check the <code class="literal">status</code> stanza of the YAML file.
						</p><p class="simpara">
							If the <code class="literal">AZURE_IMAGE_ID</code> parameter is populated, the pod VM image was created successfully.
						</p></li></ol></div><div class="orderedlist"><p class="title"><strong>Troubleshooting</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Retrieve the events log by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get events -n openshift-sandboxed-containers-operator --field-selector involvedObject.name=osc-podvm-image-creation</pre></li><li class="listitem"><p class="simpara">
							Retrieve the job log by running the following command:
						</p><pre class="programlisting language-terminal">$ oc logs -n openshift-sandboxed-containers-operator jobs/osc-podvm-image-creation</pre></li></ol></div><p>
					If you cannot resolve the issue, submit a Red Hat Support case and attach the output of both logs.
				</p></section><section class="section" id="configuring-workload-objects_azure-web"><div class="titlepage"><div><div><h3 class="title">4.2.6. Configuring workload objects</h3></div></div></div><p>
					You must configure OpenShift sandboxed containers workload objects by setting <code class="literal">kata-remote</code> as the runtime class for the following pod-templated objects:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">Pod</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicaSet</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicationController</code> objects
						</li><li class="listitem">
							<code class="literal">StatefulSet</code> objects
						</li><li class="listitem">
							<code class="literal">Deployment</code> objects
						</li><li class="listitem">
							<code class="literal">DeploymentConfig</code> objects
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.
					</p></div></div><p>
					You can define whether the workload should be deployed using the default instance size, which you defined in the config map, by adding an annotation to the YAML file.
				</p><p>
					If you do not want to define the instance size manually, you can add an annotation to use an automatic instance size, based on the memory available.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created the <code class="literal">KataConfig</code> custom resource (CR).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Workloads</strong></span> → workload type, for example, <span class="strong strong"><strong>Pods</strong></span>.
						</li><li class="listitem">
							On the workload type page, click an object to view its details.
						</li><li class="listitem">
							Click the <span class="strong strong"><strong>YAML</strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							Add <code class="literal">spec.runtimeClassName: kata-remote</code> to the manifest of each pod-templated workload object as in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata-remote
# ...</pre></li><li class="listitem"><p class="simpara">
							Add an annotation to the pod-templated object to use a manually defined instance size or an automatic instance size:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To use a manually defined instance size, add the following annotation:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;object&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.machine_type: "Standard_B2als_v2" <span id="CO16-1"/><span class="callout">1</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO16-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the instance size that you defined in the config map.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To use an automatic instance size, add the following annotations:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;Pod&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.default_vcpus: &lt;vcpus&gt;
    io.katacontainers.config.hypervisor.default_memory: &lt;memory&gt;
# ...</pre><p class="simpara">
									Define the amount of memory available for the workload to use. The workload will run on an automatic instance size based on the amount of memory available.
								</p></li></ul></div></li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Save</strong></span> to apply the changes.
						</p><p class="simpara">
							OpenShift Container Platform creates the workload object and begins scheduling it.
						</p></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Inspect the <code class="literal">spec.runtimeClassName</code> field of a pod-templated object. If the value is <code class="literal">kata-remote</code>, then the workload is running on OpenShift sandboxed containers, using peer pods.
						</li></ul></div></section></section><section class="section" id="deploying-osc-cli_azure-cli"><div class="titlepage"><div><div><h2 class="title">4.3. Deploying OpenShift sandboxed containers by using the command line</h2></div></div></div><p>
				You can deploy OpenShift sandboxed containers on Azure by using the command line interface (CLI) to perform the following tasks:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Install the OpenShift sandboxed containers Operator.
					</li><li class="listitem">
						Optional: Change the number of virtual machines running on each worker node.
					</li><li class="listitem">
						Create the peer pods secret.
					</li><li class="listitem">
						Create the peer pods config map.
					</li><li class="listitem">
						Create the Azure secret.
					</li><li class="listitem">
						Create the <code class="literal">KataConfig</code> custom resource.
					</li><li class="listitem">
						Configure the OpenShift sandboxed containers workload objects.
					</li></ol></div><section class="section" id="installing-operator-cli_azure-cli"><div class="titlepage"><div><div><h3 class="title">4.3.1. Installing the OpenShift sandboxed containers Operator</h3></div></div></div><p>
					You can install the OpenShift sandboxed containers Operator by using the CLI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-namespace.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sandboxed-containers-operator</pre></li><li class="listitem"><p class="simpara">
							Create the namespace by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-namespace.yaml</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-operatorgroup.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sandboxed-containers-operator-group
  namespace: openshift-sandboxed-containers-operator
spec:
  targetNamespaces:
  - openshift-sandboxed-containers-operator</pre></li><li class="listitem"><p class="simpara">
							Create the operator group by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-operatorgroup.yaml</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-subscription.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sandboxed-containers-operator
  namespace: openshift-sandboxed-containers-operator
spec:
  channel: stable
  installPlanApproval: Automatic
  name: sandboxed-containers-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: sandboxed-containers-operator.v1.8.0</pre></li><li class="listitem"><p class="simpara">
							Create the subscription by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-subscription.yaml</pre></li><li class="listitem"><p class="simpara">
							Verify that the Operator is correctly installed by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get csv -n openshift-sandboxed-containers-operator</pre><p class="simpara">
							This command can take several minutes to complete.
						</p></li><li class="listitem"><p class="simpara">
							Watch the process by running the following command:
						</p><pre class="programlisting language-terminal">$ watch oc get csv -n openshift-sandboxed-containers-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                             DISPLAY                                  VERSION             REPLACES                   PHASE
openshift-sandboxed-containers   openshift-sandboxed-containers-operator  1.8.0    1.7.0        Succeeded</pre>
							</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</a>.
						</li><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</a> for disconnected environments.
						</li></ul></div></section><section class="section" id="modifying-peer-pod-vm-limit_azure-cli"><div class="titlepage"><div><div><h3 class="title">4.3.2. Modifying the number of peer pod VMs per node</h3></div></div></div><p>
					You can change the limit of peer pod virtual machines (VMs) per node by editing the <code class="literal">peerpodConfig</code> custom resource (CR).
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the current limit by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
-o jsonpath='{.spec.limit}{"\n"}'</pre></li><li class="listitem"><p class="simpara">
							Modify the <code class="literal">limit</code> attribute of the <code class="literal">peerpodConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc patch peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
--type merge --patch '{"spec":{"limit":"&lt;value&gt;"}}' <span id="CO17-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO17-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace &lt;value&gt; with the limit you want to define.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="creating-peer-pods-secret_azure-cli"><div class="titlepage"><div><div><h3 class="title">4.3.3. Creating the peer pods secret</h3></div></div></div><p>
					You must create the peer pods secret for OpenShift sandboxed containers.
				</p><p>
					The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.
				</p><p>
					By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed and configured the Azure CLI tool.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Retrieve the Azure subscription ID by running the following command:
						</p><pre class="programlisting language-terminal">$ AZURE_SUBSCRIPTION_ID=$(az account list --query "[?isDefault].id" \
  -o tsv) &amp;&amp; echo "AZURE_SUBSCRIPTION_ID: \"$AZURE_SUBSCRIPTION_ID\""</pre></li><li class="listitem"><p class="simpara">
							Generate the RBAC content by running the following command:
						</p><pre class="programlisting language-terminal">$ az ad sp create-for-rbac --role Contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID \
  --query "{ client_id: appId, client_secret: password, tenant_id: tenant }"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">{
  "client_id": `AZURE_CLIENT_ID`,
  "client_secret": `AZURE_CLIENT_SECRET`,
  "tenant_id": `AZURE_TENANT_ID`
}</pre>
							</p></div></li><li class="listitem">
							Record the RBAC output to use in the <code class="literal">secret</code> object.
						</li><li class="listitem"><p class="simpara">
							Create a <code class="literal">peer-pods-secret.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  AZURE_CLIENT_ID: "&lt;azure_client_id&gt;" <span id="CO18-1"/><span class="callout">1</span>
  AZURE_CLIENT_SECRET: "&lt;azure_client_secret&gt;" <span id="CO18-2"/><span class="callout">2</span>
  AZURE_TENANT_ID: "&lt;azure_tenant_id&gt;" <span id="CO18-3"/><span class="callout">3</span>
  AZURE_SUBSCRIPTION_ID: "&lt;azure_subscription_id&gt;" <span id="CO18-4"/><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO18-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_CLIENT_ID</code> value.
								</div></dd><dt><a href="#CO18-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_CLIENT_SECRET</code> value.
								</div></dd><dt><a href="#CO18-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_TENANT_ID</code> value.
								</div></dd><dt><a href="#CO18-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_SUBSCRIPTION_ID</code> value.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the secret by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f peer-pods-secret.yaml</pre></li><li class="listitem"><p class="simpara">
							Optional: To update an existing peer pods config map, restart the <code class="literal">peerpodconfig-ctrl-caa-daemon</code> daemon set by running the following command:
						</p><pre class="programlisting language-terminal">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</pre></li></ol></div></section><section class="section" id="creating-peer-pods-config-map_azure-cli"><div class="titlepage"><div><div><h3 class="title">4.3.4. Creating the peer pods config map</h3></div></div></div><p>
					You must create the peer pods config map for OpenShift sandboxed containers.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the following values from your Azure instance:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Retrieve and record the Azure resource group:
								</p><pre class="programlisting language-terminal">$ AZURE_RESOURCE_GROUP=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.azure.resourceGroupName}') &amp;&amp; echo "AZURE_RESOURCE_GROUP: \"$AZURE_RESOURCE_GROUP\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the Azure VNet name:
								</p><pre class="programlisting language-terminal">$ AZURE_VNET_NAME=$(az network vnet list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Name:name}" --output tsv)</pre><p class="simpara">
									This value is used to retrieve the Azure subnet ID.
								</p></li><li class="listitem"><p class="simpara">
									Retrieve and record the Azure subnet ID:
								</p><pre class="programlisting language-terminal">$ AZURE_SUBNET_ID=$(az network vnet subnet list --resource-group ${AZURE_RESOURCE_GROUP} --vnet-name $AZURE_VNET_NAME --query "[].{Id:id} | [? contains(Id, 'worker')]" --output tsv) &amp;&amp; echo "AZURE_SUBNET_ID: \"$AZURE_SUBNET_ID\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the Azure network security group (NSG) ID:
								</p><pre class="programlisting language-terminal">$ AZURE_NSG_ID=$(az network nsg list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Id:id}" --output tsv) &amp;&amp; echo "AZURE_NSG_ID: \"$AZURE_NSG_ID\""</pre></li><li class="listitem"><p class="simpara">
									Retrieve and record the Azure region:
								</p><pre class="programlisting language-terminal">$ AZURE_REGION=$(az group show --resource-group ${AZURE_RESOURCE_GROUP} --query "{Location:location}" --output tsv) &amp;&amp; echo "AZURE_REGION: \"$AZURE_REGION\""</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">peer-pods-cm.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "azure"
  VXLAN_PORT: "9000"
  AZURE_INSTANCE_SIZE: "Standard_B2als_v2" <span id="CO19-1"/><span class="callout">1</span>
  AZURE_INSTANCE_SIZES: "Standard_B2als_v2,Standard_D2as_v5,Standard_D4as_v5,Standard_D2ads_v5" <span id="CO19-2"/><span class="callout">2</span>
  AZURE_SUBNET_ID: "&lt;azure_subnet_id&gt;" <span id="CO19-3"/><span class="callout">3</span>
  AZURE_NSG_ID: "&lt;azure_nsg_id&gt;" <span id="CO19-4"/><span class="callout">4</span>
  PROXY_TIMEOUT: "5m"
  AZURE_IMAGE_ID: "&lt;azure_image_id&gt;" <span id="CO19-5"/><span class="callout">5</span>
  AZURE_REGION: "&lt;azure_region&gt;" <span id="CO19-6"/><span class="callout">6</span>
  AZURE_RESOURCE_GROUP: "&lt;azure_resource_group&gt;" <span id="CO19-7"/><span class="callout">7</span>
  DISABLECVM: "true"</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO19-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									This value is the default if an instance size is not defined in the workload.
								</div></dd><dt><a href="#CO19-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Lists all of the instance sizes you can specify when creating the pod. This allows you to define smaller instance sizes for workloads that need less memory and fewer CPUs or larger instance sizes for larger workloads.
								</div></dd><dt><a href="#CO19-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_SUBNET_ID</code> value that you retrieved.
								</div></dd><dt><a href="#CO19-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_NSG_ID</code> value that you retrieved.
								</div></dd><dt><a href="#CO19-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Optional: By default, this value is populated when you run the <code class="literal">KataConfig</code> CR, using an Azure image ID based on your cluster credentials. If you create your own Azure image, specify the correct image ID.
								</div></dd><dt><a href="#CO19-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_REGION</code> value you retrieved.
								</div></dd><dt><a href="#CO19-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									Specify the <code class="literal">AZURE_RESOURCE_GROUP</code> value you retrieved.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f peer-pods-cm.yaml</pre></li><li class="listitem"><p class="simpara">
							Optional: To update an existing peer pods config map, restart the <code class="literal">peerpodconfig-ctrl-caa-daemon</code> daemon set by running the following command:
						</p><pre class="programlisting language-terminal">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</pre></li></ol></div></section><section class="section" id="creating-ssh-key-secret_azure-cli"><div class="titlepage"><div><div><h3 class="title">4.3.5. Creating the Azure secret</h3></div></div></div><p>
					You must create the secret for Azure.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to your OpenShift Container Platform cluster.
						</li><li class="listitem"><p class="simpara">
							Generate an SSH key pair by running the following command:
						</p><pre class="programlisting language-terminal">$ ssh-keygen -f ./id_rsa -N ""</pre></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">Secret</code> object by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create secret generic ssh-key-secret \
  -n openshift-sandboxed-containers-operator \
  --from-file=id_rsa.pub=./id_rsa.pub \
  --from-file=id_rsa=./id_rsa</pre></li><li class="listitem"><p class="simpara">
							Delete the SSH keys you created:
						</p><pre class="programlisting language-terminal">$ shred --remove id_rsa.pub id_rsa</pre></li></ol></div></section><section class="section" id="creating-kataconfig-cr-cli_azure-cli"><div class="titlepage"><div><div><h3 class="title">4.3.6. Creating the KataConfig custom resource</h3></div></div></div><p>
					You must create the <code class="literal">KataConfig</code> custom resource (CR) to install <code class="literal">kata-remote</code> as a runtime class on your worker nodes.
				</p><p>
					Creating the <code class="literal">KataConfig</code> CR triggers the OpenShift sandboxed containers Operator to do the following: * Create a <code class="literal">RuntimeClass</code> CR named <code class="literal">kata-remote</code> with a default configuration. This enables users to configure workloads to use <code class="literal">kata-remote</code> as the runtime by referencing the CR in the <code class="literal">RuntimeClassName</code> field. This CR also specifies the resource overhead for the runtime.
				</p><p>
					OpenShift sandboxed containers installs <code class="literal">kata-remote</code> as a <span class="emphasis"><em>secondary, optional</em></span> runtime on the cluster and not as the primary runtime.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Creating the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								A larger OpenShift Container Platform deployment with a greater number of worker nodes.
							</li><li class="listitem">
								Activation of the BIOS and Diagnostics utility.
							</li><li class="listitem">
								Deployment on a hard disk drive rather than an SSD.
							</li><li class="listitem">
								Deployment on physical nodes such as bare metal, rather than on virtual nodes.
							</li><li class="listitem">
								A slow CPU and network.
							</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">example-kataconfig.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  enablePeerPods: true
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <span id="CO20-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO20-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional: If you have applied node labels to install <code class="literal">kata-remote</code> on specific nodes, specify the key and value, for example, <code class="literal">osc: 'true'</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">KataConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f example-kataconfig.yaml</pre><p class="simpara">
							The new <code class="literal">KataConfig</code> CR is created and installs <code class="literal">kata-remote</code> as a runtime class on the worker nodes.
						</p><p class="simpara">
							Wait for the <code class="literal">kata-remote</code> installation to complete and the worker nodes to reboot before verifying the installation.
						</p></li><li class="listitem"><p class="simpara">
							Monitor the installation progress by running the following command:
						</p><pre class="programlisting language-terminal">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</pre><p class="simpara">
							When the status of all workers under <code class="literal">kataNodes</code> is <code class="literal">installed</code> and the condition <code class="literal">InProgress</code> is <code class="literal">False</code> without specifying a reason, the <code class="literal">kata-remote</code> is installed on the cluster.
						</p></li><li class="listitem"><p class="simpara">
							Verify the daemon set by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon</pre></li><li class="listitem"><p class="simpara">
							Verify the runtime classes by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get runtimeclass</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME             HANDLER          AGE
kata             kata             152m
kata-remote      kata-remote      152m</pre>
							</p></div></li></ol></div><h5 id="verifying-pod-vm-image-creation_azure-cli">Verifying the pod VM image</h5><p>
					After <code class="literal">kata-remote</code> is installed on your cluster, the OpenShift sandboxed containers Operator creates a pod VM image, which is used to create peer pods. This process can take a long time because the image is created on the cloud instance. You can verify that the pod VM image was created successfully by checking the config map that you created for the cloud provider.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Obtain the config map you created for the peer pods:
						</p><pre class="programlisting language-terminal">$ oc get configmap peer-pods-cm -n openshift-sandboxed-containers-operator -o yaml</pre></li><li class="listitem"><p class="simpara">
							Check the <code class="literal">status</code> stanza of the YAML file.
						</p><p class="simpara">
							If the <code class="literal">AZURE_IMAGE_ID</code> parameter is populated, the pod VM image was created successfully.
						</p></li></ol></div><div class="orderedlist"><p class="title"><strong>Troubleshooting</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Retrieve the events log by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get events -n openshift-sandboxed-containers-operator --field-selector involvedObject.name=osc-podvm-image-creation</pre></li><li class="listitem"><p class="simpara">
							Retrieve the job log by running the following command:
						</p><pre class="programlisting language-terminal">$ oc logs -n openshift-sandboxed-containers-operator jobs/osc-podvm-image-creation</pre></li></ol></div><p>
					If you cannot resolve the issue, submit a Red Hat Support case and attach the output of both logs.
				</p></section><section class="section" id="configuring-workload-objects_azure-cli"><div class="titlepage"><div><div><h3 class="title">4.3.7. Configuring workload objects</h3></div></div></div><p>
					You must configure OpenShift sandboxed containers workload objects by setting <code class="literal">kata-remote</code> as the runtime class for the following pod-templated objects:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">Pod</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicaSet</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicationController</code> objects
						</li><li class="listitem">
							<code class="literal">StatefulSet</code> objects
						</li><li class="listitem">
							<code class="literal">Deployment</code> objects
						</li><li class="listitem">
							<code class="literal">DeploymentConfig</code> objects
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.
					</p></div></div><p>
					You can define whether the workload should be deployed using the default instance size, which you defined in the config map, by adding an annotation to the YAML file.
				</p><p>
					If you do not want to define the instance size manually, you can add an annotation to use an automatic instance size, based on the memory available.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created the <code class="literal">KataConfig</code> custom resource (CR).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add <code class="literal">spec.runtimeClassName: kata-remote</code> to the manifest of each pod-templated workload object as in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata-remote
# ...</pre></li><li class="listitem"><p class="simpara">
							Add an annotation to the pod-templated object to use a manually defined instance size or an automatic instance size:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To use a manually defined instance size, add the following annotation:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;object&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.machine_type: "Standard_B2als_v2" <span id="CO21-1"/><span class="callout">1</span>
# ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO21-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the instance size that you defined in the config map.
										</div></dd></dl></div></li><li class="listitem"><p class="simpara">
									To use an automatic instance size, add the following annotations:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;Pod&gt;
metadata:
  annotations:
    io.katacontainers.config.hypervisor.default_vcpus: &lt;vcpus&gt;
    io.katacontainers.config.hypervisor.default_memory: &lt;memory&gt;
# ...</pre><p class="simpara">
									Define the amount of memory available for the workload to use. The workload will run on an automatic instance size based on the amount of memory available.
								</p></li></ul></div></li><li class="listitem"><p class="simpara">
							Apply the changes to the workload object by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f &lt;object.yaml&gt;</pre><p class="simpara">
							OpenShift Container Platform creates the workload object and begins scheduling it.
						</p></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Inspect the <code class="literal">spec.runtimeClassName</code> field of a pod-templated object. If the value is <code class="literal">kata-remote</code>, then the workload is running on OpenShift sandboxed containers, using peer pods.
						</li></ul></div></section></section></section><section class="chapter" id="deploying-cc_azure-cc"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Deploying Confidential Containers on Azure</h1></div></div></div><p>
			You can deploy Confidential Containers on Microsoft Azure Cloud Computing Services after you deploy OpenShift sandboxed containers.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				Confidential Containers on Azure is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
			</p><p>
				For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
			</p></div></div><div class="itemizedlist"><p class="title"><strong>Cluster requirements</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
					You have installed Red Hat OpenShift Container Platform 4.15 or later on the cluster where you are installing the Confidential compute attestation Operator.
				</li></ul></div><p>
			You deploy Confidential Containers by performing the following steps:
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Install the Confidential compute attestation Operator.
				</li><li class="listitem">
					Create the route for Trustee.
				</li><li class="listitem">
					Enable the Confidential Containers feature gate.
				</li><li class="listitem">
					Update the peer pods config map.
				</li><li class="listitem">
					Delete the <code class="literal">KataConfig</code> custom resource (CR).
				</li><li class="listitem">
					Re-create the <code class="literal">KataConfig</code> CR.
				</li><li class="listitem">
					Create the Trustee authentication secret.
				</li><li class="listitem">
					Create the Trustee config map.
				</li><li class="listitem">
					Configure Trustee values, policies, and secrets.
				</li><li class="listitem">
					Create the <code class="literal">KbsConfig</code> CR.
				</li><li class="listitem">
					Verify the Trustee configuration.
				</li><li class="listitem">
					Verify the attestation process.
				</li></ol></div><section class="section" id="cc-installing-cc-operator-cli_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.1. Installing the Confidential compute attestation Operator</h2></div></div></div><p>
				You can install the Confidential compute attestation Operator on Azure by using the CLI.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">trustee-namespace.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: trustee-operator-system</pre></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">trustee-operator-system</code> namespace by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f trustee-namespace.yaml</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">trustee-operatorgroup.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: trustee-operator-group
  namespace: trustee-operator-system
spec:
  targetNamespaces:
  - trustee-operator-system</pre></li><li class="listitem"><p class="simpara">
						Create the operator group by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f trustee-operatorgroup.yaml</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">trustee-subscription.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: trustee-operator
  namespace: trustee-operator-system
spec:
  channel: stable
  installPlanApproval: Automatic
  name: trustee-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: trustee-operator.v0.1.0</pre></li><li class="listitem"><p class="simpara">
						Create the subscription by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f trustee-subscription.yaml</pre></li><li class="listitem"><p class="simpara">
						Verify that the Operator is correctly installed by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get csv -n trustee-operator-system</pre><p class="simpara">
						This command can take several minutes to complete.
					</p></li><li class="listitem"><p class="simpara">
						Watch the process by running the following command:
					</p><pre class="programlisting language-terminal">$ watch oc get csv -n trustee-operator-system</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                      DISPLAY                        PHASE
trustee-operator.v0.1.0   Trustee Operator  0.1.0        Succeeded</pre>
						</p></div></li></ol></div></section><section class="section" id="cc-enabling-feature-gate_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.2. Enabling the Confidential Containers feature gate</h2></div></div></div><p>
				You must enable the Confidential Containers feature gate.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have subscribed to the OpenShift sandboxed containers Operator.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">cc-feature-gate.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: osc-feature-gates
  namespace: openshift-sandboxed-containers-operator
data:
  confidential: "true"</pre></li><li class="listitem"><p class="simpara">
						Create the config map by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f cc-feature-gate.yaml</pre></li></ol></div></section><section class="section" id="cc-creating-route_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.3. Creating the route for Trustee</h2></div></div></div><p>
				You can create a secure route with edge TLS termination for Trustee. External ingress traffic reaches the router pods as HTTPS and passes on to the Trustee pods as HTTP.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the Confidential compute attestation Operator.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create an edge route by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create route edge --service=kbs-service --port kbs-port \
  -n trustee-operator-system</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Note: Currently, only a route with a valid CA-signed certificate is supported. You cannot use a route with self-signed certificate.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Set the <code class="literal">TRUSTEE_HOST</code> variable by running the following command:
					</p><pre class="programlisting language-terminal">$ TRUSTEE_HOST=$(oc get route -n trustee-operator-system kbs-service \
  -o jsonpath={.spec.host})</pre></li><li class="listitem"><p class="simpara">
						Verify the route by running the following command:
					</p><pre class="programlisting language-terminal">$ echo $TRUSTEE_HOST</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">kbs-service-trustee-operator-system.apps.memvjias.eastus.aroapp.io</pre>
						</p></div><p class="simpara">
						Record this value for the peer pods config map.
					</p></li></ol></div></section><section class="section" id="creating-peer-pods-config-map_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.4. Updating the peer pods config map</h2></div></div></div><p>
				You must update the peer pods config map for Confidential Containers.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Set Secure Boot to <code class="literal">true</code> to enable it by default. The default value is <code class="literal">false</code>, which presents a security risk.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Obtain the following values from your Azure instance:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Retrieve and record the Azure resource group:
							</p><pre class="programlisting language-terminal">$ AZURE_RESOURCE_GROUP=$(oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.azure.resourceGroupName}') &amp;&amp; echo "AZURE_RESOURCE_GROUP: \"$AZURE_RESOURCE_GROUP\""</pre></li><li class="listitem"><p class="simpara">
								Retrieve and record the Azure VNet name:
							</p><pre class="programlisting language-terminal">$ AZURE_VNET_NAME=$(az network vnet list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Name:name}" --output tsv)</pre><p class="simpara">
								This value is used to retrieve the Azure subnet ID.
							</p></li><li class="listitem"><p class="simpara">
								Retrieve and record the Azure subnet ID:
							</p><pre class="programlisting language-terminal">$ AZURE_SUBNET_ID=$(az network vnet subnet list --resource-group ${AZURE_RESOURCE_GROUP} --vnet-name $AZURE_VNET_NAME --query "[].{Id:id} | [? contains(Id, 'worker')]" --output tsv) &amp;&amp; echo "AZURE_SUBNET_ID: \"$AZURE_SUBNET_ID\""</pre></li><li class="listitem"><p class="simpara">
								Retrieve and record the Azure network security group (NSG) ID:
							</p><pre class="programlisting language-terminal">$ AZURE_NSG_ID=$(az network nsg list --resource-group ${AZURE_RESOURCE_GROUP} --query "[].{Id:id}" --output tsv) &amp;&amp; echo "AZURE_NSG_ID: \"$AZURE_NSG_ID\""</pre></li><li class="listitem"><p class="simpara">
								Retrieve and record the Azure region:
							</p><pre class="programlisting language-terminal">$ AZURE_REGION=$(az group show --resource-group ${AZURE_RESOURCE_GROUP} --query "{Location:location}" --output tsv) &amp;&amp; echo "AZURE_REGION: \"$AZURE_REGION\""</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">peer-pods-cm.yaml</code> manifest file according to the following example:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "azure"
  VXLAN_PORT: "9000"
  AZURE_INSTANCE_SIZE: "Standard_DC2as_v5" <span id="CO22-1"/><span class="callout">1</span>
  AZURE_INSTANCE_SIZES: "Standard_DC2as_v5,Standard_DC4as_v5,Standard_DC8as_v5,Standard_DC16as_v5" <span id="CO22-2"/><span class="callout">2</span>
  AZURE_SUBNET_ID: "&lt;azure_subnet_id&gt;" <span id="CO22-3"/><span class="callout">3</span>
  AZURE_NSG_ID: "&lt;azure_nsg_id&gt;" <span id="CO22-4"/><span class="callout">4</span>
  PROXY_TIMEOUT: "5m"
  AZURE_IMAGE_ID: "&lt;azure_image_id&gt;" <span id="CO22-5"/><span class="callout">5</span>
  AZURE_REGION: "&lt;azure_region&gt;" <span id="CO22-6"/><span class="callout">6</span>
  AZURE_RESOURCE_GROUP: "&lt;azure_resource_group&gt;" <span id="CO22-7"/><span class="callout">7</span>
  DISABLECVM: "false"
  AA_KBC_PARAMS: "cc_kbc::https://${TRUSTEE_HOST}" <span id="CO22-8"/><span class="callout">8</span>
  ENABLE_SECURE_BOOT: "true" <span id="CO22-9"/><span class="callout">9</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO22-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								This value is the default if an instance size is not defined in the workload.
							</div></dd><dt><a href="#CO22-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Lists all of the instance sizes you can specify when creating the pod. This allows you to define smaller instance sizes for workloads that need less memory and fewer CPUs or larger instance sizes for larger workloads.
							</div></dd><dt><a href="#CO22-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">AZURE_SUBNET_ID</code> value that you retrieved.
							</div></dd><dt><a href="#CO22-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">AZURE_NSG_ID</code> value that you retrieved.
							</div></dd><dt><a href="#CO22-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								Optional: By default, this value is populated when you run the <code class="literal">KataConfig</code> CR, using an Azure image ID based on your cluster credentials. If you create your own Azure image, specify the correct image ID.
							</div></dd><dt><a href="#CO22-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">AZURE_REGION</code> value you retrieved.
							</div></dd><dt><a href="#CO22-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">AZURE_RESOURCE_GROUP</code> value you retrieved.
							</div></dd><dt><a href="#CO22-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Specify the host name of the Trustee route.
							</div></dd><dt><a href="#CO22-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								Specify <code class="literal">true</code> to enable Secure Boot by default.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Create the config map by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f peer-pods-cm.yaml</pre></li><li class="listitem"><p class="simpara">
						Restart the <code class="literal">peerpodconfig-ctrl-caa-daemon</code> daemon set by running the following command:
					</p><pre class="programlisting language-terminal">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</pre></li></ol></div></section><section class="section" id="deleting-cr-cli_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.5. Deleting the KataConfig custom resource</h2></div></div></div><p>
				You can delete the <code class="literal">KataConfig</code> custom resource (CR) by using the command line.
			</p><p>
				Deleting the <code class="literal">KataConfig</code> CR removes the runtime and its related resources from your cluster.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Deleting the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A larger OpenShift Container Platform deployment with a greater number of worker nodes.
						</li><li class="listitem">
							Activation of the BIOS and Diagnostics utility.
						</li><li class="listitem">
							Deployment on a hard drive rather than an SSD.
						</li><li class="listitem">
							Deployment on physical nodes such as bare metal, rather than on virtual nodes.
						</li><li class="listitem">
							A slow CPU and network.
						</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Delete the <code class="literal">KataConfig</code> CR by running the following command:
					</p><pre class="programlisting language-terminal">$ oc delete kataconfig example-kataconfig</pre><p class="simpara">
						The OpenShift sandboxed containers Operator removes all resources that were initially created to enable the runtime on your cluster.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							When you delete the <code class="literal">KataConfig</code> CR, the CLI stops responding until all worker nodes reboot. You must for the deletion process to complete before performing the verification.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Verify that the custom resource was deleted by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get kataconfig example-kataconfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">No example-kataconfig instances exist</pre>
						</p></div></li></ol></div></section><section class="section" id="creating-kataconfig-cr-cli_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.6. Re-creating the KataConfig custom resource</h2></div></div></div><p>
				You must re-create the <code class="literal">KataConfig</code> custom resource (CR) for Confidential Containers.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Creating the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A larger OpenShift Container Platform deployment with a greater number of worker nodes.
						</li><li class="listitem">
							Activation of the BIOS and Diagnostics utility.
						</li><li class="listitem">
							Deployment on a hard disk drive rather than an SSD.
						</li><li class="listitem">
							Deployment on physical nodes such as bare metal, rather than on virtual nodes.
						</li><li class="listitem">
							A slow CPU and network.
						</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create an <code class="literal">example-kataconfig.yaml</code> manifest file according to the following example:
					</p><pre class="programlisting language-yaml">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  enablePeerPods: true
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <span id="CO23-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO23-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Optional: If you have applied node labels to install <code class="literal">kata-remote</code> on specific nodes, specify the key and value, for example, <code class="literal">cc: 'true'</code>.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">KataConfig</code> CR by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f example-kataconfig.yaml</pre><p class="simpara">
						The new <code class="literal">KataConfig</code> CR is created and installs <code class="literal">kata-remote</code> as a runtime class on the worker nodes.
					</p><p class="simpara">
						Wait for the <code class="literal">kata-remote</code> installation to complete and the worker nodes to reboot before verifying the installation.
					</p></li><li class="listitem"><p class="simpara">
						Monitor the installation progress by running the following command:
					</p><pre class="programlisting language-terminal">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</pre><p class="simpara">
						When the status of all workers under <code class="literal">kataNodes</code> is <code class="literal">installed</code> and the condition <code class="literal">InProgress</code> is <code class="literal">False</code> without specifying a reason, the <code class="literal">kata-remote</code> is installed on the cluster.
					</p></li><li class="listitem"><p class="simpara">
						Verify the daemon set by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon</pre></li><li class="listitem"><p class="simpara">
						Verify the runtime classes by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get runtimeclass</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME             HANDLER          AGE
kata             kata             152m
kata-remote      kata-remote      152m</pre>
						</p></div></li></ol></div></section><section class="section" id="cc-creating-trustee-auth-secret_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.7. Creating the Trustee authentication secret</h2></div></div></div><p>
				You must create the authentication secret for Trustee.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a private key by running the following command:
					</p><pre class="programlisting language-terminal">$ openssl genpkey -algorithm ed25519 &gt; privateKey</pre></li><li class="listitem"><p class="simpara">
						Create a public key by running the following command:
					</p><pre class="programlisting language-terminal">$ openssl pkey -in privateKey -pubout -out publicKey</pre></li><li class="listitem"><p class="simpara">
						Create a secret by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create secret generic kbs-auth-public-key --from-file=publicKey -n trustee-operator-system</pre></li><li class="listitem"><p class="simpara">
						Verify the secret by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get secret -n trustee-operator-system</pre></li></ol></div></section><section class="section" id="cc-creating-trustee-config-map_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.8. Creating the Trustee config map</h2></div></div></div><p>
				You must create the config map to configure the Trustee server.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The following configuration example turns off security features to enable demonstration of Technology Preview features. It is not meant for a production environment.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created a route for Trustee.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">kbs-config-cm.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: kbs-config-cm
  namespace: trustee-operator-system
data:
  kbs-config.json: |
    {
      "insecure_http" : true,
      "sockets": ["0.0.0.0:8080"],
      "auth_public_key": "/etc/auth-secret/publicKey",
      "attestation_token_config": {
        "attestation_token_type": "CoCo"
      },
      "repository_config": {
        "type": "LocalFs",
        "dir_path": "/opt/confidential-containers/kbs/repository"
      },
      "as_config": {
        "work_dir": "/opt/confidential-containers/attestation-service",
        "policy_engine": "opa",
        "attestation_token_broker": "Simple",
          "attestation_token_config": {
          "duration_min": 5
          },
        "rvps_config": {
          "store_type": "LocalJson",
          "store_config": {
            "file_path": "/opt/confidential-containers/rvps/reference-values/reference-values.json"
          }
         }
      },
      "policy_engine_config": {
        "policy_path": "/opt/confidential-containers/opa/policy.rego"
      }
    }</pre></li><li class="listitem"><p class="simpara">
						Create the config map by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f kbs-config-cm.yaml</pre></li></ol></div></section><section class="section" id="configuring-trustee_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.9. Configuring Trustee values, policies, and secrets</h2></div></div></div><p>
				You can configure the following values, policies, and secrets for Trustee:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Optional: Reference values for the Reference Value Provider Service.
					</li><li class="listitem">
						Optional: Attestation policy.
					</li><li class="listitem">
						Provisioning Certificate Caching Service for Intel Trust Domain Extensions (TDX).
					</li><li class="listitem">
						Optional: Secret for custom keys for Trustee clients.
					</li><li class="listitem">
						Optional: Secret for container image signature verification.
					</li><li class="listitem">
						Container image signature verification policy. This policy is mandatory. If you do not use container image signature verification, you must create a policy that does not verify signatures.
					</li><li class="listitem">
						Resource access policy.
					</li></ul></div><section class="section" id="cc-configuring-reference-values_azure-cc"><div class="titlepage"><div><div><h3 class="title">5.9.1. Configuring reference values</h3></div></div></div><p>
					You can configure reference values for the Reference Value Provider Service (RVPS) by specifying the trusted digests of your hardware platform.
				</p><p>
					The client collects measurements from the running software, the Trusted Execution Environment (TEE) hardware and firmware and it submits a quote with the claims to the Attestation Server. These measurements must match the trusted digests registered to the Trustee. This process ensures that the confidential VM (CVM) is running the expected software stack and has not been tampered with.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">rvps-configmap.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: rvps-reference-values
  namespace: trustee-operator-system
data:
  reference-values.json: |
    [ <span id="CO24-1"/><span class="callout">1</span>
    ]</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO24-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the trusted digests for your hardware platform if required. Otherwise, leave it empty.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the RVPS config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f rvps-configmap.yaml</pre></li></ol></div></section><section class="section" id="cc-creating-attestation-policy_azure-cc"><div class="titlepage"><div><div><h3 class="title">5.9.2. Creating an attestation policy</h3></div></div></div><p>
					You can create an attestation policy that overrides the default attestation policy.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">attestation-policy.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: attestation-policy
  namespace: trustee-operator-system
data:
  default.rego: |
    package policy <span id="CO25-1"/><span class="callout">1</span>
    import future.keywords.every

    default allow = false

    allow {
      every k, v in input {
          judge_field(k, v)
      }
    }

    judge_field(input_key, input_value) {
      has_key(data.reference, input_key)
      reference_value := data.reference[input_key]
      match_value(reference_value, input_value)
    }

    judge_field(input_key, input_value) {
      not has_key(data.reference, input_key)
    }

    match_value(reference_value, input_value) {
      not is_array(reference_value)
      input_value == reference_value
    }

    match_value(reference_value, input_value) {
      is_array(reference_value)
      array_include(reference_value, input_value)
    }

    array_include(reference_value_array, input_value) {
      reference_value_array == []
    }

    array_include(reference_value_array, input_value) {
      reference_value_array != []
      some i
      reference_value_array[i] == input_value
    }

    has_key(m, k) {
      _ = m[k]
    }</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO25-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The attestation policy follows the <a class="link" href="https://www.openpolicyagent.org/docs/latest/policy-language/">Open Policy Agent</a> specification. In this example, the attestation policy compares the claims provided in the attestation report to the reference values registered in the RVPS database. The attestation process is successful only if all the values match.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the attestation policy config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f attestation-policy.yaml</pre></li></ol></div></section><section class="section" id="cc-configuring-pccs-for-tdx_azure-cc"><div class="titlepage"><div><div><h3 class="title">5.9.3. Configuring PCCS for TDX</h3></div></div></div><p>
					If you use Intel Trust Domain Extensions (TDX), you must configure Trustee to use the Provisioning Certificate Caching Service (PCCS).
				</p><p>
					The PCCS retrieves the Provisioning Certification Key (PCK) certificates and caches them in a local database.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not use the public Intel PCCS service. Use a local caching service on-premise or on the public cloud.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">tdx-config.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: tdx-config
  namespace: trustee-operator-system
data:
  sgx_default_qcnl.conf: | \
      {
        "collateral_service": "https://api.trustedservices.intel.com/sgx/certification/v4/",
        "pccs_url": "&lt;pccs_url&gt;" <span id="CO26-1"/><span class="callout">1</span>
      }</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO26-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the PCCS URL, for example, <code class="literal">https://localhost:8081/sgx/certification/v4/</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the TDX config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f tdx-config.yaml</pre></li></ol></div></section><section class="section" id="cc-creating-secret-for-clients_azure-cc"><div class="titlepage"><div><div><h3 class="title">5.9.4. Creating a secret with custom keys for clients</h3></div></div></div><p>
					You can create a secret that contains one or more custom keys for Trustee clients.
				</p><p>
					In this example, the <code class="literal">kbsres1</code> secret has two entries (<code class="literal">key1</code>, <code class="literal">key2</code>), which the clients retrieve. You can add additional secrets according to your requirements by using the same format.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created one or more custom keys.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Create a secret for the custom keys according to the following example:
						</p><pre class="programlisting language-terminal">$ oc apply secret generic kbsres1 \
  --from-literal key1=&lt;custom_key1&gt; \ <span id="CO27-1"/><span class="callout">1</span>
  --from-literal key2=&lt;custom_key2&gt; \
  -n trustee-operator-system</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO27-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a custom key.
								</div></dd></dl></div><p class="simpara">
							The <code class="literal">kbsres1</code> secret is specified in the <code class="literal">spec.kbsSecretResources</code> key of the <code class="literal">KbsConfig</code> custom resource.
						</p></li></ul></div></section><section class="section" id="cc-creating-secret-signed-container-images_azure-cc"><div class="titlepage"><div><div><h3 class="title">5.9.5. Creating a secret for container image signature verification</h3></div></div></div><p>
					If you use container image signature verification, you must create a secret that contains the public container image signing key.
				</p><p>
					The Key Broker Service on the Trustee cluster uses the secret to verify the signature, ensuring that only trusted and authenticated container images are deployed in your environment.
				</p><p>
					You can use <a class="link" href="https://developers.redhat.com/products/trusted-artifact-signer/overview">Red Hat Trusted Artifact Signer</a> or other tools to sign container images.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a secret for container image signature verification by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply secret generic &lt;type&gt; \ <span id="CO28-1"/><span class="callout">1</span>
  --from-file=&lt;tag&gt;=./&lt;public_key_file&gt; \ <span id="CO28-2"/><span class="callout">2</span>
  -n trustee-operator-system</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO28-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the KBS secret type, for example, <code class="literal">img-sig</code>.
								</div></dd><dt><a href="#CO28-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the secret tag, for example, <code class="literal">pub-key</code>, and the public container image signing key.
								</div></dd></dl></div></li><li class="listitem">
							Record the <code class="literal">&lt;type&gt;</code> value. You must add this value to the <code class="literal">spec.kbsSecretResources</code> key when you create the <code class="literal">KbsConfig</code> custom resource.
						</li></ol></div></section><section class="section" id="cc-creating-container-image-sig-policy_azure-cc"><div class="titlepage"><div><div><h3 class="title">5.9.6. Creating the container image signature verification policy</h3></div></div></div><p>
					You create the container image signature verification policy because signature verification is always enabled. If this policy is missing, the pods will not start.
				</p><p>
					If you are not using container image signature verification, you create the policy without signature verification.
				</p><p>
					For more information, see <a class="link" href="https://github.com/containers/image/blob/main/docs/containers-policy.json.5.md">containers-policy.json 5</a>.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">security-policy-config.json</code> file according to the following examples:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Without signature verification:
								</p><pre class="programlisting language-json">{
  "default": [
  {
    "type": "insecureAcceptAnything"
  }],
  "transports": {}
}</pre></li><li class="listitem"><p class="simpara">
									With signature verification:
								</p><pre class="programlisting language-json">{
  "default": [
      {
      "type": "insecureAcceptAnything"
      }
  ],
  "transports": {
      "&lt;transport&gt;": { <span id="CO29-1"/><span class="callout">1</span>
          "&lt;registry&gt;/&lt;image&gt;": <span id="CO29-2"/><span class="callout">2</span>
          [
              {
                  "type": "sigstoreSigned",
                  "keyPath": "kbs:///default/&lt;type&gt;/&lt;tag&gt;" <span id="CO29-3"/><span class="callout">3</span>
              }
          ]
      }
  }
}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO29-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the image repository for <code class="literal">transport</code>, for example, <code class="literal">"docker":</code>. For more information, see <a class="link" href="https://github.com/containers/image/blob/main/docs/containers-transports.5.md">containers-transports 5</a>.
										</div></dd><dt><a href="#CO29-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify the container registry and image, for example, "quay.io/my-image".
										</div></dd><dt><a href="#CO29-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specify the type and tag of the container image signature verification secret that you created, for example, <code class="literal">img-sig/pub-key</code>.
										</div></dd></dl></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Create the security policy by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply secret generic security-policy \
  --from-file=osc=./&lt;security-policy-config.json&gt; \
  -n trustee-operator-system</pre><p class="simpara">
							Do not alter the secret type, <code class="literal">security-policy</code>, or the key, <code class="literal">osc</code>.
						</p><p class="simpara">
							The <code class="literal">security-policy</code> secret is specified in the <code class="literal">spec.kbsSecretResources</code> key of the <code class="literal">KbsConfig</code> custom resource.
						</p></li></ol></div></section><section class="section" id="cc-creating-resource-access-policy_azure-cc"><div class="titlepage"><div><div><h3 class="title">5.9.7. Creating the resource access policy</h3></div></div></div><p>
					You configure the resource access policy for the Trustee policy engine. This policy determines which resources Trustee can access.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The Trustee policy engine is different from the Attestation Service policy engine, which determines the validity of TEE evidence.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">resourcepolicy-configmap.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: resource-policy
  namespace: trustee-operator-system
data:
  policy.rego: | <span id="CO30-1"/><span class="callout">1</span>
    package policy <span id="CO30-2"/><span class="callout">2</span>
    default allow = false
    allow {
      input["tee"] != "sample"
    }</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO30-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the resource policy, <code class="literal">policy.rego</code>, must match the resource policy defined in the Trustee config map.
								</div></dd><dt><a href="#CO30-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The resource policy follows the <a class="link" href="https://www.openpolicyagent.org/docs/latest/policy-language/">Open Policy Agent</a> specification. This example allows the retrieval of all resources when the TEE is not the sample attester.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the resource policy config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f resourcepolicy-configmap.yaml</pre></li></ol></div></section></section><section class="section" id="cc-creating-kbsconfig-cr_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.10. Creating the KbsConfig custom resource</h2></div></div></div><p>
				You create the <code class="literal">KbsConfig</code> custom resource (CR) to launch Trustee.
			</p><p>
				Then, you check the Trustee pods and pod logs to verify the configuration.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">kbsconfig-cr.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: confidentialcontainers.org/v1alpha1
kind: KbsConfig
metadata:
  labels:
    app.kubernetes.io/name: kbsconfig
    app.kubernetes.io/instance: kbsconfig
    app.kubernetes.io/part-of: trustee-operator
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/created-by: trustee-operator
  name: kbsconfig
  namespace: trustee-operator-system
spec:
  kbsConfigMapName: kbs-config-cm
  kbsAuthSecretName: kbs-auth-public-key
  kbsDeploymentType: AllInOneDeployment
  kbsRvpsRefValuesConfigMapName: rvps-reference-values
  kbsSecretResources: ["kbsres1", "security-policy", "&lt;type&gt;"] <span id="CO31-1"/><span class="callout">1</span>
  kbsResourcePolicyConfigMapName: resource-policy
# tdxConfigSpec:
#   kbsTdxConfigMapName: tdx-config <span id="CO31-2"/><span class="callout">2</span>
# kbsAttestationPolicyConfigMapName: attestation-policy <span id="CO31-3"/><span class="callout">3</span>
# kbsServiceType: &lt;service_type&gt; <span id="CO31-4"/><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO31-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">type</code> value of the container image signature verification secret you created, for example, <code class="literal">img-sig</code>.
							</div></dd><dt><a href="#CO31-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								<code class="literal">tdxConfigSpec.kbsTdxConfigMapName: tdx-config</code> is required for Intel Trust Domain Extensions.
							</div></dd><dt><a href="#CO31-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								<code class="literal">kbsAttestationPolicyConfigMapName: attestation-policy</code> is required if you create a customized attestation policy.
							</div></dd><dt><a href="#CO31-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								<code class="literal">kbsServiceType: &lt;service_type&gt;</code> is required if you created a service type. Specify <code class="literal">NodePort</code>, <code class="literal">LoadBalancer</code>, or <code class="literal">ExternalName</code>. The default service type is <code class="literal">ClusterIP</code>.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">KbsConfig</code> CR by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f kbsconfig-cr.yaml</pre></li></ol></div></section><section class="section" id="cc-verifing-trustee-config_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.11. Verifying the Trustee configuration</h2></div></div></div><p>
				You verify the Trustee configuration by checking the Trustee pods and logs.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Set the default project by running the following command:
					</p><pre class="programlisting language-terminal">$ oc project trustee-operator-system</pre></li><li class="listitem"><p class="simpara">
						Check the Trustee pods by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get pods -n trustee-operator-system</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">NAME                                                   READY   STATUS    RESTARTS   AGE
trustee-deployment-8585f98449-9bbgl                    1/1     Running   0          22m
trustee-operator-controller-manager-5fbd44cd97-55dlh   2/2     Running   0          59m</pre>
						</p></div></li><li class="listitem"><p class="simpara">
						Set the <code class="literal">POD_NAME</code> environmental variable by running the following command:
					</p><pre class="programlisting language-terminal">$ POD_NAME=$(oc get pods -l app=kbs -o jsonpath='{.items[0].metadata.name}' -n trustee-operator-system)</pre></li><li class="listitem"><p class="simpara">
						Check the pod logs by running the following command:
					</p><pre class="programlisting language-terminal">$ oc logs -n trustee-operator-system $POD_NAME</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">[2024-05-30T13:44:24Z INFO  kbs] Using config file /etc/kbs-config/kbs-config.json
[2024-05-30T13:44:24Z WARN  attestation_service::rvps] No RVPS address provided and will launch a built-in rvps
[2024-05-30T13:44:24Z INFO  attestation_service::token::simple] No Token Signer key in config file, create an ephemeral key and without CA pubkey cert
[2024-05-30T13:44:24Z INFO  api_server] Starting HTTPS server at [0.0.0.0:8080]
[2024-05-30T13:44:24Z INFO  actix_server::builder] starting 12 workers
[2024-05-30T13:44:24Z INFO  actix_server::server] Tokio runtime found; starting in existing Tokio runtime</pre>
						</p></div></li></ol></div></section><section class="section" id="verifying-attestation-process_azure-cc"><div class="titlepage"><div><div><h2 class="title">5.12. Verifying the attestation process</h2></div></div></div><p>
				You verify the attestation process by creating a test pod and retrieving its secret.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					This procedure is an example to verify that attestation is working. Do not write sensitive data to standard I/O because the data can be captured by using a memory dump. Only data written to memory is encrypted.
				</p></div></div><p>
				By default, an agent side policy embedded in the pod VM image disables the <code class="literal">exec</code> and <code class="literal">log</code> APIs for a Confidential Containers pod. This policy ensures that sensitive data is not written to standard I/O.
			</p><p>
				In a test scenario, you can override the restriction at runtime by adding a policy annotation to the pod. For Technology Preview, runtime policy annotations are not verified by remote attestation.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created a route if the Trustee server and the test pod are not running in the same cluster.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">verification-pod.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: ocp-cc-pod
  labels:
    app: ocp-cc-pod
  annotations:
    io.katacontainers.config.agent.policy: cGFja2FnZSBhZ2VudF9wb2xpY3kKCmRlZmF1bHQgQWRkQVJQTmVpZ2hib3JzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgQWRkU3dhcFJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENsb3NlU3RkaW5SZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBDb3B5RmlsZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENyZWF0ZUNvbnRhaW5lclJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IENyZWF0ZVNhbmRib3hSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBEZXN0cm95U2FuZGJveFJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IEV4ZWNQcm9jZXNzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgR2V0TWV0cmljc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IEdldE9PTUV2ZW50UmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgR3Vlc3REZXRhaWxzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgTGlzdEludGVyZmFjZXNSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBMaXN0Um91dGVzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgTWVtSG90cGx1Z0J5UHJvYmVSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBPbmxpbmVDUFVNZW1SZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBQYXVzZUNvbnRhaW5lclJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFB1bGxJbWFnZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFJlYWRTdHJlYW1SZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZW1vdmVDb250YWluZXJSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZW1vdmVTdGFsZVZpcnRpb2ZzU2hhcmVNb3VudHNSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZXNlZWRSYW5kb21EZXZSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBSZXN1bWVDb250YWluZXJSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBTZXRHdWVzdERhdGVUaW1lUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU2V0UG9saWN5UmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU2lnbmFsUHJvY2Vzc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFN0YXJ0Q29udGFpbmVyUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU3RhcnRUcmFjaW5nUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgU3RhdHNDb250YWluZXJSZXF1ZXN0IDo9IHRydWUKZGVmYXVsdCBTdG9wVHJhY2luZ1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFR0eVdpblJlc2l6ZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZUNvbnRhaW5lclJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZUVwaGVtZXJhbE1vdW50c1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZUludGVyZmFjZVJlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFVwZGF0ZVJvdXRlc1JlcXVlc3QgOj0gdHJ1ZQpkZWZhdWx0IFdhaXRQcm9jZXNzUmVxdWVzdCA6PSB0cnVlCmRlZmF1bHQgV3JpdGVTdHJlYW1SZXF1ZXN0IDo9IHRydWUK <span id="CO32-1"/><span class="callout">1</span>
spec:
  runtimeClassName: kata-remote
  containers:
    - name: skr-openshift
      image: registry.access.redhat.com/ubi9/ubi:9.3
      command:
        - sleep
        - "36000"
      securityContext:
        privileged: false
        seccompProfile:
          type: RuntimeDefault</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO32-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								This pod annotation overrides the policy that prevents sensitive data from being written to standard I/O.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Create the pod by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create -f verification-pod.yaml</pre></li><li class="listitem"><p class="simpara">
						Connect to the Bash shell of the <code class="literal">ocp-cc-pod</code> by running the following command:
					</p><pre class="programlisting language-terminal">$ oc exec -it ocp-cc-pod -- bash</pre></li><li class="listitem"><p class="simpara">
						Fetch the pod secret by running the following command:
					</p><pre class="programlisting language-terminal">$ curl http://127.0.0.1:8006/cdh/resource/default/kbsres1/key1</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">res1val1</pre>
						</p></div><p class="simpara">
						The Trustee server returns the secret only if the attestation is successful.
					</p></li></ol></div></section></section><section class="chapter" id="deploying-osc-ibm"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Deploying OpenShift sandboxed containers on IBM Z and IBM LinuxONE</h1></div></div></div><p>
			You can deploy OpenShift sandboxed containers on IBM Z® and IBM® LinuxONE.
		</p><p>
			OpenShift sandboxed containers deploys peer pods. The peer pod design circumvents the need for nested virtualization. For more information, see <a class="link" href="#peer-pods">peer pods</a>.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				OpenShift sandboxed containers on IBM Z® and IBM® LinuxONE is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
			</p><p>
				For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
			</p></div></div><div class="itemizedlist"><p class="title"><strong>Cluster requirements</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
					You have installed Red Hat OpenShift Container Platform 4.14 or later on the cluster where you are installing the OpenShift sandboxed containers Operator.
				</li><li class="listitem">
					Your cluster has at least one worker node.
				</li></ul></div><section class="section" id="peer-pod-resource-requirements_ibm-osc"><div class="titlepage"><div><div><h2 class="title">6.1. Peer pod resource requirements</h2></div></div></div><p>
				You must ensure that your cluster has sufficient resources.
			</p><p>
				Peer pod virtual machines (VMs) require resources in two locations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The worker node. The worker node stores metadata, Kata shim resources (<code class="literal">containerd-shim-kata-v2</code>), remote-hypervisor resources (<code class="literal">cloud-api-adaptor</code>), and the tunnel setup between the worker nodes and the peer pod VM.
					</li><li class="listitem">
						The libvirt virtual machine instance. This is the actual peer pod VM running in the LPAR (KVM host).
					</li></ul></div><p>
				The CPU and memory resources used in the Kubernetes worker node are handled by the <a class="link" href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-overhead/">pod overhead</a> included in the RuntimeClass (<code class="literal">kata-remote</code>) definition used for creating peer pods.
			</p><p>
				The total number of peer pod VMs running in the cloud is defined as Kubernetes Node extended resources. This limit is per node and is set by the <code class="literal">limit</code> attribute in the <code class="literal">peerpodConfig</code> custom resource (CR).
			</p><p>
				The <code class="literal">peerpodConfig</code> CR, named <code class="literal">peerpodconfig-openshift</code>, is created when you create the <code class="literal">kataConfig</code> CR and enable peer pods, and is located in the <code class="literal">openshift-sandboxed-containers-operator</code> namespace.
			</p><p>
				The following <code class="literal">peerpodConfig</code> CR example displays the default <code class="literal">spec</code> values:
			</p><pre class="programlisting language-yaml">apiVersion: confidentialcontainers.org/v1alpha1
kind: PeerPodConfig
metadata:
  name: peerpodconfig-openshift
  namespace: openshift-sandboxed-containers-operator
spec:
  cloudSecretName: peer-pods-secret
  configMapName: peer-pods-cm
  limit: "10" <span id="CO33-1"/><span class="callout">1</span>
  nodeSelector:
    node-role.kubernetes.io/kata-oc: ""</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO33-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						The default limit is 10 VMs per node.
					</div></dd></dl></div><p>
				The extended resource is named <code class="literal">kata.peerpods.io/vm</code>, and enables the Kubernetes scheduler to handle capacity tracking and accounting.
			</p><p>
				You can edit the limit per node based on the requirements for your environment after you install the OpenShift sandboxed containers Operator.
			</p><p>
				A <a class="link" href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">mutating webhook</a> adds the extended resource <code class="literal">kata.peerpods.io/vm</code> to the pod specification. It also removes any resource-specific entries from the pod specification, if present. This enables the Kubernetes scheduler to account for these extended resources, ensuring the peer pod is only scheduled when resources are available.
			</p><p>
				The mutating webhook modifies a Kubernetes pod as follows:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The mutating webhook checks the pod for the expected <code class="literal">RuntimeClassName</code> value, specified in the <code class="literal">TARGET_RUNTIME_CLASS</code> environment variable. If the value in the pod specification does not match the value in the <code class="literal">TARGET_RUNTIME_CLASS</code>, the webhook exits without modifying the pod.
					</li><li class="listitem"><p class="simpara">
						If the <code class="literal">RuntimeClassName</code> values match, the webhook makes the following changes to the pod spec:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								The webhook removes every resource specification from the <code class="literal">resources</code> field of all containers and init containers in the pod.
							</li><li class="listitem">
								The webhook adds the extended resource (<code class="literal">kata.peerpods.io/vm</code>) to the spec by modifying the resources field of the first container in the pod. The extended resource <code class="literal">kata.peerpods.io/vm</code> is used by the Kubernetes scheduler for accounting purposes.
							</li></ol></div></li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The mutating webhook excludes specific system namespaces in OpenShift Container Platform from mutation. If a peer pod is created in those system namespaces, then resource accounting using Kubernetes extended resources does not work unless the pod spec includes the extended resource.
				</p><p>
					As a best practice, define a cluster-wide policy to only allow peer pod creation in specific namespaces.
				</p></div></div></section><section class="section" id="deploying-osc-cli_ibm-osc"><div class="titlepage"><div><div><h2 class="title">6.2. Deploying OpenShift sandboxed containers on IBM Z and IBM LinuxONE</h2></div></div></div><p>
				You can deploy OpenShift sandboxed containers on IBM Z® and IBM® LinuxONE by using the command line interface (CLI) to perform the following tasks:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Install the OpenShift sandboxed containers Operator.
					</li><li class="listitem">
						Optional: Change the number of virtual machines running on each worker node.
					</li><li class="listitem">
						Configure the libvirt volume.
					</li><li class="listitem">
						Optional: Create a custom peer pod VM image.
					</li><li class="listitem">
						Create the peer pods secret.
					</li><li class="listitem">
						Create the peer pods config map.
					</li><li class="listitem">
						Create the peer pod VM image config map.
					</li><li class="listitem">
						Create the KVM host secret.
					</li><li class="listitem">
						Create the <code class="literal">KataConfig</code> custom resource.
					</li><li class="listitem">
						Configure the OpenShift sandboxed containers workload objects.
					</li></ol></div><section class="section" id="installing-operator-cli_ibm-osc"><div class="titlepage"><div><div><h3 class="title">6.2.1. Installing the OpenShift sandboxed containers Operator</h3></div></div></div><p>
					You can install the OpenShift sandboxed containers Operator by using the CLI.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-namespace.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sandboxed-containers-operator</pre></li><li class="listitem"><p class="simpara">
							Create the namespace by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-namespace.yaml</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-operatorgroup.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sandboxed-containers-operator-group
  namespace: openshift-sandboxed-containers-operator
spec:
  targetNamespaces:
  - openshift-sandboxed-containers-operator</pre></li><li class="listitem"><p class="simpara">
							Create the operator group by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-operatorgroup.yaml</pre></li><li class="listitem"><p class="simpara">
							Create an <code class="literal">osc-subscription.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sandboxed-containers-operator
  namespace: openshift-sandboxed-containers-operator
spec:
  channel: stable
  installPlanApproval: Automatic
  name: sandboxed-containers-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: sandboxed-containers-operator.v1.8.0</pre></li><li class="listitem"><p class="simpara">
							Create the subscription by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f osc-subscription.yaml</pre></li><li class="listitem"><p class="simpara">
							Verify that the Operator is correctly installed by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get csv -n openshift-sandboxed-containers-operator</pre><p class="simpara">
							This command can take several minutes to complete.
						</p></li><li class="listitem"><p class="simpara">
							Watch the process by running the following command:
						</p><pre class="programlisting language-terminal">$ watch oc get csv -n openshift-sandboxed-containers-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME                             DISPLAY                                  VERSION             REPLACES                   PHASE
openshift-sandboxed-containers   openshift-sandboxed-containers-operator  1.8.0    1.7.0        Succeeded</pre>
							</p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</a>.
						</li><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-configuring-proxy-support.html">Configuring proxy support in Operator Lifecycle Manager</a> for disconnected environments.
						</li></ul></div></section><section class="section" id="modifying-peer-pod-vm-limit_ibm-osc"><div class="titlepage"><div><div><h3 class="title">6.2.2. Modifying the number of peer pod VMs per node</h3></div></div></div><p>
					You can change the limit of peer pod virtual machines (VMs) per node by editing the <code class="literal">peerpodConfig</code> custom resource (CR).
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the current limit by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
-o jsonpath='{.spec.limit}{"\n"}'</pre></li><li class="listitem"><p class="simpara">
							Modify the <code class="literal">limit</code> attribute of the <code class="literal">peerpodConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc patch peerpodconfig peerpodconfig-openshift -n openshift-sandboxed-containers-operator \
--type merge --patch '{"spec":{"limit":"&lt;value&gt;"}}' <span id="CO34-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO34-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace &lt;value&gt; with the limit you want to define.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="ibm-configuring-libvirt_ibm-osc"><div class="titlepage"><div><div><h3 class="title">6.2.3. Configuring the libvirt volume</h3></div></div></div><p>
					You must configure the libvirt volume on your KVM host. Peer pods use the libvirt provider of the Cloud API Adaptor to create and manage virtual machines.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift sandboxed containers Operator on your OpenShift Container Platform cluster by using the OpenShift Container Platform web console or the command line.
						</li><li class="listitem">
							You have administrator privileges for your KVM host.
						</li><li class="listitem">
							You have installed <code class="literal">podman</code> on your KVM host.
						</li><li class="listitem">
							You have installed <code class="literal">virt-customize</code> on your KVM host.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the KVM host.
						</li><li class="listitem"><p class="simpara">
							Set the name of the libvirt pool by running the following command:
						</p><pre class="programlisting language-terminal">$ export LIBVIRT_POOL=&lt;libvirt_pool&gt;</pre><p class="simpara">
							You need the <code class="literal">LIBVIRT_POOL</code> value to create the secret for the libvirt provider.
						</p></li><li class="listitem"><p class="simpara">
							Set the name of the libvirt pool by running the following command:
						</p><pre class="programlisting language-terminal">$ export LIBVIRT_VOL_NAME=&lt;libvirt_volume&gt;</pre><p class="simpara">
							You need the <code class="literal">LIBVIRT_VOL_NAME</code> value to create the secret for the libvirt provider.
						</p></li><li class="listitem"><p class="simpara">
							Set the path of the default storage pool location, by running the following command:
						</p><pre class="programlisting language-terminal">$ export LIBVIRT_POOL_DIRECTORY=&lt;target_directory&gt; <span id="CO35-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO35-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									To ensure libvirt has read and write access permissions, use a subdirectory of the libvirt storage directory. The default is <code class="literal">/var/lib/libvirt/images/</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create a libvirt pool by running the following command:
						</p><pre class="programlisting language-terminal">$ virsh pool-define-as $LIBVIRT_POOL --type dir --target "$LIBVIRT_POOL_DIRECTORY"</pre></li><li class="listitem"><p class="simpara">
							Start the libvirt pool by running the following command:
						</p><pre class="programlisting language-terminal">$ virsh pool-start $LIBVIRT_POOL</pre></li><li class="listitem"><p class="simpara">
							Create a libvirt volume for the pool by running the following command:
						</p><pre class="programlisting language-terminal">$ virsh -c qemu:///system \
  vol-create-as --pool $LIBVIRT_POOL \
  --name $LIBVIRT_VOL_NAME \
  --capacity 20G \
  --allocation 2G \
  --prealloc-metadata \
  --format qcow2</pre></li></ol></div></section><section class="section" id="ibm-embedding-podvm-image_ibm-osc"><div class="titlepage"><div><div><h3 class="title">6.2.4. Creating a custom peer pod VM image</h3></div></div></div><p>
					You can create a custom peer pod virtual machine (VM) image instead of using the default Operator-built image.
				</p><p>
					You build an Open Container Initiative (OCI) container with the peer pod QCOW2 image. Later, you add the container registry URL and the image path to the peer pod VM image config map.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">Dockerfile.podvm-oci</code> file:
						</p><pre class="programlisting language-docker">FROM scratch

ARG PODVM_IMAGE_SRC
ENV PODVM_IMAGE_PATH="/image/podvm.qcow2"

COPY $PODVM_IMAGE_SRC $PODVM_IMAGE_PATH</pre></li><li class="listitem"><p class="simpara">
							Build a container with the pod VM QCOW2 image by running the following command:
						</p><pre class="programlisting language-terminal">$ docker build -t podvm-libvirt \
  --build-arg PODVM_IMAGE_SRC=&lt;podvm_image_source&gt; \ <span id="CO36-1"/><span class="callout">1</span>
  --build-arg PODVM_IMAGE_PATH=&lt;podvm_image_path&gt; \ <span id="CO36-2"/><span class="callout">2</span>
  -f Dockerfile.podvm-oci .</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO36-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the QCOW2 image source on the host.
								</div></dd><dt><a href="#CO36-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Optional: Specify the path of the QCOW2 image if you do not use the default, <code class="literal">/image/podvm.qcow2</code>.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="creating-peer-pods-secret_ibm-osc"><div class="titlepage"><div><div><h3 class="title">6.2.5. Creating the peer pods secret</h3></div></div></div><p>
					You must create the peer pods secret for OpenShift sandboxed containers.
				</p><p>
					The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.
				</p><p>
					By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">LIBVIRT_POOL</code>. Use the value you set when you configured libvirt on the KVM host.
						</li><li class="listitem">
							<code class="literal">LIBVIRT_VOL_NAME</code>. Use the value you set when you configured libvirt on the KVM host.
						</li><li class="listitem"><p class="simpara">
							<code class="literal">LIBVIRT_URI</code>. This value is the default gateway IP address of the libvirt network. Check your libvirt network setup to obtain this value.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If libvirt uses the default bridge virtual network, you can obtain the <code class="literal">LIBVIRT_URI</code> by running the following commands:
							</p><pre class="programlisting language-terminal">$ virtint=$(bridge_line=$(virsh net-info default | grep Bridge);  echo "${bridge_line//Bridge:/}" | tr -d [:blank:])

$ LIBVIRT_URI=$( ip -4 addr show $virtint | grep -oP '(?&lt;=inet\s)\d+(\.\d+){3}')

$ LIBVIRT_GATEWAY_URI="qemu+ssh://root@${LIBVIRT_URI}/system?no_verify=1"</pre></div></div></li><li class="listitem">
							<code class="literal">REDHAT_OFFLINE_TOKEN</code>. You have generated this token to download the RHEL image at <a class="link" href="https://access.redhat.com/management/api">Red Hat API Tokens</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">peer-pods-secret.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  CLOUD_PROVIDER: "libvirt"
  LIBVIRT_URI: "&lt;libvirt_gateway_uri&gt;" <span id="CO37-1"/><span class="callout">1</span>
  LIBVIRT_POOL: "&lt;libvirt_pool&gt;" <span id="CO37-2"/><span class="callout">2</span>
  LIBVIRT_VOL_NAME: "&lt;libvirt_volume&gt;" <span id="CO37-3"/><span class="callout">3</span>
  REDHAT_OFFLINE_TOKEN: "&lt;rh_offline_token&gt;" <span id="CO37-4"/><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO37-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the libvirt URI.
								</div></dd><dt><a href="#CO37-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the libvirt pool.
								</div></dd><dt><a href="#CO37-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify the libvirt volume name.
								</div></dd><dt><a href="#CO37-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify the Red Hat offline token, which is required for the Operator-built image.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the secret by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f peer-pods-secret.yaml</pre></li><li class="listitem"><p class="simpara">
							Optional: To update an existing peer pods config map, restart the <code class="literal">peerpodconfig-ctrl-caa-daemon</code> daemon set by running the following command:
						</p><pre class="programlisting language-terminal">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</pre></li></ol></div></section><section class="section" id="creating-peer-pods-config-map_ibm-osc"><div class="titlepage"><div><div><h3 class="title">6.2.6. Creating the peer pods config map</h3></div></div></div><p>
					You must create the peer pods config map for OpenShift sandboxed containers.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">peer-pods-cm.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "libvirt"
  DISABLECVM: "true"</pre></li><li class="listitem"><p class="simpara">
							Create the config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f peer-pods-cm.yaml</pre></li><li class="listitem"><p class="simpara">
							Optional: To update an existing peer pods config map, restart the <code class="literal">peerpodconfig-ctrl-caa-daemon</code> daemon set by running the following command:
						</p><pre class="programlisting language-terminal">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</pre></li></ol></div></section><section class="section" id="creating-libvirt-config-map_ibm-osc"><div class="titlepage"><div><div><h3 class="title">6.2.7. Creating the peer pod VM image config map</h3></div></div></div><p>
					You must create the config map for the peer pod VM image.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">libvirt-podvm-image-cm.yaml</code> manifest according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: libvirt-podvm-image-cm
  namespace: openshift-sandboxed-containers-operator
data:
  PODVM_DISTRO: "rhel"
  CAA_SRC: "https://github.com/confidential-containers/cloud-api-adaptor"
  CAA_REF: "&lt;cloud_api_adaptor_version&gt;" <span id="CO38-1"/><span class="callout">1</span>
  DOWNLOAD_SOURCES: "no"
  CONFIDENTIAL_COMPUTE_ENABLED: "yes"
  UPDATE_PEERPODS_CM: "yes"
  ORG_ID: "&lt;rhel_organization_id&gt;"
  ACTIVATION_KEY: "&lt;rhel_activation_key&gt;" <span id="CO38-2"/><span class="callout">2</span>
  IMAGE_NAME: "&lt;podvm_libvirt_image&gt;"
  PODVM_IMAGE_URI: "oci::&lt;image_repo_url&gt;:&lt;image_tag&gt;::&lt;image_path&gt;" <span id="CO38-3"/><span class="callout">3</span>
  SE_BOOT: "true" <span id="CO38-4"/><span class="callout">4</span>
  BASE_OS_VERSION: "&lt;rhel_image_os_version&gt;" <span id="CO38-5"/><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO38-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the latest version of the Cloud API Adaptor source.
								</div></dd><dt><a href="#CO38-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify your RHEL activation key.
								</div></dd><dt><a href="#CO38-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Optional: Specify the following values if you created a container image:
								</div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">image_repo_url</code>: Container registry URL.
										</li><li class="listitem">
											<code class="literal">image_tag</code>: Image tag.
										</li><li class="listitem">
											<code class="literal">image_path</code>: Image path. Default: <code class="literal">/image/podvm.qcow2</code>.
										</li></ul></div></dd><dt><a href="#CO38-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									<code class="literal">SE_BOOT: "true"</code> enables IBM Secure Execution for an Operator-built image. Set to <code class="literal">false</code> if you created a container image.
								</div></dd><dt><a href="#CO38-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specify the RHEL image operating system version. IBM Z® Secure Execution supports RHEL 9.4 and later versions.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f libvirt-podvm-image-cm.yaml</pre><p class="simpara">
							The libvirt pod VM image config map is created for your libvirt provider.
						</p></li></ol></div></section><section class="section" id="creating-ssh-key-secret_ibm-osc"><div class="titlepage"><div><div><h3 class="title">6.2.8. Creating the KVM host secret</h3></div></div></div><p>
					You must create the secret for your KVM host.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to your OpenShift Container Platform cluster.
						</li><li class="listitem"><p class="simpara">
							Generate an SSH key pair by running the following command:
						</p><pre class="programlisting language-terminal">$ ssh-keygen -f ./id_rsa -N ""</pre></li><li class="listitem"><p class="simpara">
							Copy the public SSH key to your KVM host:
						</p><pre class="programlisting language-terminal">$ ssh-copy-id -i ./id_rsa.pub &lt;KVM_HOST_IP&gt;</pre></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">Secret</code> object by running the following command:
						</p><pre class="programlisting language-terminal">$ oc create secret generic ssh-key-secret \
  -n openshift-sandboxed-containers-operator \
  --from-file=id_rsa.pub=./id_rsa.pub \
  --from-file=id_rsa=./id_rsa</pre></li><li class="listitem"><p class="simpara">
							Delete the SSH keys you created:
						</p><pre class="programlisting language-terminal">$ shred --remove id_rsa.pub id_rsa</pre></li></ol></div></section><section class="section" id="creating-kataconfig-cr-cli_ibm-osc"><div class="titlepage"><div><div><h3 class="title">6.2.9. Creating the KataConfig custom resource</h3></div></div></div><p>
					You must create the <code class="literal">KataConfig</code> custom resource (CR) to install <code class="literal">kata-remote</code> as a runtime class on your worker nodes.
				</p><p>
					Creating the <code class="literal">KataConfig</code> CR triggers the OpenShift sandboxed containers Operator to do the following: * Create a <code class="literal">RuntimeClass</code> CR named <code class="literal">kata-remote</code> with a default configuration. This enables users to configure workloads to use <code class="literal">kata-remote</code> as the runtime by referencing the CR in the <code class="literal">RuntimeClassName</code> field. This CR also specifies the resource overhead for the runtime.
				</p><p>
					OpenShift sandboxed containers installs <code class="literal">kata-remote</code> as a <span class="emphasis"><em>secondary, optional</em></span> runtime on the cluster and not as the primary runtime.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Creating the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								A larger OpenShift Container Platform deployment with a greater number of worker nodes.
							</li><li class="listitem">
								Activation of the BIOS and Diagnostics utility.
							</li><li class="listitem">
								Deployment on a hard disk drive rather than an SSD.
							</li><li class="listitem">
								Deployment on physical nodes such as bare metal, rather than on virtual nodes.
							</li><li class="listitem">
								A slow CPU and network.
							</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">example-kataconfig.yaml</code> manifest file according to the following example:
						</p><pre class="programlisting language-yaml">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  enablePeerPods: true
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <span id="CO39-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO39-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Optional: If you have applied node labels to install <code class="literal">kata-remote</code> on specific nodes, specify the key and value, for example, <code class="literal">osc: 'true'</code>.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">KataConfig</code> CR by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f example-kataconfig.yaml</pre><p class="simpara">
							The new <code class="literal">KataConfig</code> CR is created and installs <code class="literal">kata-remote</code> as a runtime class on the worker nodes.
						</p><p class="simpara">
							Wait for the <code class="literal">kata-remote</code> installation to complete and the worker nodes to reboot before verifying the installation.
						</p></li><li class="listitem"><p class="simpara">
							Monitor the installation progress by running the following command:
						</p><pre class="programlisting language-terminal">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</pre><p class="simpara">
							When the status of all workers under <code class="literal">kataNodes</code> is <code class="literal">installed</code> and the condition <code class="literal">InProgress</code> is <code class="literal">False</code> without specifying a reason, the <code class="literal">kata-remote</code> is installed on the cluster.
						</p></li><li class="listitem"><p class="simpara">
							Verify that you have built the peer pod image and uploaded it to the libvirt volume by running the following command:
						</p><pre class="programlisting language-terminal">$ oc describe configmap peer-pods-cm -n openshift-sandboxed-containers-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-text">Name: peer-pods-cm
Namespace: openshift-sandboxed-containers-operator
Labels: &lt;none&gt;
Annotations: &lt;none&gt;

Data
====
CLOUD_PROVIDER: libvirt

BinaryData
====
Events: &lt;none&gt;</pre>
							</p></div></li><li class="listitem"><p class="simpara">
							Monitor the <code class="literal">kata-oc</code> machine config pool progress to ensure that it is in the <code class="literal">UPDATED</code> state, when <code class="literal">UPDATEDMACHINECOUNT</code> equals <code class="literal">MACHINECOUNT</code>, by running the following command:
						</p><pre class="programlisting language-terminal">$ watch oc get mcp/kata-oc</pre></li><li class="listitem"><p class="simpara">
							Verify the daemon set by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon</pre></li><li class="listitem"><p class="simpara">
							Verify the runtime classes by running the following command:
						</p><pre class="programlisting language-terminal">$ oc get runtimeclass</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME             HANDLER          AGE
kata             kata             152m
kata-remote      kata-remote      152m</pre>
							</p></div></li></ol></div></section><section class="section" id="configuring-workload-objects_ibm-osc"><div class="titlepage"><div><div><h3 class="title">6.2.10. Configuring workload objects</h3></div></div></div><p>
					You must configure OpenShift sandboxed containers workload objects by setting <code class="literal">kata-remote</code> as the runtime class for the following pod-templated objects:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">Pod</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicaSet</code> objects
						</li><li class="listitem">
							<code class="literal">ReplicationController</code> objects
						</li><li class="listitem">
							<code class="literal">StatefulSet</code> objects
						</li><li class="listitem">
							<code class="literal">Deployment</code> objects
						</li><li class="listitem">
							<code class="literal">DeploymentConfig</code> objects
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Do not deploy workloads in an Operator namespace. Create a dedicated namespace for these resources.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created the <code class="literal">KataConfig</code> custom resource (CR).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add <code class="literal">spec.runtimeClassName: kata-remote</code> to the manifest of each pod-templated workload object as in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: &lt;object&gt;
# ...
spec:
  runtimeClassName: kata-remote
# ...</pre><p class="simpara">
							OpenShift Container Platform creates the workload object and begins scheduling it.
						</p></li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Inspect the <code class="literal">spec.runtimeClassName</code> field of a pod-templated object. If the value is <code class="literal">kata-remote</code>, then the workload is running on OpenShift sandboxed containers, using peer pods.
						</li></ul></div></section></section></section><section class="chapter" id="deploying-cc_ibm-cc"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Deploying Confidential Containers on IBM Z and IBM LinuxONE</h1></div></div></div><p>
			You can deploy Confidential Containers on IBM Z® and IBM® LinuxONE after you deploy OpenShift sandboxed containers.
		</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				Confidential Containers on IBM Z® and IBM® LinuxONE is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
			</p><p>
				For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
			</p></div></div><div class="itemizedlist"><p class="title"><strong>Cluster requirements</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
					You have installed Red Hat OpenShift Container Platform 4.15 or later on the cluster where you are installing the Confidential compute attestation Operator.
				</li></ul></div><p>
			You deploy Confidential Containers by performing the following steps:
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Install the Confidential compute attestation Operator.
				</li><li class="listitem">
					Create the route for Trustee.
				</li><li class="listitem">
					Enable the Confidential Containers feature gate.
				</li><li class="listitem">
					Update the peer pods config map.
				</li><li class="listitem">
					Delete the <code class="literal">KataConfig</code> custom resource (CR).
				</li><li class="listitem">
					Update the peer pods secret.
				</li><li class="listitem">
					Re-create the <code class="literal">KataConfig</code> CR.
				</li><li class="listitem">
					Create the Trustee authentication secret.
				</li><li class="listitem">
					Create the Trustee config map.
				</li><li class="listitem">
					Obtain the IBM Secure Execution (SE) header.
				</li><li class="listitem">
					Configure the SE certificates and keys.
				</li><li class="listitem">
					Create the persistent storage components.
				</li><li class="listitem">
					Configure Trustee values, policies, and secrets.
				</li><li class="listitem">
					Create the <code class="literal">KbsConfig</code> CR.
				</li><li class="listitem">
					Verify the Trustee configuration.
				</li><li class="listitem">
					Verify the attestation process.
				</li></ol></div><section class="section" id="cc-installing-cc-operator-cli_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.1. Installing the Confidential compute attestation Operator</h2></div></div></div><p>
				You can install the Confidential compute attestation Operator on IBM Z® and IBM® LinuxONE by using the CLI.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">trustee-namespace.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: trustee-operator-system</pre></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">trustee-operator-system</code> namespace by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f trustee-namespace.yaml</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">trustee-operatorgroup.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: trustee-operator-group
  namespace: trustee-operator-system
spec:
  targetNamespaces:
  - trustee-operator-system</pre></li><li class="listitem"><p class="simpara">
						Create the operator group by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f trustee-operatorgroup.yaml</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">trustee-subscription.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: trustee-operator
  namespace: trustee-operator-system
spec:
  channel: stable
  installPlanApproval: Automatic
  name: trustee-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: trustee-operator.v0.1.0</pre></li><li class="listitem"><p class="simpara">
						Create the subscription by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f trustee-subscription.yaml</pre></li><li class="listitem"><p class="simpara">
						Verify that the Operator is correctly installed by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get csv -n trustee-operator-system</pre><p class="simpara">
						This command can take several minutes to complete.
					</p></li><li class="listitem"><p class="simpara">
						Watch the process by running the following command:
					</p><pre class="programlisting language-terminal">$ watch oc get csv -n trustee-operator-system</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME                      DISPLAY                        PHASE
trustee-operator.v0.1.0   Trustee Operator  0.1.0        Succeeded</pre>
						</p></div></li></ol></div></section><section class="section" id="cc-enabling-feature-gate_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.2. Enabling the Confidential Containers feature gate</h2></div></div></div><p>
				You must enable the Confidential Containers feature gate.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have subscribed to the OpenShift sandboxed containers Operator.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">cc-feature-gate.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: osc-feature-gates
  namespace: openshift-sandboxed-containers-operator
data:
  confidential: "true"</pre></li><li class="listitem"><p class="simpara">
						Create the config map by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f cc-feature-gate.yaml</pre></li></ol></div></section><section class="section" id="cc-creating-route_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.3. Creating the route for Trustee</h2></div></div></div><p>
				You can create a secure route with edge TLS termination for Trustee. External ingress traffic reaches the router pods as HTTPS and passes on to the Trustee pods as HTTP.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the Confidential compute attestation Operator.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create an edge route by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create route edge --service=kbs-service --port kbs-port \
  -n trustee-operator-system</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Note: Currently, only a route with a valid CA-signed certificate is supported. You cannot use a route with self-signed certificate.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Set the <code class="literal">TRUSTEE_HOST</code> variable by running the following command:
					</p><pre class="programlisting language-terminal">$ TRUSTEE_HOST=$(oc get route -n trustee-operator-system kbs-service \
  -o jsonpath={.spec.host})</pre></li><li class="listitem"><p class="simpara">
						Verify the route by running the following command:
					</p><pre class="programlisting language-terminal">$ echo $TRUSTEE_HOST</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">kbs-service-trustee-operator-system.apps.memvjias.eastus.aroapp.io</pre>
						</p></div></li></ol></div></section><section class="section" id="creating-peer-pods-config-map_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.4. Updating the peer pods config map</h2></div></div></div><p>
				You must update the peer pods config map for Confidential Containers.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Set Secure Boot to <code class="literal">true</code> to enable it by default. The default value is <code class="literal">false</code>, which presents a security risk.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">peer-pods-cm.yaml</code> manifest file according to the following example:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: peer-pods-cm
  namespace: openshift-sandboxed-containers-operator
data:
  CLOUD_PROVIDER: "libvirt"
  DISABLECVM: "false"</pre></li><li class="listitem"><p class="simpara">
						Create the config map by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f peer-pods-cm.yaml</pre></li><li class="listitem"><p class="simpara">
						Restart the <code class="literal">peerpodconfig-ctrl-caa-daemon</code> daemon set by running the following command:
					</p><pre class="programlisting language-terminal">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</pre></li></ol></div></section><section class="section" id="deleting-cr-cli_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.5. Deleting the KataConfig custom resource</h2></div></div></div><p>
				You can delete the <code class="literal">KataConfig</code> custom resource (CR) by using the command line.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Delete the <code class="literal">KataConfig</code> CR by running the following command:
					</p><pre class="programlisting language-terminal">$ oc delete kataconfig example-kataconfig</pre></li><li class="listitem"><p class="simpara">
						Verify that the custom resource was deleted by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get kataconfig example-kataconfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">No example-kataconfig instances exist</pre>
						</p></div></li></ol></div></section><section class="section" id="creating-peer-pods-secret_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.6. Updating the peer pods secret</h2></div></div></div><p>
				You must update the peer pods secret for Confidential Containers.
			</p><p>
				The secret stores credentials for creating the pod virtual machine (VM) image and peer pod instances.
			</p><p>
				By default, the OpenShift sandboxed containers Operator creates the secret based on the credentials used to create the cluster. However, you can manually create a secret that uses different credentials.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">REDHAT_OFFLINE_TOKEN</code>. You have generated this token to download the RHEL image at <a class="link" href="https://access.redhat.com/management/api">Red Hat API Tokens</a>.
					</li><li class="listitem">
						<code class="literal">HKD_CRT</code>. The Host Key Document (HKD) certificate enables secure execution on IBM Z®. For more information, see <a class="link" href="https://www.ibm.com/docs/en/linux-on-systems?topic=linux-obtain-host-key-document">Obtaining a host key document from Resource Link</a> in the IBM documentation.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">peer-pods-secret.yaml</code> manifest file according to the following example:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: peer-pods-secret
  namespace: openshift-sandboxed-containers-operator
type: Opaque
stringData:
  REDHAT_OFFLINE_TOKEN: "&lt;rh_offline_token&gt;" <span id="CO40-1"/><span class="callout">1</span>
  HKD_CRT: "&lt;hkd_crt_value&gt;" <span id="CO40-2"/><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO40-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the Red Hat offline token, which is required for the Operator-built image.
							</div></dd><dt><a href="#CO40-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Specify the HKD certificate value to enable IBM Secure Execution for the Operator-built image.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Create the secret by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f peer-pods-secret.yaml</pre></li><li class="listitem"><p class="simpara">
						Restart the <code class="literal">peerpodconfig-ctrl-caa-daemon</code> daemon set by running the following command:
					</p><pre class="programlisting language-terminal">$ oc set env ds/peerpodconfig-ctrl-caa-daemon \
  -n openshift-sandboxed-containers-operator REBOOT="$(date)"</pre></li></ol></div></section><section class="section" id="creating-kataconfig-cr-cli_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.7. Re-creating the KataConfig custom resource</h2></div></div></div><p>
				You must re-create the <code class="literal">KataConfig</code> custom resource (CR) for Confidential Containers.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Creating the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A larger OpenShift Container Platform deployment with a greater number of worker nodes.
						</li><li class="listitem">
							Activation of the BIOS and Diagnostics utility.
						</li><li class="listitem">
							Deployment on a hard disk drive rather than an SSD.
						</li><li class="listitem">
							Deployment on physical nodes such as bare metal, rather than on virtual nodes.
						</li><li class="listitem">
							A slow CPU and network.
						</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create an <code class="literal">example-kataconfig.yaml</code> manifest file according to the following example:
					</p><pre class="programlisting language-yaml">apiVersion: kataconfiguration.openshift.io/v1
kind: KataConfig
metadata:
  name: example-kataconfig
spec:
  enablePeerPods: true
  logLevel: info
#  kataConfigPoolSelector:
#    matchLabels:
#      &lt;label_key&gt;: '&lt;label_value&gt;' <span id="CO41-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO41-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Optional: If you have applied node labels to install <code class="literal">kata-remote</code> on specific nodes, specify the key and value, for example, <code class="literal">cc: 'true'</code>.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">KataConfig</code> CR by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f example-kataconfig.yaml</pre><p class="simpara">
						The new <code class="literal">KataConfig</code> CR is created and installs <code class="literal">kata-remote</code> as a runtime class on the worker nodes.
					</p><p class="simpara">
						Wait for the <code class="literal">kata-remote</code> installation to complete and the worker nodes to reboot before verifying the installation.
					</p></li><li class="listitem"><p class="simpara">
						Monitor the installation progress by running the following command:
					</p><pre class="programlisting language-terminal">$ watch "oc describe kataconfig | sed -n /^Status:/,/^Events/p"</pre><p class="simpara">
						When the status of all workers under <code class="literal">kataNodes</code> is <code class="literal">installed</code> and the condition <code class="literal">InProgress</code> is <code class="literal">False</code> without specifying a reason, the <code class="literal">kata-remote</code> is installed on the cluster.
					</p></li><li class="listitem"><p class="simpara">
						Verify that you have built the peer pod image and uploaded it to the libvirt volume by running the following command:
					</p><pre class="programlisting language-terminal">$ oc describe configmap peer-pods-cm -n openshift-sandboxed-containers-operator</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">Name: peer-pods-cm
Namespace: openshift-sandboxed-containers-operator
Labels: &lt;none&gt;
Annotations: &lt;none&gt;

Data
====
CLOUD_PROVIDER: libvirt
DISABLECVM: false <span id="CO42-1"/><span class="callout">1</span>
LIBVIRT_IMAGE_ID: fa-pp-vol <span id="CO42-2"/><span class="callout">2</span>

BinaryData
====
Events: &lt;none&gt;</pre>
						</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO42-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Enables the Confidential VM during IBM Secure Execution for the Operator-built image.
							</div></dd><dt><a href="#CO42-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Contains a value if you have built the peer pod image and uploaded it to the libvirt volume.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Monitor the <code class="literal">kata-oc</code> machine config pool progress to ensure that it is in the <code class="literal">UPDATED</code> state, when <code class="literal">UPDATEDMACHINECOUNT</code> equals <code class="literal">MACHINECOUNT</code>, by running the following command:
					</p><pre class="programlisting language-terminal">$ watch oc get mcp/kata-oc</pre></li><li class="listitem"><p class="simpara">
						Verify the daemon set by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get -n openshift-sandboxed-containers-operator ds/peerpodconfig-ctrl-caa-daemon</pre></li><li class="listitem"><p class="simpara">
						Verify the runtime classes by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get runtimeclass</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">NAME             HANDLER          AGE
kata             kata             152m
kata-remote      kata-remote      152m</pre>
						</p></div></li></ol></div></section><section class="section" id="cc-creating-trustee-auth-secret_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.8. Creating the Trustee authentication secret</h2></div></div></div><p>
				You must create the authentication secret for Trustee.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a private key by running the following command:
					</p><pre class="programlisting language-terminal">$ openssl genpkey -algorithm ed25519 &gt; privateKey</pre></li><li class="listitem"><p class="simpara">
						Create a public key by running the following command:
					</p><pre class="programlisting language-terminal">$ openssl pkey -in privateKey -pubout -out publicKey</pre></li><li class="listitem"><p class="simpara">
						Create a secret by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create secret generic kbs-auth-public-key --from-file=publicKey -n trustee-operator-system</pre></li><li class="listitem"><p class="simpara">
						Verify the secret by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get secret -n trustee-operator-system</pre></li></ol></div></section><section class="section" id="cc-creating-trustee-config-map_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.9. Creating the Trustee config map</h2></div></div></div><p>
				You must create the config map to configure the Trustee server.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The following configuration example turns off security features to enable demonstration of Technology Preview features. It is not meant for a production environment.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created a route for Trustee.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">kbs-config-cm.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: kbs-config-cm
  namespace: trustee-operator-system
data:
  kbs-config.json: |
    {
      "insecure_http" : true,
      "sockets": ["0.0.0.0:8080"],
      "auth_public_key": "/etc/auth-secret/publicKey",
      "attestation_token_config": {
        "attestation_token_type": "CoCo"
      },
      "repository_config": {
        "type": "LocalFs",
        "dir_path": "/opt/confidential-containers/kbs/repository"
      },
      "as_config": {
        "work_dir": "/opt/confidential-containers/attestation-service",
        "policy_engine": "opa",
        "attestation_token_broker": "Simple",
          "attestation_token_config": {
          "duration_min": 5
          },
        "rvps_config": {
          "store_type": "LocalJson",
          "store_config": {
            "file_path": "/opt/confidential-containers/rvps/reference-values/reference-values.json"
          }
         }
      },
      "policy_engine_config": {
        "policy_path": "/opt/confidential-containers/opa/policy.rego"
      }
    }</pre></li><li class="listitem"><p class="simpara">
						Create the config map by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f kbs-config-cm.yaml</pre></li></ol></div></section><section class="section" id="ibm-cc-obtaining-se-header.adoc_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.10. Obtaining the IBM Secure Execution header</h2></div></div></div><p>
				You must obtain the IBM Secure Execution (SE) header.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have a network block storage device to store the SE header temporarily.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a temporary folder for the SE header by running the following command:
					</p><pre class="programlisting language-terminal">$ mkdir -p /tmp/ibmse/hdr</pre></li><li class="listitem"><p class="simpara">
						Download the <code class="literal">pvextract-hdr</code> tool from <a class="link" href="https://github.com/ibm-s390-linux/s390-tools/blob/v2.33.1/rust/pvattest/tools/pvextract-hdr">IBM s390 Linux</a> repository by running the following command:
					</p><pre class="programlisting language-terminal">$ wget https://github.com/ibm-s390-linux/s390-tools/raw/v2.33.1/rust/pvattest/tools/pvextract-hdr -O /tmp/pvextract-hdr</pre></li><li class="listitem"><p class="simpara">
						Make the tool executable by running the following command:
					</p><pre class="programlisting language-terminal">$ chmod +x /tmp/pvextract-hdr</pre></li><li class="listitem"><p class="simpara">
						Set the <code class="literal">$IMAGE_OUTPUT_DIR</code> variable by running the following command:
					</p><pre class="programlisting language-terminal">$ export IMAGE=$IMAGE_OUTPUT_DIR/se-podvm-commit-short-id.qcow2</pre></li><li class="listitem"><p class="simpara">
						Set the <code class="literal">$IMAGE</code> variable by running the following command:
					</p><pre class="programlisting language-terminal">$ export IMAGE=/root/rooo/se-podvm-d1fb986-dirty-s390x.qcow2</pre></li><li class="listitem"><p class="simpara">
						Enable the <code class="literal">nbd</code> kernel module by running the following command:
					</p><pre class="programlisting language-terminal">$ modprobe nbd</pre></li><li class="listitem"><p class="simpara">
						Connect the SE image as a network block device (NBD) by running the following command:
					</p><pre class="programlisting language-terminal">$ qemu-nbd --connect=/dev/nbd0 $IMAGE</pre></li><li class="listitem"><p class="simpara">
						Create a mount directory for the SE image by running the following command:
					</p><pre class="programlisting language-terminal">$ mkdir -p /mnt/se-image/</pre></li><li class="listitem"><p class="simpara">
						Pause the process by running the following command:
					</p><pre class="programlisting language-terminal">$ sleep 1</pre></li><li class="listitem"><p class="simpara">
						List your block devices by running the following command:
					</p><pre class="programlisting language-terminal">$ lsblk</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">nbd0                                           43:0    0  100G  0 disk
├─nbd0p1                                       43:1    0  255M  0 part
├─nbd0p2                                       43:2    0    6G  0 part
│ └─luks-e23e15fa-9c2a-45a5-9275-aae9d8e709c3 253:2    0    6G  0 crypt
└─nbd0p3                                       43:3    0 12.4G  0 part
nbd1                                           43:32   0   20G  0 disk
├─nbd1p1                                       43:33   0  255M  0 part
├─nbd1p2                                       43:34   0    6G  0 part
│ └─luks-5a540f7c-c0cb-419b-95e0-487670d91525 253:3    0    6G  0 crypt
└─nbd1p3                                       43:35   0 86.9G  0 part
nbd2                                           43:64   0    0B  0 disk
nbd3                                           43:96   0    0B  0 disk
nbd4                                           43:128  0    0B  0 disk
nbd5                                           43:160  0    0B  0 disk
nbd6                                           43:192  0    0B  0 disk
nbd7                                           43:224  0    0B  0 disk
nbd8                                           43:256  0    0B  0 disk
nbd9                                           43:288  0    0B  0 disk
nbd10                                          43:320  0    0B  0 disk</pre>
						</p></div></li><li class="listitem"><p class="simpara">
						Mount the SE image directory on an available NBD partition and extract the SE header by running the following command:
					</p><pre class="programlisting language-terminal">$ mount /dev/&lt;nbdXp1&gt; /mnt/se-image/ /tmp/pvextract-hdr \
  -o /tmp/ibmse/hdr/hdr.bin /mnt/se-image/se.img</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">SE header found at offset 0x014000
SE header written to '/tmp/ibmse/hdr/hdr.bin' (640 bytes)</pre>
						</p></div><p class="simpara">
						The following error is displayed if the NBD is unavailable:
					</p><pre class="programlisting language-text">mount: /mnt/se-image: can't read superblock on /dev/nbd0p1</pre></li><li class="listitem"><p class="simpara">
						Unmount the SE image directory by running the following command:
					</p><pre class="programlisting language-terminal">$ umount /mnt/se-image/</pre></li><li class="listitem"><p class="simpara">
						Disconnect the network block storage device by running the following command:
					</p><pre class="programlisting language-terminal">$ qemu-nbd --disconnect /dev/nbd0</pre></li></ol></div></section><section class="section" id="ibm-cc-obtaining-attest-fields-certs-keys_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.11. Configuring the IBM Secure Execution certificates and keys</h2></div></div></div><p>
				You must configure the IBM Secure Execution (SE) certificates and keys for your worker nodes.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have the IP address of the bastion node.
					</li><li class="listitem">
						You have the internal IP addresses of the worker nodes.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Obtain the attestation policy fields by performing the following steps:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Download the <code class="literal">se_parse_hdr.py</code> script from the OpenShift Trustee repository by running the following command:
							</p><pre class="programlisting language-terminal">$ wget https://github.com/openshift/trustee/raw/main/attestation-service/verifier/src/se/se_parse_hdr.py -O /tmp/se_parse_hdr.py</pre></li><li class="listitem"><p class="simpara">
								Create a temporary directory for the SE Host Key Document (HKD) certificate by running the following command:
							</p><pre class="programlisting language-terminal">$ mkdir /tmp/ibmse/hkds/</pre></li><li class="listitem"><p class="simpara">
								Copy your Host Key Document (HKD) certificate to the temporary directory by running the following command:
							</p><pre class="programlisting language-terminal">$ cp ~/path/to/&lt;hkd_cert.crt&gt; /tmp/ibmse/hkds/&lt;hkd_cert.crt&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The HKD certificate must be the same certificate that you downloaded when you created the peer pods secret.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Obtain the attestation policy fields by running the <code class="literal">se_parse_hdr.py</code> script:
							</p><pre class="programlisting language-terminal">$ python3 /tmp/se_parse_hdr.py /tmp/ibmse/hdr/hdr.bin /tmp/ibmse/hkds/&lt;hkd_cert.crt&gt;</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-text">...
  ================================================
  se.image_phkh: xxx
  se.version: 256
  se.tag: xxx
  se.attestation_phkh: xxx</pre>
								</p></div><p class="simpara">
								Record these values for the SE attestation policy config map.
							</p></li></ol></div></li><li class="listitem"><p class="simpara">
						Obtain the certificates and certificate revocation lists (CRLs) by performing the following steps:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create a temporary directory for certificates by running the following command:
							</p><pre class="programlisting language-terminal">$ mkdir /tmp/ibmse/certs</pre></li><li class="listitem"><p class="simpara">
								Download the <code class="literal">ibm-z-host-key-signing-gen2.crt</code> certificate by running the following command:
							</p><pre class="programlisting language-terminal">$ wget https://www.ibm.com/support/resourcelink/api/content/public/ibm-z-host-key-signing-gen2.crt -O /tmp/ibmse/certs/ibm-z-host-key-signing-gen2.crt</pre></li><li class="listitem"><p class="simpara">
								Download the <code class="literal">DigiCertCA.crt</code> certificate by running the following command:
							</p><pre class="programlisting language-terminal">$ wget https://www.ibm.com/support/resourcelink/api/content/public/DigiCertCA.crt -O /tmp/ibmse/certs/DigiCertCA.crt</pre></li><li class="listitem"><p class="simpara">
								Create a temporary directory for the CRLs by running the following command:
							</p><pre class="programlisting language-terminal">$ mkdir /tmp/ibmse/crls</pre></li><li class="listitem"><p class="simpara">
								Download the <code class="literal">DigiCertTrustedRootG4.crl</code> file by running the following command:
							</p><pre class="programlisting language-terminal">$ wget http://crl3.digicert.com/DigiCertTrustedRootG4.crl -O /tmp/ibmse/crls/DigiCertTrustedRootG4.crl</pre></li><li class="listitem"><p class="simpara">
								Download the <code class="literal">DigiCertTrustedG4CodeSigningRSA4096SHA3842021CA1.crl</code> file by running the following command:
							</p><pre class="programlisting language-terminal">$ wget http://crl3.digicert.com/DigiCertTrustedG4CodeSigningRSA4096SHA3842021CA1.crl -O /tmp/ibmse/crls/DigiCertTrustedG4CodeSigningRSA4096SHA3842021CA1.crl</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Generate the RSA keys:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Generate an RSA key pair by running the following command:
							</p><pre class="programlisting language-terminal">$ openssl genrsa -aes256 -passout pass:&lt;password&gt; -out /tmp/encrypt_key-psw.pem 4096 <span id="CO43-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO43-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the RSA key password.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Create a temporary directory for the RSA keys by running the following command:
							</p><pre class="programlisting language-terminal">$ mkdir /tmp/ibmse/rsa</pre></li><li class="listitem"><p class="simpara">
								Create an <code class="literal">encrypt_key.pub</code> key by running the following command:
							</p><pre class="programlisting language-terminal">$ openssl rsa -in /tmp/encrypt_key-psw.pem -passin pass:&lt;password&gt; -pubout -out /tmp/ibmse/rsa/encrypt_key.pub</pre></li><li class="listitem"><p class="simpara">
								Create an <code class="literal">encrypt_key.pem</code> key by running the following command:
							</p><pre class="programlisting language-terminal">$ openssl rsa -in /tmp/encrypt_key-psw.pem -passin pass:&lt;password&gt; -out /tmp/ibmse/rsa/encrypt_key.pem</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Verify the structure of the <code class="literal">/tmp/ibmse</code> directory by running the following command:
					</p><pre class="programlisting language-terminal">$ tree /tmp/ibmse</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">/tmp/ibmse
├── certs
│   ├── ibm-z-host-key-signing-gen2.crt
|   └── DigiCertCA.crt
├── crls
│   └── ibm-z-host-key-gen2.crl
│   └── DigiCertTrustedRootG4.crl
│   └── DigiCertTrustedG4CodeSigningRSA4096SHA3842021CA1.crl
├── hdr
│   └── hdr.bin
├── hkds
│   └── &lt;hkd_cert.crt&gt;
└── rsa
    ├── encrypt_key.pem
    └── encrypt_key.pub</pre>
						</p></div></li><li class="listitem"><p class="simpara">
						Copy these files to the OpenShift Container Platform worker nodes by performing the following steps:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create a compressed file from the <code class="literal">/tmp/ibmse</code> directory by running the following command:
							</p><pre class="programlisting language-terminal">$ tar -czf ibmse.tar.gz -C /tmp/ibmse</pre></li><li class="listitem"><p class="simpara">
								Copy the <code class="literal">.tar.gz</code> file to the bastion node in your cluster by running the following command:
							</p><pre class="programlisting language-terminal">$ scp /tmp/ibmse.tar.gz root@&lt;ocp_bastion_ip&gt;:/tmp <span id="CO44-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO44-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the IP address of the bastion node.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Connect to the bastion node over SSH by running the following command:
							</p><pre class="programlisting language-terminal">$ ssh root@&lt;ocp_bastion_ip&gt;</pre></li><li class="listitem"><p class="simpara">
								Copy the <code class="literal">.tar.gz</code> file to each worker node by running the following command:
							</p><pre class="programlisting language-terminal">$ scp /tmp/ibmse.tar.gz core@&lt;worker_node_ip&gt;:/tmp <span id="CO45-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO45-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the IP address of the worker node.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Extract the <code class="literal">.tar.gz</code> on each worker node by running the following command:
							</p><pre class="programlisting language-terminal">$ ssh core@&lt;worker_node_ip&gt; 'sudo mkdir -p /opt/confidential-containers/ &amp;&amp; sudo tar -xzf /tmp/ibmse.tar.gz -C /opt/confidential-containers/'</pre></li><li class="listitem"><p class="simpara">
								Update the <code class="literal">ibmse</code> folder permissions by running the following command:
							</p><pre class="programlisting language-terminal">$ ssh core@&lt;worker_node_ip&gt; 'sudo chmod -R 755 /opt/confidential-containers/ibmse/'</pre></li></ol></div></li></ol></div></section><section class="section" id="ibm-cc-creating-persistent-storage-components_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.12. Creating the persistent storage components</h2></div></div></div><p>
				You must create persistent storage components, persistent volume (PV) and persistent volume claim (PVC) to mount the <code class="literal">ibmse</code> folder to the Trustee pod.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">persistent-volume.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: ibmse-pv
  namespace: trustee-operator-system
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadOnlyMany
  storageClassName: ""
  local:
    path: /opt/confidential-containers/ibmse
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: node-role.kubernetes.io/worker
              operator: Exists</pre></li><li class="listitem"><p class="simpara">
						Create the persistent volume by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f persistent-volume.yaml</pre></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">persistent-volume-claim.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ibmse-pvc
  namespace: trustee-operator-system
spec:
  accessModes:
    - ReadOnlyMany
  storageClassName: ""
  resources:
    requests:
      storage: 100Mi</pre></li><li class="listitem"><p class="simpara">
						Create the persistent volume claim by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f persistent-volume-claim.yaml</pre></li></ol></div></section><section class="section" id="configuring-trustee_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.13. Configuring Trustee values, policies, and secrets</h2></div></div></div><p>
				You can configure the following values, policies, and secrets for Trustee:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Optional: Reference values for the Reference Value Provider Service.
					</li><li class="listitem">
						Attestation policy for IBM Secure Execution.
					</li><li class="listitem">
						Optional: Secret for custom keys for Trustee clients.
					</li><li class="listitem">
						Optional: Secret for container image signature verification.
					</li><li class="listitem">
						Container image signature verification policy. This policy is mandatory. If you do not use container image signature verification, you must create a policy that does not verify signatures.
					</li><li class="listitem">
						Resource access policy.
					</li></ul></div><section class="section" id="cc-configuring-reference-values_ibm-cc"><div class="titlepage"><div><div><h3 class="title">7.13.1. Configuring reference values</h3></div></div></div><p>
					You can configure reference values for the Reference Value Provider Service (RVPS) by specifying the trusted digests of your hardware platform.
				</p><p>
					The client collects measurements from the running software, the Trusted Execution Environment (TEE) hardware and firmware and it submits a quote with the claims to the Attestation Server. These measurements must match the trusted digests registered to the Trustee. This process ensures that the confidential VM (CVM) is running the expected software stack and has not been tampered with.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">rvps-configmap.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: rvps-reference-values
  namespace: trustee-operator-system
data:
  reference-values.json: |
    [ <span id="CO46-1"/><span class="callout">1</span>
    ]</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO46-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Leave this value empty.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the RVPS config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f rvps-configmap.yaml</pre></li></ol></div></section><section class="section" id="cc-creating-attestation-policy_ibm-cc"><div class="titlepage"><div><div><h3 class="title">7.13.2. Creating the attestation policy for IBM Secure Execution</h3></div></div></div><p>
					You must create the attestation policy for IBM Secure Execution.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an <code class="literal">attestation-policy.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: attestation-policy
  namespace: trustee-operator-system
data:
  default.rego: | <span id="CO47-1"/><span class="callout">1</span>
    package policy
    import rego.v1
    default allow = false
    converted_version := sprintf("%v", [input["se.version"]])
    allow if {
        input["se.attestation_phkh"] == "&lt;se.attestation_phkh&gt;" <span id="CO47-2"/><span class="callout">2</span>
        input["se.image_phkh"] == "&lt;se.image_phkh&gt;"
        input["se.tag"] == "&lt;se.tag&gt;"
        converted_version == "256"
    }</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO47-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Do not modify the policy name.
								</div></dd><dt><a href="#CO47-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the attestation policy fields you obtained by running the <code class="literal">se_parse_hdr.py</code> script.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the attestation policy config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f attestation-policy.yaml</pre></li></ol></div></section><section class="section" id="cc-creating-secret-for-clients_ibm-cc"><div class="titlepage"><div><div><h3 class="title">7.13.3. Creating a secret with custom keys for clients</h3></div></div></div><p>
					You can create a secret that contains one or more custom keys for Trustee clients.
				</p><p>
					In this example, the <code class="literal">kbsres1</code> secret has two entries (<code class="literal">key1</code>, <code class="literal">key2</code>), which the clients retrieve. You can add additional secrets according to your requirements by using the same format.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have created one or more custom keys.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Create a secret for the custom keys according to the following example:
						</p><pre class="programlisting language-terminal">$ oc apply secret generic kbsres1 \
  --from-literal key1=&lt;custom_key1&gt; \ <span id="CO48-1"/><span class="callout">1</span>
  --from-literal key2=&lt;custom_key2&gt; \
  -n trustee-operator-system</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO48-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a custom key.
								</div></dd></dl></div><p class="simpara">
							The <code class="literal">kbsres1</code> secret is specified in the <code class="literal">spec.kbsSecretResources</code> key of the <code class="literal">KbsConfig</code> custom resource.
						</p></li></ul></div></section><section class="section" id="cc-creating-secret-signed-container-images_ibm-cc"><div class="titlepage"><div><div><h3 class="title">7.13.4. Creating a secret for container image signature verification</h3></div></div></div><p>
					If you use container image signature verification, you must create a secret that contains the public container image signing key.
				</p><p>
					The Key Broker Service on the Trustee cluster uses the secret to verify the signature, ensuring that only trusted and authenticated container images are deployed in your environment.
				</p><p>
					You can use <a class="link" href="https://developers.redhat.com/products/trusted-artifact-signer/overview">Red Hat Trusted Artifact Signer</a> or other tools to sign container images.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a secret for container image signature verification by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply secret generic &lt;type&gt; \ <span id="CO49-1"/><span class="callout">1</span>
  --from-file=&lt;tag&gt;=./&lt;public_key_file&gt; \ <span id="CO49-2"/><span class="callout">2</span>
  -n trustee-operator-system</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO49-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the KBS secret type, for example, <code class="literal">img-sig</code>.
								</div></dd><dt><a href="#CO49-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the secret tag, for example, <code class="literal">pub-key</code>, and the public container image signing key.
								</div></dd></dl></div></li><li class="listitem">
							Record the <code class="literal">&lt;type&gt;</code> value. You must add this value to the <code class="literal">spec.kbsSecretResources</code> key when you create the <code class="literal">KbsConfig</code> custom resource.
						</li></ol></div></section><section class="section" id="cc-creating-container-image-sig-policy_ibm-cc"><div class="titlepage"><div><div><h3 class="title">7.13.5. Creating the container image signature verification policy</h3></div></div></div><p>
					You create the container image signature verification policy because signature verification is always enabled. If this policy is missing, the pods will not start.
				</p><p>
					If you are not using container image signature verification, you create the policy without signature verification.
				</p><p>
					For more information, see <a class="link" href="https://github.com/containers/image/blob/main/docs/containers-policy.json.5.md">containers-policy.json 5</a>.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">security-policy-config.json</code> file according to the following examples:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Without signature verification:
								</p><pre class="programlisting language-json">{
  "default": [
  {
    "type": "insecureAcceptAnything"
  }],
  "transports": {}
}</pre></li><li class="listitem"><p class="simpara">
									With signature verification:
								</p><pre class="programlisting language-json">{
  "default": [
      {
      "type": "insecureAcceptAnything"
      }
  ],
  "transports": {
      "&lt;transport&gt;": { <span id="CO50-1"/><span class="callout">1</span>
          "&lt;registry&gt;/&lt;image&gt;": <span id="CO50-2"/><span class="callout">2</span>
          [
              {
                  "type": "sigstoreSigned",
                  "keyPath": "kbs:///default/&lt;type&gt;/&lt;tag&gt;" <span id="CO50-3"/><span class="callout">3</span>
              }
          ]
      }
  }
}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO50-1"><span class="callout">1</span></a> </dt><dd><div class="para">
											Specify the image repository for <code class="literal">transport</code>, for example, <code class="literal">"docker":</code>. For more information, see <a class="link" href="https://github.com/containers/image/blob/main/docs/containers-transports.5.md">containers-transports 5</a>.
										</div></dd><dt><a href="#CO50-2"><span class="callout">2</span></a> </dt><dd><div class="para">
											Specify the container registry and image, for example, "quay.io/my-image".
										</div></dd><dt><a href="#CO50-3"><span class="callout">3</span></a> </dt><dd><div class="para">
											Specify the type and tag of the container image signature verification secret that you created, for example, <code class="literal">img-sig/pub-key</code>.
										</div></dd></dl></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Create the security policy by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply secret generic security-policy \
  --from-file=osc=./&lt;security-policy-config.json&gt; \
  -n trustee-operator-system</pre><p class="simpara">
							Do not alter the secret type, <code class="literal">security-policy</code>, or the key, <code class="literal">osc</code>.
						</p><p class="simpara">
							The <code class="literal">security-policy</code> secret is specified in the <code class="literal">spec.kbsSecretResources</code> key of the <code class="literal">KbsConfig</code> custom resource.
						</p></li></ol></div></section><section class="section" id="cc-creating-resource-access-policy_ibm-cc"><div class="titlepage"><div><div><h3 class="title">7.13.6. Creating the resource access policy</h3></div></div></div><p>
					You configure the resource access policy for the Trustee policy engine. This policy determines which resources Trustee can access.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The Trustee policy engine is different from the Attestation Service policy engine, which determines the validity of TEE evidence.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">resourcepolicy-configmap.yaml</code> manifest file:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: resource-policy
  namespace: trustee-operator-system
data:
  policy.rego: | <span id="CO51-1"/><span class="callout">1</span>
    package policy <span id="CO51-2"/><span class="callout">2</span>
    path := split(data["resource-path"], "/")
    default allow = false
    allow {
      count(path) == 3
      input["tee"] == "se"
    }</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO51-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the resource policy, <code class="literal">policy.rego</code>, must match the resource policy defined in the Trustee config map.
								</div></dd><dt><a href="#CO51-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The resource policy follows the <a class="link" href="https://www.openpolicyagent.org/docs/latest/policy-language/">Open Policy Agent</a> specification. This example allows the retrieval of all resources when the TEE is not the sample attester.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Create the resource policy config map by running the following command:
						</p><pre class="programlisting language-terminal">$ oc apply -f resourcepolicy-configmap.yaml</pre></li></ol></div></section></section><section class="section" id="cc-creating-kbsconfig-cr_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.14. Creating the KbsConfig custom resource</h2></div></div></div><p>
				You create the <code class="literal">KbsConfig</code> custom resource (CR) to launch Trustee.
			</p><p>
				Then, you check the Trustee pods and pod logs to verify the configuration.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">kbsconfig-cr.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: confidentialcontainers.org/v1alpha1
kind: KbsConfig
metadata:
  labels:
    app.kubernetes.io/name: kbsconfig
    app.kubernetes.io/instance: kbsconfig
    app.kubernetes.io/part-of: trustee-operator
    app.kubernetes.io/managed-by: kustomize
    app.kubernetes.io/created-by: trustee-operator
  name: kbsconfig
  namespace: trustee-operator-system
spec:
  kbsConfigMapName: kbs-config-cm
  kbsAuthSecretName: kbs-auth-public-key
  kbsDeploymentType: AllInOneDeployment
  kbsRvpsRefValuesConfigMapName: rvps-reference-values
  kbsSecretResources: ["kbsres1", "security-policy", "&lt;type&gt;"] <span id="CO52-1"/><span class="callout">1</span>
  kbsResourcePolicyConfigMapName: resource-policy
  kbsAttestationPolicyConfigMapName: attestation-policy
  kbsServiceType: NodePort
  ibmSEConfigSpec:
    certStorePvc: ibmse-pvc</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO52-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Specify the <code class="literal">type</code> value of the container image signature verification secret you created, for example, <code class="literal">img-sig</code>.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">KbsConfig</code> CR by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f kbsconfig-cr.yaml</pre></li></ol></div></section><section class="section" id="cc-verifing-trustee-config_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.15. Verifying the Trustee configuration</h2></div></div></div><p>
				You verify the Trustee configuration by checking the Trustee pods and logs.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Set the default project by running the following command:
					</p><pre class="programlisting language-terminal">$ oc project trustee-operator-system</pre></li><li class="listitem"><p class="simpara">
						Check the Trustee pods by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get pods -n trustee-operator-system</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">NAME                                                   READY   STATUS    RESTARTS   AGE
trustee-deployment-8585f98449-9bbgl                    1/1     Running   0          22m
trustee-operator-controller-manager-5fbd44cd97-55dlh   2/2     Running   0          59m</pre>
						</p></div></li><li class="listitem"><p class="simpara">
						Set the <code class="literal">POD_NAME</code> environmental variable by running the following command:
					</p><pre class="programlisting language-terminal">$ POD_NAME=$(oc get pods -l app=kbs -o jsonpath='{.items[0].metadata.name}' -n trustee-operator-system)</pre></li><li class="listitem"><p class="simpara">
						Check the pod logs by running the following command:
					</p><pre class="programlisting language-terminal">$ oc logs -n trustee-operator-system $POD_NAME</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">[2024-05-30T13:44:24Z INFO  kbs] Using config file /etc/kbs-config/kbs-config.json
[2024-05-30T13:44:24Z WARN  attestation_service::rvps] No RVPS address provided and will launch a built-in rvps
[2024-05-30T13:44:24Z INFO  attestation_service::token::simple] No Token Signer key in config file, create an ephemeral key and without CA pubkey cert
[2024-05-30T13:44:24Z INFO  api_server] Starting HTTPS server at [0.0.0.0:8080]
[2024-05-30T13:44:24Z INFO  actix_server::builder] starting 12 workers
[2024-05-30T13:44:24Z INFO  actix_server::server] Tokio runtime found; starting in existing Tokio runtime</pre>
						</p></div></li><li class="listitem"><p class="simpara">
						Expose the <code class="literal">ibmse-pvc</code> persistent volume claim to the Trustee pods by running the following command:
					</p><pre class="programlisting language-terminal">$ oc patch deployment trustee-deployment \
  --namespace=trustee-operator-system --type=json \
  -p='[{"op": "remove", "path": "/spec/template/spec/volumes/5/persistentVolumeClaim/readOnly"}]'</pre></li><li class="listitem"><p class="simpara">
						Verify that the <code class="literal">kbs-service</code> is exposed on a node port by running the following command:
					</p><pre class="programlisting language-terminal">$ oc get svc kbs-service -n trustee-operator-system</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-text">NAME          TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kbs-service   NodePort   198.51.100.54   &lt;none&gt;        8080:31862/TCP   23h</pre>
						</p></div><p class="simpara">
						The <code class="literal">kbs-service</code> URL is <code class="literal">https://&lt;worker_node_ip&gt;:&lt;node_port&gt;</code>, for example, <code class="literal">https://172.16.0.56:31862</code>.
					</p></li></ol></div></section><section class="section" id="verifying-attestation-process_ibm-cc"><div class="titlepage"><div><div><h2 class="title">7.16. Verifying the attestation process</h2></div></div></div><p>
				You verify the attestation process by creating a test pod and retrieving its secret. The pod image deploys the KBS client, a tool for testing the Key Broker Service and basic attestation flows.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					This procedure is an example to verify that attestation is working. Do not write sensitive data to standard I/O because the data can be captured by using a memory dump. Only data written to memory is encrypted.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have created a route if the Trustee server and the test pod are not running in the same cluster.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a <code class="literal">verification-pod.yaml</code> manifest file:
					</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: kbs-client
spec:
  containers:
  - name: kbs-client
    image: quay.io/confidential-containers/kbs-client:latest
    imagePullPolicy: IfNotPresent
    command:
      - sleep
      - "360000"
    env:
      - name: RUST_LOG
        value:  none</pre></li><li class="listitem"><p class="simpara">
						Create the pod by running the following command:
					</p><pre class="programlisting language-terminal">$ oc create -f verification-pod.yaml</pre></li><li class="listitem"><p class="simpara">
						Copy the <code class="literal">https.crt</code> file to the <code class="literal">kbs-client</code> pod by running the following command:
					</p><pre class="programlisting language-terminal">$ oc cp https.crt kbs-client:/</pre></li><li class="listitem"><p class="simpara">
						Fetch the pod secret by running the following command:
					</p><pre class="programlisting language-terminal">$ oc exec -it kbs-client -- kbs-client --cert-file https.crt \
  --url https://kbs-service:8080 get-resource \
  --path default/kbsres1/key1</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
							
<pre class="programlisting language-terminal">res1val1</pre>
						</p></div><p class="simpara">
						The Trustee server returns the secret only if the attestation is successful.
					</p></li></ol></div></section></section><section class="chapter" id="monitoring"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Monitoring</h1></div></div></div><p>
			You can use the OpenShift Container Platform web console to monitor metrics related to the health status of your sandboxed workloads and nodes.
		</p><p>
			OpenShift sandboxed containers has a pre-configured dashboard available in the OpenShift Container Platform web console. Administrators can also access and query raw metrics through Prometheus.
		</p><section class="section" id="about-metrics_monitoring"><div class="titlepage"><div><div><h2 class="title">8.1. About metrics</h2></div></div></div><p>
				OpenShift sandboxed containers metrics enable administrators to monitor how their sandboxed containers are running. You can query for these metrics in Metrics UI In the OpenShift Container Platform web console.
			</p><p>
				OpenShift sandboxed containers metrics are collected for the following categories:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Kata agent metrics</span></dt><dd>
							Kata agent metrics display information about the kata agent process running in the VM embedded in your sandboxed containers. These metrics include data from <code class="literal">/proc/&lt;pid&gt;/[io, stat, status]</code>.
						</dd><dt><span class="term">Kata guest operating system metrics</span></dt><dd>
							Kata guest operating system metrics display data from the guest operating system running in your sandboxed containers. These metrics include data from <code class="literal">/proc/[stats, diskstats, meminfo, vmstats]</code> and <code class="literal">/proc/net/dev</code>.
						</dd><dt><span class="term">Hypervisor metrics</span></dt><dd>
							Hypervisor metrics display data regarding the hypervisor running the VM embedded in your sandboxed containers. These metrics mainly include data from <code class="literal">/proc/&lt;pid&gt;/[io, stat, status]</code>.
						</dd><dt><span class="term">Kata monitor metrics</span></dt><dd>
							Kata monitor is the process that gathers metric data and makes it available to Prometheus. The kata monitor metrics display detailed information about the resource usage of the kata-monitor process itself. These metrics also include counters from Prometheus data collection.
						</dd><dt><span class="term">Kata containerd shim v2 metrics</span></dt><dd>
							Kata containerd shim v2 metrics display detailed information about the kata shim process. These metrics include data from <code class="literal">/proc/&lt;pid&gt;/[io, stat, status]</code> and detailed resource usage metrics.
						</dd></dl></div></section><section class="section" id="viewing-metrics_monitoring"><div class="titlepage"><div><div><h2 class="title">8.2. Viewing metrics</h2></div></div></div><p>
				You can access the metrics for OpenShift sandboxed containers in the <span class="strong strong"><strong>Metrics</strong></span> page In the OpenShift Container Platform web console.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role or with view permissions for all projects.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Metrics</strong></span>.
					</li><li class="listitem"><p class="simpara">
						In the input field, enter the query for the metric you want to observe.
					</p><p class="simpara">
						All kata-related metrics begin with <span class="strong strong"><strong>kata</strong></span>. Typing <span class="strong strong"><strong>kata</strong></span> displays a list of all available kata metrics.
					</p></li></ol></div><p>
				The metrics from your query are visualized on the page.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
						<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/monitoring/index#querying-metrics.html">Querying metrics</a>.
					</li><li class="listitem">
						<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/support/index#gathering-cluster-data.html">Gathering data about your cluster</a>.
					</li></ul></div></section></section><section class="chapter" id="uninstalling"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Uninstalling</h1></div></div></div><p>
			You can uninstall OpenShift sandboxed containers and remove the Confidential Containers environment.
		</p><section class="section" id="uninstalling-ocs"><div class="titlepage"><div><div><h2 class="title">9.1. Uninstalling OpenShift sandboxed containers</h2></div></div></div><p>
				You can uninstall OpenShift sandboxed containers by using the OpenShift Container Platform web console or the command line.
			</p><p>
				You uninstall OpenShift sandboxed containers by performing the following tasks:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Delete the workload pods.
					</li><li class="listitem">
						Delete the <code class="literal">KataConfig</code> custom resource.
					</li><li class="listitem">
						Uninstall the OpenShift sandboxed containers Operator.
					</li><li class="listitem">
						Delete the <code class="literal">KataConfig</code> custom resource definition.
					</li></ol></div><section class="section" id="uninstalling-osc-by-using-web-console"><div class="titlepage"><div><div><h3 class="title">9.1.1. Uninstalling OpenShift sandboxed containers by using the web console</h3></div></div></div><p>
					You can uninstall OpenShift sandboxed containers by using the OpenShift Container Platform web console.
				</p><section class="section" id="deleting-workload-pods-web_uninstalling-osc-web"><div class="titlepage"><div><div><h4 class="title">9.1.1.1. Deleting workload pods</h4></div></div></div><p>
						You can delete the OpenShift sandboxed containers workload pods by using the OpenShift Container Platform web console.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have a list of pods that use the OpenShift sandboxed containers runtime class.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Pods</strong></span>.
							</li><li class="listitem">
								Enter the name of the pod that you want to delete in the <span class="strong strong"><strong>Search by name</strong></span> field.
							</li><li class="listitem">
								Click the pod name to open it.
							</li><li class="listitem">
								On the <span class="strong strong"><strong>Details</strong></span> page, check that <code class="literal">kata</code> or <code class="literal">kata-remote</code> is displayed for <span class="strong strong"><strong>Runtime class</strong></span>.
							</li><li class="listitem">
								Click the <span class="strong strong"><strong>Options</strong></span> menu 
								<span class="inlinemediaobject"><img src="images/kebab.png" alt="kebab"/></span>
								 and select <span class="strong strong"><strong>Delete Pod</strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Delete</strong></span>.
							</li></ol></div></section><section class="section" id="deleting-cr-web_uninstalling-osc-web"><div class="titlepage"><div><div><h4 class="title">9.1.1.2. Deleting the KataConfig custom resource</h4></div></div></div><p>
						You can delete the <code class="literal">KataConfig</code> custom resource (CR) by using the web console.
					</p><p>
						Deleting the <code class="literal">KataConfig</code> CR removes and uninstalls the <code class="literal">kata</code> runtime and its related resources from your cluster.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Deleting the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									A larger OpenShift Container Platform deployment with a greater number of worker nodes.
								</li><li class="listitem">
									Activation of the BIOS and Diagnostics utility.
								</li><li class="listitem">
									Deployment on a hard drive rather than an SSD.
								</li><li class="listitem">
									Deployment on physical nodes such as bare metal, rather than on virtual nodes.
								</li><li class="listitem">
									A slow CPU and network.
								</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have deleted all running pods that use <code class="literal">kata</code> as the <code class="literal">runtimeClass</code>.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
							</li><li class="listitem">
								Enter <code class="literal">OpenShift sandboxed containers Operator</code> in the <span class="strong strong"><strong>Search by name</strong></span> field.
							</li><li class="listitem">
								Click the Operator to open it and then click the <span class="strong strong"><strong>KataConfig</strong></span> tab.
							</li><li class="listitem">
								Click the <span class="strong strong"><strong>Options</strong></span> menu 
								<span class="inlinemediaobject"><img src="images/kebab.png" alt="kebab"/></span>
								 and select <span class="strong strong"><strong>Delete <code class="literal">KataConfig</code></strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Delete</strong></span> in the confirmation window.
							</li></ol></div><p>
						Wait for the <code class="literal">kata</code> runtime and resources to uninstall and for the worker nodes to reboot before continuing to the next step.
					</p></section><section class="section" id="uninstalling-operator-web_uninstalling-osc-web"><div class="titlepage"><div><div><h4 class="title">9.1.1.3. Uninstalling the OpenShift sandboxed containers Operator</h4></div></div></div><p>
						You can uninstall the OpenShift sandboxed containers Operator by using OpenShift Container Platform web console.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have deleted the <code class="literal">KataConfig</code> custom resource.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
							</li><li class="listitem">
								Enter <code class="literal">OpenShift sandboxed containers Operator</code> in the <span class="strong strong"><strong>Search by name</strong></span> field.
							</li><li class="listitem"><p class="simpara">
								On the right side of the <span class="strong strong"><strong>Operator Details</strong></span> page, select <span class="strong strong"><strong>Uninstall Operator</strong></span> from the <span class="strong strong"><strong>Actions</strong></span> list.
							</p><p class="simpara">
								An <span class="strong strong"><strong>Uninstall Operator?</strong></span> dialog box is displayed.
							</p></li><li class="listitem">
								Click <span class="strong strong"><strong>Uninstall</strong></span> to remove the Operator, Operator deployments, and pods.
							</li><li class="listitem">
								Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Namespaces</strong></span>.
							</li><li class="listitem">
								Enter <code class="literal">openshift-sandboxed-containers-operator</code> in the <span class="strong strong"><strong>Search by name</strong></span> field.
							</li><li class="listitem">
								Click the <span class="strong strong"><strong>Options</strong></span> menu 
								<span class="inlinemediaobject"><img src="images/kebab.png" alt="kebab"/></span>
								 and select <span class="strong strong"><strong>Delete Namespace</strong></span>.
							</li><li class="listitem">
								In the confirmation dialog, enter <code class="literal">openshift-sandboxed-containers-operator</code> and click <span class="strong strong"><strong>Delete</strong></span>.
							</li></ol></div></section><section class="section" id="deleting-crd-web_uninstalling-osc-web"><div class="titlepage"><div><div><h4 class="title">9.1.1.4. Deleting the KataConfig CRD</h4></div></div></div><p>
						You can delete the <code class="literal">KataConfig</code> custom resource definition (CRD) by using the OpenShift Container Platform web console.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have deleted the <code class="literal">KataConfig</code> custom resource.
							</li><li class="listitem">
								You have uninstalled the OpenShift sandboxed containers Operator.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In the web console, navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>CustomResourceDefinitions</strong></span>.
							</li><li class="listitem">
								Enter the <code class="literal">KataConfig</code> name in the <span class="strong strong"><strong>Search by name</strong></span> field.
							</li><li class="listitem">
								Click the <span class="strong strong"><strong>Options</strong></span> menu and select <span class="strong strong"><strong>Delete CustomResourceDefinition</strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Delete</strong></span> in the confirmation window.
							</li></ol></div></section></section><section class="section" id="uninstalling-osc-by-using-cli"><div class="titlepage"><div><div><h3 class="title">9.1.2. Uninstalling OpenShift sandboxed containers by using the CLI</h3></div></div></div><p>
					You can uninstall OpenShift sandboxed containers by using the command-line interface (CLI).
				</p><section class="section" id="deleting-workload-pods-cli_uninstalling-osc-cli"><div class="titlepage"><div><div><h4 class="title">9.1.2.1. Deleting workload pods</h4></div></div></div><p>
						You can delete the OpenShift sandboxed containers workload pods by using the CLI.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have the JSON processor (<code class="literal">jq</code>) utility installed.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Search for the pods by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get pods -A -o json | jq -r '.items[] | \
  select(.spec.runtimeClassName == "&lt;runtime&gt;").metadata.name' <span id="CO53-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO53-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify <code class="literal">kata</code> for bare metal deployments. Specify <code class="literal">kata-remote</code> for AWS, Azure, IBM Z®, and IBM® LinuxONE.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Delete each pod by running the following command:
							</p><pre class="programlisting language-terminal">$ oc delete pod &lt;pod&gt;</pre></li></ol></div></section><section class="section" id="deleting-cr-cli_uninstalling-osc-cli"><div class="titlepage"><div><div><h4 class="title">9.1.2.2. Deleting the KataConfig custom resource</h4></div></div></div><p>
						You can delete the <code class="literal">KataConfig</code> custom resource (CR) by using the command line.
					</p><p>
						Deleting the <code class="literal">KataConfig</code> CR removes the runtime and its related resources from your cluster.
					</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							Deleting the <code class="literal">KataConfig</code> CR automatically reboots the worker nodes. The reboot can take from 10 to more than 60 minutes. Factors that impede reboot time are as follows:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									A larger OpenShift Container Platform deployment with a greater number of worker nodes.
								</li><li class="listitem">
									Activation of the BIOS and Diagnostics utility.
								</li><li class="listitem">
									Deployment on a hard drive rather than an SSD.
								</li><li class="listitem">
									Deployment on physical nodes such as bare metal, rather than on virtual nodes.
								</li><li class="listitem">
									A slow CPU and network.
								</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Delete the <code class="literal">KataConfig</code> CR by running the following command:
							</p><pre class="programlisting language-terminal">$ oc delete kataconfig example-kataconfig</pre><p class="simpara">
								The OpenShift sandboxed containers Operator removes all resources that were initially created to enable the runtime on your cluster.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									When you delete the <code class="literal">KataConfig</code> CR, the CLI stops responding until all worker nodes reboot. You must for the deletion process to complete before performing the verification.
								</p></div></div></li><li class="listitem"><p class="simpara">
								Verify that the custom resource was deleted by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get kataconfig example-kataconfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-text">No example-kataconfig instances exist</pre>
								</p></div></li></ol></div></section><section class="section" id="uninstalling-operator-cli_uninstalling-osc-cli"><div class="titlepage"><div><div><h4 class="title">9.1.2.3. Uninstalling the OpenShift sandboxed containers Operator</h4></div></div></div><p>
						You can uninstall the OpenShift sandboxed containers Operator by using the command line.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have deleted the OpenShift sandboxed containers workload pods.
							</li><li class="listitem">
								You have deleted <code class="literal">KataConfig</code> custom resource.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Delete the subscription by running the following command:
							</p><pre class="programlisting language-terminal">$ oc delete subscription sandboxed-containers-operator -n openshift-sandboxed-containers-operator</pre></li><li class="listitem"><p class="simpara">
								Delete the namespace by running the following command:
							</p><pre class="programlisting language-terminal">$ oc delete namespace openshift-sandboxed-containers-operator</pre></li></ol></div></section><section class="section" id="deleting-crd-cli_uninstalling-osc-cli"><div class="titlepage"><div><div><h4 class="title">9.1.2.4. Deleting the KataConfig CRD</h4></div></div></div><p>
						You can delete the <code class="literal">KataConfig</code> custom resource definition (CRD) by using the command line.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have deleted the <code class="literal">KataConfig</code> custom resource.
							</li><li class="listitem">
								You have uninstalled the OpenShift sandboxed containers Operator.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Delete the <code class="literal">KataConfig</code> CRD by running the following command:
							</p><pre class="programlisting language-terminal">$ oc delete crd kataconfigs.kataconfiguration.openshift.io</pre></li><li class="listitem"><p class="simpara">
								Verify that the CRD was deleted by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get crd kataconfigs.kataconfiguration.openshift.io</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-text">Unknown CRD kataconfigs.kataconfiguration.openshift.io</pre>
								</p></div></li></ol></div></section></section></section><section class="section" id="removing-cc-environment"><div class="titlepage"><div><div><h2 class="title">9.2. Removing the Confidential Containers environment</h2></div></div></div><p>
				You can remove the Confidential Containers environment by using the OpenShift Container Platform web console or the command line.
			</p><p>
				You remove the Confidential Containers environment by performing the following tasks:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Delete the <code class="literal">KbsConfig</code> custom resource.
					</li><li class="listitem">
						Uninstall the Confidential compute attestation Operator.
					</li><li class="listitem">
						Delete the <code class="literal">KbsConfig</code> custom resource definition.
					</li></ol></div><section class="section" id="removing-cc-environment-web"><div class="titlepage"><div><div><h3 class="title">9.2.1. Removing the Confidential Containers environment by using the web console</h3></div></div></div><p>
					You can remove the Confidential Containers environment by using the OpenShift Container Platform web console.
				</p><section class="section" id="deleting-cr-web_removing-cc-web"><div class="titlepage"><div><div><h4 class="title">9.2.1.1. Deleting the KbsConfig custom resource</h4></div></div></div><p>
						You can delete the <code class="literal">KbsConfig</code> custom resource (CR) by using the web console.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have uninstalled OpenShift sandboxed containers.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In the OpenShift Container Platform web console, navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
							</li><li class="listitem">
								Enter <code class="literal">Confidential compute attestation</code> in the <span class="strong strong"><strong>Search by name</strong></span> field.
							</li><li class="listitem">
								Click the Operator to open it and then click the <span class="strong strong"><strong>KbsConfig</strong></span> tab.
							</li><li class="listitem">
								Click the <span class="strong strong"><strong>Options</strong></span> menu 
								<span class="inlinemediaobject"><img src="images/kebab.png" alt="kebab"/></span>
								 and select <span class="strong strong"><strong>Delete <code class="literal">KbsConfig</code></strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Delete</strong></span> in the confirmation window.
							</li></ol></div></section><section class="section" id="uninstalling-operator-web_removing-cc-web"><div class="titlepage"><div><div><h4 class="title">9.2.1.2. Uninstalling the Confidential compute attestation Operator</h4></div></div></div><p>
						You can uninstall the Confidential compute attestation Operator by using OpenShift Container Platform web console.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have deleted the <code class="literal">KbsConfig</code> custom resource.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Navigate to <span class="strong strong"><strong>Operators</strong></span> → <span class="strong strong"><strong>Installed Operators</strong></span>.
							</li><li class="listitem">
								Enter <code class="literal">Confidential compute attestation</code> in the <span class="strong strong"><strong>Search by name</strong></span> field.
							</li><li class="listitem"><p class="simpara">
								On the right side of the <span class="strong strong"><strong>Operator Details</strong></span> page, select <span class="strong strong"><strong>Uninstall Operator</strong></span> from the <span class="strong strong"><strong>Actions</strong></span> list.
							</p><p class="simpara">
								An <span class="strong strong"><strong>Uninstall Operator?</strong></span> dialog box is displayed.
							</p></li><li class="listitem">
								Click <span class="strong strong"><strong>Uninstall</strong></span> to remove the Operator, Operator deployments, and pods.
							</li><li class="listitem">
								Navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Namespaces</strong></span>.
							</li><li class="listitem">
								Enter <code class="literal">trustee-operator-system</code> in the <span class="strong strong"><strong>Search by name</strong></span> field.
							</li><li class="listitem">
								Click the <span class="strong strong"><strong>Options</strong></span> menu 
								<span class="inlinemediaobject"><img src="images/kebab.png" alt="kebab"/></span>
								 and select <span class="strong strong"><strong>Delete Namespace</strong></span>.
							</li><li class="listitem">
								In the confirmation dialog, enter <code class="literal">trustee-operator-system</code> and click <span class="strong strong"><strong>Delete</strong></span>.
							</li></ol></div></section><section class="section" id="deleting-crd-web_removing-cc-web"><div class="titlepage"><div><div><h4 class="title">9.2.1.3. Deleting the KbsConfig CRD</h4></div></div></div><p>
						You can delete the <code class="literal">KbsConfig</code> custom resource definition (CRD) by using the OpenShift Container Platform web console.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have deleted the <code class="literal">KbsConfig</code> custom resource.
							</li><li class="listitem">
								You have uninstalled the Confidential compute attestation Operator.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In the web console, navigate to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>CustomResourceDefinitions</strong></span>.
							</li><li class="listitem">
								Enter the <code class="literal">KbsConfig</code> name in the <span class="strong strong"><strong>Search by name</strong></span> field.
							</li><li class="listitem">
								Click the <span class="strong strong"><strong>Options</strong></span> menu and select <span class="strong strong"><strong>Delete CustomResourceDefinition</strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Delete</strong></span> in the confirmation window.
							</li></ol></div></section></section><section class="section" id="removing-cc-environment-cli"><div class="titlepage"><div><div><h3 class="title">9.2.2. Removing the Confidential Containers environment by using the CLI</h3></div></div></div><p>
					You can remove the Confidential Containers environment by using the command-line interface (CLI).
				</p><section class="section" id="deleting-cr-cli_removing-cc-cli"><div class="titlepage"><div><div><h4 class="title">9.2.2.1. Deleting the KbsConfig custom resource</h4></div></div></div><p>
						You can delete the <code class="literal">KbsConfig</code> custom resource (CR) by using the command line.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have uninstalled OpenShift sandboxed containers.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Delete the <code class="literal">KbsConfig</code> CR by running the following command:
							</p><pre class="programlisting language-terminal">$ oc delete kbsconfig kbsconfig</pre></li><li class="listitem"><p class="simpara">
								Verify that the custom resource was deleted by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get kbsconfig kbsconfig</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-text">No kbsconfig instances exist</pre>
								</p></div></li></ol></div></section><section class="section" id="uninstalling-operator-cli_removing-cc-cli"><div class="titlepage"><div><div><h4 class="title">9.2.2.2. Uninstalling the Confidential compute attestation Operator</h4></div></div></div><p>
						You can uninstall the Confidential compute attestation Operator by using the command line.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have deleted the <code class="literal">KbsConfig</code> custom resource.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Delete the subscription by running the following command:
							</p><pre class="programlisting language-terminal">$ oc delete subscription trustee-operator -n trustee-operator-system</pre></li><li class="listitem"><p class="simpara">
								Delete the namespace by running the following command:
							</p><pre class="programlisting language-terminal">$ oc delete namespace trustee-operator-system</pre></li></ol></div></section><section class="section" id="deleting-crd-cli_removing-cc-cli"><div class="titlepage"><div><div><h4 class="title">9.2.2.3. Deleting the KbsConfig CRD</h4></div></div></div><p>
						You can delete the <code class="literal">KbsConfig</code> custom resource definition (CRD) by using the command line.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have deleted the <code class="literal">KbsConfig</code> custom resource.
							</li><li class="listitem">
								You have uninstalled the Confidential compute attestation Operator.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Delete the <code class="literal">KbsConfig</code> CRD by running the following command:
							</p><pre class="programlisting language-terminal">$ oc delete crd kbsconfigs.confidentialcontainers.org</pre></li><li class="listitem"><p class="simpara">
								Verify that the CRD was deleted by running the following command:
							</p><pre class="programlisting language-terminal">$ oc get crd kbsconfigs.confidentialcontainers.org</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
									
<pre class="programlisting language-text">Unknown CRD kbsconfigs.confidentialcontainers.org</pre>
								</p></div></li></ol></div></section></section></section></section><section class="chapter" id="upgrading"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Upgrading</h1></div></div></div><p>
			The upgrade of the OpenShift sandboxed containers components consists of the following three steps:
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Upgrade OpenShift Container Platform to update the <code class="literal">Kata</code> runtime and its dependencies.
				</li><li class="listitem">
					Upgrade the OpenShift sandboxed containers Operator to update the Operator subscription.
				</li></ol></div><p>
			You can upgrade OpenShift Container Platform before or after the OpenShift sandboxed containers Operator upgrade, with the one exception noted below. Always apply the <code class="literal">KataConfig</code> patch immediately after upgrading OpenShift sandboxed containers Operator.
		</p><section class="section" id="upgrading-resources"><div class="titlepage"><div><div><h2 class="title">10.1. Upgrading resources</h2></div></div></div><p>
				The OpenShift sandboxed containers resources are deployed onto the cluster using Red Hat Enterprise Linux CoreOS (RHCOS) extensions.
			</p><p>
				The RHCOS extension <code class="literal">sandboxed containers</code> contains the required components to run OpenShift sandboxed containers, such as the Kata containers runtime, the hypervisor QEMU, and other dependencies. You upgrade the extension by upgrading the cluster to a new release of OpenShift Container Platform.
			</p><p>
				For more information about upgrading OpenShift Container Platform, see <a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/updating/index">Updating Clusters</a>.
			</p></section><section class="section" id="upgrading-operator"><div class="titlepage"><div><div><h2 class="title">10.2. Upgrading the Operator</h2></div></div></div><p>
				Use Operator Lifecycle Manager (OLM) to upgrade the OpenShift sandboxed containers Operator either manually or automatically. Selecting between manual or automatic upgrade during the initial deployment determines the future upgrade mode. For manual upgrades, the OpenShift Container Platform web console shows the available updates that can be installed by the cluster administrator.
			</p><p>
				For more information about upgrading the OpenShift sandboxed containers Operator in Operator Lifecycle Manager (OLM), see <a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/operators/index#olm-upgrading-operators">Updating installed Operators</a>.
			</p></section></section><section class="chapter" id="troubleshooting"><div class="titlepage"><div><div><h1 class="title">Chapter 11. Troubleshooting</h1></div></div></div><p>
			When troubleshooting OpenShift sandboxed containers, you can open a support case and provide debugging information using the <code class="literal">must-gather</code> tool.
		</p><p>
			If you are a cluster administrator, you can also review logs on your own, enabling a more detailed level of logs.
		</p><section class="section" id="collect_data_rh_support"><div class="titlepage"><div><div><h2 class="title">11.1. Collecting data for Red Hat Support</h2></div></div></div><p>
				When opening a support case, it is helpful to provide debugging information about your cluster to Red Hat Support.
			</p><p>
				The <code class="literal">must-gather</code> tool enables you to collect diagnostic information about your OpenShift Container Platform cluster, including virtual machines and other data related to OpenShift sandboxed containers.
			</p><p>
				For prompt support, supply diagnostic information for both OpenShift Container Platform and OpenShift sandboxed containers.
			</p><h4 id="using-must-gather_troubleshooting">Using the must-gather tool</h4><p>
				The <code class="literal">oc adm must-gather</code> CLI command collects the information from your cluster that is most likely needed for debugging issues, including:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Resource definitions
					</li><li class="listitem">
						Service logs
					</li></ul></div><p>
				By default, the <code class="literal">oc adm must-gather</code> command uses the default plugin image and writes into <code class="literal">./must-gather.local</code>.
			</p><p>
				Alternatively, you can collect specific information by running the command with the appropriate arguments as described in the following sections:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						To collect data related to one or more specific features, use the <code class="literal">--image</code> argument with an image, as listed in a following section.
					</p><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc adm must-gather --image=registry.redhat.io/openshift-sandboxed-containers/osc-must-gather-rhel9:1.8.0</pre></li><li class="listitem"><p class="simpara">
						To collect the audit logs, use the <code class="literal">-- /usr/bin/gather_audit_logs</code> argument, as described in a following section.
					</p><p class="simpara">
						For example:
					</p><pre class="programlisting language-terminal">$ oc adm must-gather -- /usr/bin/gather_audit_logs</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Audit logs are not collected as part of the default set of information to reduce the size of the files.
						</p></div></div></li></ul></div><p>
				When you run <code class="literal">oc adm must-gather</code>, a new pod with a random name is created in a new project on the cluster. The data is collected on that pod and saved in a new directory that starts with <code class="literal">must-gather.local</code>. This directory is created in the current working directory.
			</p><p>
				For example:
			</p><pre class="programlisting language-terminal">NAMESPACE                      NAME                 READY   STATUS      RESTARTS      AGE
...
openshift-must-gather-5drcj    must-gather-bklx4    2/2     Running     0             72s
openshift-must-gather-5drcj    must-gather-s8sdh    2/2     Running     0             72s
...</pre><p>
				Optionally, you can run the <code class="literal">oc adm must-gather</code> command in a specific namespace by using the <code class="literal">--run-namespace</code> option.
			</p><p>
				For example:
			</p><pre class="programlisting language-terminal">$ oc adm must-gather --run-namespace &lt;namespace&gt; --image=registry.redhat.io/openshift-sandboxed-containers/osc-must-gather-rhel9:1.8.0</pre></section><section class="section" id="collecting-log-data"><div class="titlepage"><div><div><h2 class="title">11.2. Collecting log data</h2></div></div></div><p>
				The following features and objects are associated with OpenShift sandboxed containers:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						All namespaces and their child objects that belong to OpenShift sandboxed containers resources
					</li><li class="listitem">
						All OpenShift sandboxed containers custom resource definitions (CRDs)
					</li></ul></div><p>
				You can collect the following component logs for each pod running with the <code class="literal">kata</code> runtime:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Kata agent logs
					</li><li class="listitem">
						Kata runtime logs
					</li><li class="listitem">
						QEMU logs
					</li><li class="listitem">
						Audit logs
					</li><li class="listitem">
						CRI-O logs
					</li></ul></div><section class="section" id="enabling-debug-logs-crio_troubleshooting"><div class="titlepage"><div><div><h3 class="title">11.2.1. Enabling debug logs for CRI-O runtime</h3></div></div></div><p>
					You can enable debug logs by updating the <code class="literal">logLevel</code> field in the <code class="literal">KataConfig</code> CR. This changes the log level in the CRI-O runtime for the worker nodes running OpenShift sandboxed containers.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Change the <code class="literal">logLevel</code> field in your existing <code class="literal">KataConfig</code> CR to <code class="literal">debug</code>:
						</p><pre class="programlisting language-terminal">$ oc patch kataconfig &lt;kataconfig&gt; --type merge --patch '{"spec":{"logLevel":"debug"}}'</pre></li><li class="listitem"><p class="simpara">
							Monitor the <code class="literal">kata-oc</code> machine config pool until the value of <code class="literal">UPDATED</code> is <code class="literal">True</code>, indicating that all worker nodes are updated:
						</p><pre class="programlisting language-terminal">$ oc get mcp kata-oc</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">NAME     CONFIG                 UPDATED  UPDATING  DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT  DEGRADEDMACHINECOUNT  AGE
kata-oc  rendered-kata-oc-169   False    True      False     3             1                  1                    0                     9h</pre>
							</p></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Start a debug session with a node in the machine config pool:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;node_name&gt;</pre></li><li class="listitem"><p class="simpara">
							Change the root directory to <code class="literal">/host</code>:
						</p><pre class="programlisting language-terminal"># chroot /host</pre></li><li class="listitem"><p class="simpara">
							Verify the changes in the <code class="literal">crio.conf</code> file:
						</p><pre class="programlisting language-terminal"># crio config | egrep 'log_level</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
								
<pre class="programlisting language-terminal">log_level = "debug"</pre>
							</p></div></li></ol></div></section><section class="section" id="viewing-debug-logs-components_troubleshooting"><div class="titlepage"><div><div><h3 class="title">11.2.2. Viewing debug logs for components</h3></div></div></div><p>
					Cluster administrators can use the debug logs to troubleshoot issues. The logs for each node are printed to the node journal.
				</p><p>
					You can review the logs for the following OpenShift sandboxed containers components:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Kata agent
						</li><li class="listitem">
							Kata runtime (<code class="literal">containerd-shim-kata-v2</code>)
						</li><li class="listitem">
							<code class="literal">virtiofsd</code>
						</li></ul></div><p>
					QEMU only generates warning and error logs. These warnings and errors print to the node journal in both the Kata runtime logs and the CRI-O logs with an extra <code class="literal">qemuPid</code> field.
				</p><div class="formalpara"><p class="title"><strong>Example of QEMU logs</strong></p><p>
						
<pre class="programlisting language-text">Mar 11 11:57:28 openshift-worker-0 kata[2241647]: time="2023-03-11T11:57:28.587116986Z" level=info msg="Start logging QEMU (qemuPid=2241693)" name=containerd-shim-v2 pid=2241647 sandbox=d1d4d68efc35e5ccb4331af73da459c13f46269b512774aa6bde7da34db48987 source=virtcontainers/hypervisor subsystem=qemu

Mar 11 11:57:28 openshift-worker-0 kata[2241647]: time="2023-03-11T11:57:28.607339014Z" level=error msg="qemu-kvm: -machine q35,accel=kvm,kernel_irqchip=split,foo: Expected '=' after parameter 'foo'" name=containerd-shim-v2 pid=2241647 qemuPid=2241693 sandbox=d1d4d68efc35e5ccb4331af73da459c13f46269b512774aa6bde7da34db48987 source=virtcontainers/hypervisor subsystem=qemu

Mar 11 11:57:28 openshift-worker-0 kata[2241647]: time="2023-03-11T11:57:28.60890737Z" level=info msg="Stop logging QEMU (qemuPid=2241693)" name=containerd-shim-v2 pid=2241647 sandbox=d1d4d68efc35e5ccb4331af73da459c13f46269b512774aa6bde7da34db48987 source=virtcontainers/hypervisor subsystem=qemu</pre>
					</p></div><p>
					The Kata runtime prints <code class="literal">Start logging QEMU</code> when QEMU starts, and <code class="literal">Stop Logging QEMU</code> when QEMU stops. The error appears in between these two log messages with the <code class="literal">qemuPid</code> field. The actual error message from QEMU appears in red.
				</p><p>
					The console of the QEMU guest is printed to the node journal as well. You can view the guest console logs together with the Kata agent logs.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
						</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							To review the Kata agent logs and guest console logs, run the following command:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;nodename&gt; -- journalctl -D /host/var/log/journal -t kata -g “reading guest console”</pre></li><li class="listitem"><p class="simpara">
							To review the Kata runtime logs, run the following command:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;nodename&gt; -- journalctl -D /host/var/log/journal -t kata</pre></li><li class="listitem"><p class="simpara">
							To review the <code class="literal">virtiofsd</code> logs, run the following command:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;nodename&gt; -- journalctl -D /host/var/log/journal -t virtiofsd</pre></li><li class="listitem"><p class="simpara">
							To review the QEMU logs, run the following command:
						</p><pre class="programlisting language-terminal">$ oc debug node/&lt;nodename&gt; -- journalctl -D /host/var/log/journal -t kata -g "qemuPid=\d+"</pre></li></ul></div><h3 id="additional_resources_4">Additional resources</h3><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.17/html-single/support/index#support_gathering_data_gathering-cluster-data">Gathering data about your cluster</a> in the OpenShift Container Platform documentation
						</li></ul></div></section></section></section><section class="appendix" id="kataconfig-status-messages"><div class="titlepage"><div><div><h1 class="title">Appendix A. KataConfig status messages</h1></div></div></div><p>
			The following table displays the status messages for the <code class="literal">KataConfig</code> custom resource (CR) for a cluster with two worker nodes.
		</p><div class="table" id="idm46763384607552"><p class="title"><strong>Table A.1. <code class="literal">KataConfig</code> status messages</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm46763384602400" scope="col">Status</th><th align="left" valign="top" id="idm46763384601312" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm46763384602400">
						<p>
							<span class="strong strong"><strong>Initial installation</strong></span>
						</p>
						<p>
							When a <code class="literal">KataConfig</code> CR is created and starts installing <code class="literal">kata-remote</code> on both workers, the following status is displayed for a few seconds.
						</p>
						</td><td align="left" valign="top" headers="idm46763384601312">
<pre class="programlisting language-yaml"> conditions:
    message: Performing initial installation of kata-remote on cluster
    reason: Installing
    status: 'True'
    type: InProgress
 kataNodes:
   nodeCount: 0
   readyNodeCount: 0</pre>
						</td></tr><tr><td align="left" valign="top" headers="idm46763384602400">
						<p>
							<span class="strong strong"><strong>Installing</strong></span>
						</p>
						<p>
							Within a few seconds the status changes.
						</p>
						</td><td align="left" valign="top" headers="idm46763384601312">
<pre class="programlisting language-yaml"> kataNodes:
   nodeCount: 2
   readyNodeCount: 0
   waitingToInstall:
   - worker-0
   - worker-1</pre>
						</td></tr><tr><td align="left" valign="top" headers="idm46763384602400">
						<p>
							<span class="strong strong"><strong>Installing</strong></span> (Worker-1 installation starting)
						</p>
						<p>
							For a short period of time, the status changes, signifying that one node has initiated the installation of <code class="literal">kata-remote</code>, while the other is in a waiting state. This is because only one node can be unavailable at any given time. The <code class="literal">nodeCount</code> remains at 2 because both nodes will eventually receive <code class="literal">kata-remote</code>, but the <code class="literal">readyNodeCount</code> is currently 0 as neither of them has reached that state yet.
						</p>
						</td><td align="left" valign="top" headers="idm46763384601312">
<pre class="programlisting language-yaml"> kataNodes:
   installing:
   - worker-1
   nodeCount: 2
   readyNodeCount: 0
   waitingToInstall:
   - worker-0</pre>
						</td></tr><tr><td align="left" valign="top" headers="idm46763384602400">
						<p>
							<span class="strong strong"><strong>Installing</strong></span> (Worker-1 installed, worker-0 installation started)
						</p>
						<p>
							After some time, <code class="literal">worker-1</code> will complete its installation, causing a change in the status. The <code class="literal">readyNodeCount</code> is updated to 1, indicating that <code class="literal">worker-1</code> is now prepared to execute <code class="literal">kata-remote</code> workloads. You cannot schedule or run <code class="literal">kata-remote</code> workloads until the runtime class is created at the end of the installation process.
						</p>
						</td><td align="left" valign="top" headers="idm46763384601312">
<pre class="programlisting language-yaml"> kataNodes:
   installed:
   - worker-1
   installing:
   - worker-0
   nodeCount: 2
   readyNodeCount: 1</pre>
						</td></tr><tr><td align="left" valign="top" headers="idm46763384602400">
						<p>
							<span class="strong strong"><strong>Installed</strong></span>
						</p>
						<p>
							When installed, both workers are listed as installed, and the <code class="literal">InProgress</code> condition transitions to <code class="literal">False</code> without specifying a reason, indicating the successful installation of <code class="literal">kata-remote</code> on the cluster.
						</p>
						</td><td align="left" valign="top" headers="idm46763384601312">
<pre class="programlisting language-yaml"> conditions:
    message: ""
    reason: ""
    status: 'False'
    type: InProgress
 kataNodes:
   installed:
   - worker-0
   - worker-1
   nodeCount: 2
   readyNodeCount: 2</pre>
						</td></tr></tbody></table></div></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"/><col style="width: 50%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm46763384810928" scope="col">Status</th><th align="left" valign="top" id="idm46763384809840" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm46763384810928">
						<p>
							<span class="strong strong"><strong>Initial uninstall</strong></span>
						</p>
						<p>
							If <code class="literal">kata-remote</code> is installed on both workers, and you delete the <code class="literal">KataConfig</code> to remove <code class="literal">kata-remote</code> from the cluster, both workers briefly enter a waiting state for a few seconds.
						</p>
						</td><td align="left" valign="top" headers="idm46763384809840">
<pre class="programlisting language-yaml"> conditions:
    message: Removing kata-remote from cluster
    reason: Uninstalling
    status: 'True'
    type: InProgress
 kataNodes:
   nodeCount: 0
   readyNodeCount: 0
   waitingToUninstall:
   - worker-0
   - worker-1</pre>
						</td></tr><tr><td align="left" valign="top" headers="idm46763384810928">
						<p>
							<span class="strong strong"><strong>Uninstalling</strong></span>
						</p>
						<p>
							After a few seconds, one of the workers starts uninstalling.
						</p>
						</td><td align="left" valign="top" headers="idm46763384809840">
<pre class="programlisting language-yaml"> kataNodes:
   nodeCount: 0
   readyNodeCount: 0
   uninstalling:
   - worker-1
   waitingToUninstall:
   - worker-0</pre>
						</td></tr><tr><td align="left" valign="top" headers="idm46763384810928">
						<p>
							<span class="strong strong"><strong>Uninstalling</strong></span>
						</p>
						<p>
							Worker-1 finishes and worker-0 starts uninstalling.
						</p>
						</td><td align="left" valign="top" headers="idm46763384809840">
<pre class="programlisting language-yaml"> kataNodes:
   nodeCount: 0
   readyNodeCount: 0
   uninstalling:
   - worker-0</pre>
						</td></tr></tbody></table></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				The <code class="literal">reason</code> field can also report the following causes:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">Failed</code>: This is reported if the node cannot finish its transition. The <code class="literal">status</code> reports <code class="literal">True</code> and the <code class="literal">message</code> is <code class="literal">Node &lt;node_name&gt; Degraded: &lt;error_message_from_the_node&gt;</code>.
					</li><li class="listitem">
						<code class="literal">BlockedByExistingKataPods</code>: This is reported if there are pods running on a cluster that use the <code class="literal">kata-remote</code> runtime while <code class="literal">kata-remote</code> is being uninstalled. The <code class="literal">status</code> field is <code class="literal">False</code> and the <code class="literal">message</code> is <code class="literal">Existing pods using "kata-remote" RuntimeClass found. Please delete the pods manually for KataConfig deletion to proceed</code>. There could also be a technical error message reported like <code class="literal">Failed to list kata pods: &lt;error_message&gt;</code> if communication with the cluster control plane fails.
					</li></ul></div></div></div></section><div><div xml:lang="en-US" class="legalnotice" id="idm46763383936928"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"/>© 2024 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div></div><script type="text/javascript">
                        jQuery(document).ready(function() {
                            initSwitchery();
                            jQuery('pre[class*="language-"]').each(function(i, block){hljs.highlightBlock(block);});
                        });
                    </script></body></html>